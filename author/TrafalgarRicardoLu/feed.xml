<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="http://localhost:4000/author/TrafalgarRicardoLu/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2020-08-17T23:16:11+08:00</updated>
  <id>http://localhost:4000/author/TrafalgarRicardoLu/feed.xml</id>

  
  
  

  
    <title type="html">Ghost | </title>
  

  
    <subtitle>The professional publishing platform</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">浅谈Spanner</title>
      <link href="http://localhost:4000/%E6%B5%85%E8%B0%88Spanner" rel="alternate" type="text/html" title="浅谈Spanner" />
      <published>2020-08-17T19:00:00+08:00</published>
      <updated>2020-08-17T19:00:00+08:00</updated>
      <id>http://localhost:4000/%E6%B5%85%E8%B0%88Spanner</id>
      <content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Spanner">&lt;p&gt;在2012年的OSDI上，谷歌发表了《Spanner: Google’s Globally-Distributed Database》，其中介绍了谷歌第二代的数据库，也就是Bigtable的继任者——Spanner。在使用Bigtable的过程中，谷歌的开发人员逐渐意识到Bigtable的一些不足之处，比如不能处理变化的数据格式，不能保证大范围内数据库的一致性以及对跨行事务的处理。谷歌为了解决这些问题，开发出了Spanner。&lt;/p&gt;

&lt;h2 id=&quot;总体架构&quot;&gt;总体架构&lt;/h2&gt;

&lt;p&gt;谷歌设计Spanner的一个重要目标是对全球范围内的数据库进行管理，为了更加清晰有效地划分和管理数据，Spanner划分了多个层级。其中，最高级的是Universe，在论文中谷歌表示目前只有三个Universe，包括一个用于测试或后台运行的Universe，一个用于部署或生产的Universe和一个仅用于生产的Universe。每个Universe下面包含多个Zone，每个Universe使用UniverseMaster和PlaceDriver检测和管理Zone，这里的Zone就相当于Bigtable的Server，对应实际情况中的一台或多台物理机器。而每个Zone中有一个ZoneMaster管理多个LocationProxy和数百至数千个SpannerServer，其中，LocationProxy负责将客户端的请求转发到对应的SpannerServer。论文中给出的示意图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Spanner structure.png&quot; alt=&quot;Spanner structure&quot; /&gt;&lt;/p&gt;

&lt;p&gt;论文中仅仅披露了SpannerServer的具体内容，所以下面我们也只讨论SpannerServer。由于Spanner是为了代替Bigtable而设计的，所以SpannerServer的内部架构其实和Bigtable有一点类似，但是Spanner又作出了很多优化。内部架构的示意图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Spanner Server.png&quot; alt=&quot;Spanner Server&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这张示意图以三副本的情况为例介绍了Spanner Server的内部架构。Spanner和Bigtable相似的地方在于，他们都基于分布式的文件系统GFS，这里的Colossus是第二代GFS，具体的实现细节仍未公布，而且他们都使用了Tablet作为单位管理存储数据，但是，这里的Tablet和Bigtable中Tablet是有些不同的。但是再上面一层就有些不同了，因为Bigtable中数据基于爬虫获得而且使用了SSTable存储数据，这保证了数据的唯一性，所以在Bigtable中没有使用具体的算法保证数据的一致性，但也因为这一点，Bigtable没有很好地支持事务。Spanner中使用了多副本的机制备份数据，同时在副本之间使用Single Paxos算法保证了数据一致性，在这里，谷歌可能是为了提高Paxos算法的性能并降低耦合，他们并不是把整个机器作为Paxos算法的基本角色，而是将机器中的数据分割为多个Paxos Group，每个机器中的相同数据被标识为同一Group，每次运行Paxos算法都仅由关联的Paxos Group参与。再往上一层就是各机器之间的关系了，Spanner使用主从结构管理副本，Leader节点要额外维护LockTable和Transaction Manager，其中LockTable的作用类似Bigtable中的Chubby，用于协调各副本之间的并发操作，Transaction Manager则用于管理分布式事务。Leader节点还要负责所有的写操作和和其他节点的沟通。&lt;/p&gt;

&lt;p&gt;Paxos Group仍然是很大的操作单位，想要更加灵活地进行数据迁移工作就需要更小的数据单位。于是Spanner将每个Paxos Group分割为多个Directory，而每个Directory包含若干个拥有连续前缀Key的数据。不得不说，这里的连续前缀Key有点像SSTable的设计。Spanner将Directory作为物理位置记录的单元，同时也是均衡负载和数据迁移的基础单元。这很好理解，均衡负载是把请求转发到拥有相同数据的机器，数据迁移是把数据从一台机器复制到另一台机器，这两者都需要目的机器的物理地址，所以最小只能把Directory作为单位。&lt;/p&gt;

&lt;p&gt;Spanner把在Paxos Group之间迁移Directory设计为后台任务，但是由于数据迁移可能造成读写阻塞，所以它不被设计成事务。操作的时候是先将实际数据移动到指定位置，然后再用一个原子的操作更新元数据，完成整个移动过程。&lt;/p&gt;

&lt;p&gt;这里要特别说明的是，Spanner中的Tablet和Bigtable中的Tablet有些不同。Bigtable中的Tablet可以简单地看做是若干连续的有序记录，而Spanner中的Tablet则被设计成一种容器，其不一定是连续的有序记录，而可能包括多个副本的数据。&lt;/p&gt;

&lt;h2 id=&quot;数据模型&quot;&gt;数据模型&lt;/h2&gt;

&lt;p&gt;Spanner和Bigtable的数据模型差别也很大，其数据模型如下。很容易地可以发现，其从Bigtable中类似于关系型的数据库变成了类似于K-V的数据库。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Spanner datamodel.png&quot; alt=&quot;Spanner datamodel&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这种变化重要的原因是Bigtable的数据模型仅适用于类似PageRank等数据格式长期稳定且不怎么变化的任务，如果任务需要快速版本迭代可能就不再使用。&lt;/p&gt;

&lt;p&gt;另一方面，在Google内部有一个Megastore数据库，尽管要忍受性能不够的折磨，但是在Google有300多个应用在用它，包括Gmail, Picasa, Calendar, Android Market和AppEngine。而这仅仅因为Megastore支持一个类似关系数据库的语法和同步复制。所以，Spanner决定支持数据库的语法，论文中说这种语法类似于SQL语句。其结果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Spanner Data Example.png&quot; alt=&quot;Spanner Data Example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，Spanner底层的存储结构仍然是K-V的形式，但是在创建Albums的表时，其指出了父类表为User，那么在Directory中组织成了上图中关联的User和Albums相邻的形式。那么在进行SQL查询的时候，就可以通过顺序读写得到数据，相比随机读写要快得多。&lt;/p&gt;

&lt;h2 id=&quot;truetime&quot;&gt;TrueTime&lt;/h2&gt;

&lt;p&gt;前面讨论了Spanner总体架构，内部架构以及数据模型，平常的论文可能到此就结束了，但是这篇论文到这里进入最重要的部分，因为Spanner引入了一个开创性的想法，使用TrueTime标记时间。而TrueTime指的是真实的时间戳，谷歌使用GPS和原子钟两个物理元件得到这个时间。正常情况下使用GPS获取该时间，如果GPS由于电波影响不能工作，那么原子钟就会接替任务直到GPS恢复工作。Spanner中提供了三个和其相关的API，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/TrueTime API.png&quot; alt=&quot;TrueTime API&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中，TT.now()返回的值称为TTinterval，它不是一个确切的时刻，而是一段时间，包括最早时间戳Earlist和最晚时间戳Latest。因为全球范围内的时间总是不可能完全同步，各机器的通信也有延迟，所以只要在一个时间段内，他们就认为是同步的。另外两个函数的伪代码定义如下&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TT.after(t) = TT.now().earliest &amp;gt; t.latest
TT.before(t) = TT.now().latest &amp;lt; t.earliest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Spanner给每个数据中心都安装了TrueTime系统，有了这三个API，Spanner就能判断两个时间戳的先后关系了，再以此为基础，就能实现很多功能。&lt;/p&gt;

&lt;h2 id=&quot;并发控制&quot;&gt;并发控制&lt;/h2&gt;

&lt;p&gt;TrueTime是相当强大的工具，Spanner中相当多的操作都依赖于它。下面的表列举了Spanner支持的操作类型：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Spanner Transaction.png&quot; alt=&quot;Spanner Transaction&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;leader-leases&quot;&gt;Leader Leases&lt;/h3&gt;

&lt;p&gt;在讨论事务之前，我们回到Paxos算法上。与Raft使用的解决方案不同，Spanner不使用Heartbeats来检测是否有失效节点，而是使用Lease来规定Leader的任期。如果Leader执行了写操作，那么它的Lease会自动延长。否则，Spanner默认每十秒Leader要发起续租Lease的请求，当收到Quorum的投票后会延长，反之就失去Lease，转变为Follower。&lt;/p&gt;

&lt;p&gt;在这里，Spanner要求单个Paxos Group中的Leader Lease中的TrueTime要不相交，因为一旦相交，就意味同一时刻有两个Leader，这是不被允许的。所以，当Leader退出系统或者降级为Follower时，要保证TT.after(t)对每个副本都成立。&lt;/p&gt;

&lt;h3 id=&quot;读写事务&quot;&gt;读写事务&lt;/h3&gt;

&lt;p&gt;Spanner保证了强外部一致性：如果一个事务T2开始（Start）在事务T1提交之后发生，那么T2的提交（Commit）时间肯定比T1的提交时间大。简单地说，早到早完成，迟到迟完成。&lt;/p&gt;

&lt;p&gt;Spanner使用2PC提交事务，为了保证外部一致性，Spanner对每个事务作出以下两点要求：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Start：事务T的协调者Leader分配了一个提交时间戳S，S不小于TT.now().latest并且S不早于Commit请求到达时间&lt;/p&gt;

  &lt;p&gt;Commit Wait：Spanner保证只有当当前时间晚于事务T的提交时间后，其他成员才能看见。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;论文中的符号表示如下
&lt;script type=&quot;math/tex&quot;&gt;e^{Start}_i:事务Start事件\\e^{Server}_i:事务Commit事件请求到达\\e^{Commit}_i:中事务Commit完成事件\\t_{abs}(e):该事务完成的真实时间\\S_{i}:事务Commit的时间戳\\&lt;/script&gt;
那么上述的要求就变为
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
Start:t_{abs}(e^{Server}_i) &lt; S_{i}\\
Commit Wait:S_{i} &lt; t_{abs}(e^{Commit}_i)\\ %]]&gt;&lt;/script&gt;
有如下推断
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
如果\\
t_{abs}(e^{Commit}_i) &lt; t_{abs}(e^{start}_{i+1})\\
那么\\
S_{i} &lt; t_{abs}(e^{Commit}_i)\\
t_{abs}(e^{Commit}_i) &lt; t_{abs}(e^{start}_{i+1})\\
t_{abs}(e^{start}_{i+1}) &lt; t_{abs}(e^{server}_{i+1})\\
t_{abs}(e^{Server}_{i+1}) &lt; S_{i+1}\\
所以\\
S_{i} &lt; S_{i+1} %]]&gt;&lt;/script&gt;
所以，只要满足上述两个要求，就能实现外部一致性。而Spanner中使用Coordinate Leader来实现这两个要求。&lt;/p&gt;

&lt;p&gt;另外，还需要保证任意时间读取到的数据都是可靠的安全的。Spanner通过规定安全时间保证任意时间的读。论文中安全时间的定义为
&lt;script type=&quot;math/tex&quot;&gt;t_{sate}=min(t^{Paxos}_{safe},t^{TM}_{safe})\\
t^{Paxos}_{safe}:Paxos算法能保证的安全时间\\
t^{TM}_{safe}:事务管理器能保证的安全时间&lt;/script&gt;
其中，Paxos算法能保证的安全时间是最近Paxos算法写入数据的时间。事务管理器能保证的安全时间在，没有事务处于Prepare阶段的情况下，是无限大的。因为它无法确定第二阶段的事务可能发生什么。如果有事务已经完成Prepare阶段，那么它的值就是完成Prepare阶段的事务中最早的时间-1。在这个时间前，事务管理器能保证所有已提交的数据都被看见。只要读取的数据在这个时间之前，那么这个数据就是安全的。&lt;/p&gt;

&lt;p&gt;以下是读写事务的具体流程&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;执行读操作时，Spanner先找到相关数据的副本，然后加上读锁并读取最新的数据。在事务开启的时候，客户端会发送Keeplive消息防止超时。&lt;/li&gt;
    &lt;li&gt;当客户端完成了所有的读操作并缓存了所有的写操作，就准备开始2PC。客户端选择一个Coordinator Group，并给每一个相关的Leader发送Coordinator的id和缓存的写数据。&lt;/li&gt;
    &lt;li&gt;每个和事务相关的Leader（除了Coordinator Leader）会尝试得到一个写锁，然后选取一个比现有事务晚的时间戳并通过Paxos发送给其他副本。（这里是为了保证上面的假设）&lt;/li&gt;
    &lt;li&gt;Coordinator Leader一开始也会上个写锁，但是会跳过Prepare阶段。当接受到其他Leader的时间戳之后，他会选择一个提交时间戳。这个提交的时间戳必须满足Start保证并且大于所有完成Prepare阶段的时间戳（安全读取保证）。然后将这个信息通过Paxos记录下来。&lt;/li&gt;
    &lt;li&gt;Coordinator必须等到TT.after(S)成立才能在副本提交事务。这是为了满足Commit Wait保证。这需要等两倍时间误差，大约是20ms。&lt;/li&gt;
    &lt;li&gt;然后Coordinator将提交时间戳发送给客户端还有其他的副本。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;只读事务&quot;&gt;只读事务&lt;/h3&gt;

&lt;p&gt;一个只读的事务会分2阶段来执行：先给该事务分配一个时间戳，然后执行事务的读操作。相当于通过快照读来读取时刻S的值。那么只需要保证如下要求，由于时间永远向前，所以能保证读取的是最新的数据。
&lt;script type=&quot;math/tex&quot;&gt;S_{read}=TT.now().least&lt;/script&gt;
以下是只读事务的具体流程&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;由于读操作涉及多个Paxos Group，所以要通过协商阶段决定这个值Scope。&lt;/li&gt;
    &lt;li&gt;如果Scope只被一个Paxos Group处理，那么客户端会把只读事务发给那个的Leader。我们把LastTS()定义为Paxos组里最后一个写操作提交的时间戳。如果没有prepare阶段的事务，那么让S = LastTS()就能满足要求。&lt;/li&gt;
    &lt;li&gt;如果Scope会被多个Paxos Group处理，Spanner当前的实现是一个简单的方案。客户端会让S = TT.now().latest，然后以S时间戳执行读操作。所有事务里的读操作都会被发送到数据足够新的副本。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;谷歌充分了利用了使用Bigtable的丰富经验，设计出了Spanner。Spanner的全球架构和内部架构虽然精巧，但和Amazon的Aurora比起来也没有过人的地方。但是，TrueTime的提出和其在事务中的使用实在令人惊叹。以前的工作，比如2PC的交互和Raft中的Index，又或者是现在流行的消息队列，几乎都是通过自增的主键或者是逻辑上的前置条件判断事务的先后。Spanner简单粗暴却优雅地使用了时间，这一永远自增不会重复的量解决了这个问题。同时使用时间还直接解决了全球范围内的数据同步这一问题，而这一问题直接使用逻辑上的先后是无法实现的。这应该是我在6.824这门课上读到的最强的也是最令人赞叹的系统了。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">在2012年的OSDI上，谷歌发表了《Spanner:Google’s Globally-Distributed Database》，其中介绍了谷歌第二代的数据库，也就是Bigtable的继任者——Spanner。在使用Bigtable的过程中，谷歌的开发人员逐渐意识到Bigtable的一些不足之处，比如不能处理变化的数据格式，不能保证大范围内数据库的一致性以及对跨行事务的处理。谷歌为了解决这些问题，开发出了Spanner。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">总结Google的三驾马车</title>
      <link href="http://localhost:4000/%E6%80%BB%E7%BB%93Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6" rel="alternate" type="text/html" title="总结Google的三驾马车" />
      <published>2020-08-13T19:00:00+08:00</published>
      <updated>2020-08-13T19:00:00+08:00</updated>
      <id>http://localhost:4000/%E6%80%BB%E7%BB%93Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6</id>
      <content type="html" xml:base="http://localhost:4000/%E6%80%BB%E7%BB%93Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6">&lt;p&gt;前面的三篇文章，我们谈了Google的三驾马车，现在我们总体地对它们进行一个总结。&lt;/p&gt;

&lt;p&gt;要想对这三篇文章作总结，那么就必须要对这三篇文章的背景更加仔细地了解。文章开头的介绍是从总体的角度来看的，但是我最近看到一篇关于Jeff Dean的文章，从Jeff Dean的角度更加详细地介绍了当时的困局。我下面将引用一些该文章的中内容，并以此讨论三架马车的设计出发点。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;2000 年 3 月的一天，谷歌公司最顶尖的六位工程师齐聚某临时“作战指挥室”。当时的谷歌，正面临着前所未有的紧急状况。前一年 10 月，谷歌用于爬取 Web 以建立网络内容索引的核心系统宣告停止工作。虽然用户仍然可以通过 http://google.com 网站进行结果查询，但他们收到的结果实际上已经过期了五个月。由此引发的利益冲突远超工程师们的想象……就在当时，互联网的体量在一年之内增长了一倍。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;相当有趣的是，哪怕谷歌的爬虫已经崩溃了快半年，所有的查询结果都是六个月之前的，他们仍然敢和雅虎谈合作。也就是当时互联网内容更新不够快，要是放在今天，一个星期就够谷歌倒闭了。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;在作战室中度过的第五天，Jeff 与 Sanjay 开始怀疑其中的问题不是出在逻辑层面，而属于物理范畴。他们将混乱的索引文件转换为最原始的表示形式：二进制代码。更具体地讲，他们想了解机器到底看到了什么。&lt;/p&gt;

  &lt;p&gt;Sanjay 的显示器上出现了一系列粗体的 0 和 1，每一行代表一个索引词。Sanjay 发现，其中某个本应是 0 的数位却显示成了 1。接下来，Jeff 与 Sanjay 将所有错误排序的词语汇总了起来，并发现了其中的共通模式——每个词语都存在相同的问题。他们机器上的内存芯片发生了故障。&lt;/p&gt;

  &lt;p&gt;Sanjay 不由自主将目光投向 Jeff。几个月以来，谷歌一直在经历各种各样且持续增加的硬件故障。问题是，随着谷歌业务的快速发展，其计算基础设施也在不断扩大。计算机硬件发生故障的概率一般是很低的，因此问题堆积起来之后，一下子引发了破坏性的影响。电线磨损、磁盘损坏、主板过热。相当一部分设备根本无法一次性成功启动，也有一些莫名其妙地速度变慢了。&lt;/p&gt;

  &lt;p&gt;我们都会使用“搜索 Web”或者“搜索网络”的说法，但实际上这种表述并不准确。实际上，我们的搜索引擎遍历的是 Web 的索引——或者说地图。在 1996 年还在使用 BackRub 这个名号时，谷歌的索引地图还很小，足以安装在 Page 宿舍中的个人电脑里。但到 2000 年 3 月，其规模已经超出了任何超级计算机的处理能力。&lt;/p&gt;

  &lt;p&gt;要跟上如此迅猛的发展速度，谷歌公司唯一的方法就是购买消费级设备并将其组成一个集群。由于消费级设备当中有半数成本来自对谷歌公司毫无意义的“垃圾”——包括软盘驱动器与金属机箱，因此他们决定直接订购主板与磁盘并将其连接起来。谷歌在加利福尼亚州圣克拉拉市的一栋楼里将 1500 套这样的设备堆到了六英尺高； 但由于硬件故障，其中只有 1200 套设备能够正常工作。另外，随机发生的故障也在不断破坏着这套系统。为了维持业务运转，谷歌方面必须将这些计算机构建成一套无缝且具备弹性的整体。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从这里，我们一方面可以看出Jeff和Sanjay确实厉害，一般人最多查到逻辑层面，现在他们直接怀疑是硬件的问题。另一方面，这也说明了为什么GFS，Bigtable和MapReduce都要将容错（Fault-Tolerance）作为重要的考虑因素，并且都考虑最坏的情况，甚至在GFS中使用了多个备份的机器。&lt;/p&gt;

&lt;p&gt;这里必须要吹的是，GFS能充分发挥这么一堆“商业垃圾”的性能实在是强得可怕。哪怕是读过GFS的论文，对GFS有了初步的了解，我也很难想象它能在这么差劲的机器上撑起足够能用的存储体系，而且还能够保证数据的完整性。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;在 2003 年的四个月当中，Jeff 与 Sanjay 帮助谷歌完成了创立以来规模最大的一次升级。他们所使用的软件，就是后来赫赫有名的 MapReduce。这次升级的灵感，来自他们在对谷歌爬取工具与索引器进行第三次重写的过程。他们意识到，每一次他们解决一个重要问题，所面向的都是地理分布广阔且个别设备可能不太可靠的无数计算机上协同运行。因此，只有对解决方案进行全面推广，才能避免一次又一次重复面对同样的问题。更具体地讲，应该创建一款工具，确保谷歌公司的每一位程序员都能够利用其运行数据中心内的机器——换言之，将谷歌的所有基础设施视为一台硕大无朋的整体计算机。&lt;/p&gt;

  &lt;p&gt;MapReduce 的意义在于把可能令人费解的复杂流程整理得井然有序。在 MapReduce 出现之前，每一位程序员都必须弄清楚要如何对数据进行分割与分发，分配工作并自行负责硬件故障的处理。MapReduce 为编程人员提供了一种用于考量此类问题的结构化方法。正如厨师在食材下锅之前要先对其进行分类一样，MapReduce 也要求程序员将自己的任务分成两个阶段。首先，程序员需要告诉每台机器如何完成任务的“Map”阶段（比如计算某个词语在网页中出现的次数）； 接下来，程序员要编写指令以实现全部机器结果的“Reduce”（例如将上述结果相加）。MapReduce 可以处理分发工作的细节，从而帮助程序员摆脱这些复杂且枯燥的任务。&lt;/p&gt;

  &lt;p&gt;在接下来的一年当中，Jeff 与 Sanjay 以 MapReduce 任务的形式重写了谷歌的爬取与索引系统。很快，当其他工程师意识到 MapReduce 的强大力量后，他们也开始利用其处理视频并在谷歌地图上渲染图块。MapReduce 非常简单，能够轻松消化各类新型任务。谷歌的业务有着明确的所谓“昼夜使用曲线”——即白天的流量比晚间更多，而 MapReduce 任务则开始使用闲置部分的容量。正如生物会在梦中回顾白天的经历，现在谷歌也在利用同样的方式处理自己的数据。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从这里，我们就可以完全理解MapReduce的设计逻辑了。在这三篇文章发布的时候，谷歌几乎所有的重心都在搜索引擎上面，谷歌搜索引擎的核心就是PageRank算法，而URL之间的引用次数是PageRank重要的一部分。MapReduce要做的就是计算网页直接的引用次数，所以他们简单地将爬取的数据作为输入文件交给Map处理，当MapReduce都得到结果后，他们就能得到相关网页的引用次数，然后将其作为参数传入到PageRank中就能得到想要的答案。&lt;/p&gt;

&lt;p&gt;另外，由于夜间的流量较低，所以他们可以在晚上运行MapReduce程序而不影响用户体验。这也就解释了为什么谷歌能够容忍那么长的计算时间。&lt;/p&gt;

&lt;p&gt;简单地说，MapReduce之所以简单是因为它负责的计算就很简单，但是计算量很大，所以使用分布式的架构。MapReduce慢是因为谷歌根本不需要它快。所以MapReduce可能仅仅适用于谷歌和一些业务场景符合的公司。&lt;/p&gt;

&lt;p&gt;这篇文件中没有提到Bigtable，而且看起来MapReduce可以调用GFS而无需访问Bigtable，那么为什么Bigtable仍然成为人们津津乐道的技术，甚至在后来的Spanner中谷歌也使用了相当一部分的设计呢？那就是，数据库在业务逻辑中是无可替代的。&lt;/p&gt;

&lt;p&gt;数据库提供的不仅仅是数据的存储，更重要的是数据的读取。这里的存储和读取，不仅仅是完整的一致的，还要求是快速的。假如我们直接使用GFS，那么我们就没有完整的数据视图，需要像查文件一样查找数据，不仅效率低下，而且指定文件路径在编程上也是十分死板的做法。另一方面，GFS在数据写入方面也完全没有优化的处理。&lt;/p&gt;

&lt;p&gt;而在Bigtable中，由于Bigtable保证数据有序，那么读取和查找数据的效率会指数级别地提高。在写的方面，它使用WAL保证容错，使用Memtable保证高效，是GFS所不能比的。另一方面，它使用Column Family实现了类似于事务的功能，能够实现ACID的要求，这也是业务所强烈要求的。&lt;/p&gt;

&lt;p&gt;而且我相信，根据Bigtable的数据模型来看，爬虫爬下来的网页数据应该不是直接存入GFS，而是先存入Bigtable，然后通过Bigtable保存到GFS上的。因为Bigtable的数据模型怎么看都像是为了存储网页直接相互引用的信息设计的。而且，由于MapReduce产生的数据有可能在未来也被使用，这些数据被存入GFS。&lt;/p&gt;

&lt;p&gt;那么我们最后来总结一下三者的调用关系。谷歌使用爬虫爬取网页信息，并且把这些信息存入到Bigtable中，Bigtable会将这些数据排序并保存到GFS中。MapReduce为了得到PageRank的参数，会调用Bigtable中的数据，计算各网页的引用次数。计算完成后，MapReduce会将这些数据保存到GFS中以待下次使用。&lt;/p&gt;

&lt;p&gt;如果要对这三驾马车作出一个评价的话，那么应该这么说：在谷歌危急的时候，这三驾马车不仅力挽狂澜，而且为谷歌和其他互联网企业的发展指明了未来，是分布式系统的重要基石。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">前面的三篇文章，我们谈了Google的三驾马车，现在我们总体地对它们进行一个总结。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">浅谈Google的三驾马车之MapReduce</title>
      <link href="http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BMapReduce" rel="alternate" type="text/html" title="浅谈Google的三驾马车之MapReduce" />
      <published>2020-08-13T18:00:00+08:00</published>
      <updated>2020-08-13T18:00:00+08:00</updated>
      <id>http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BMapReduce</id>
      <content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BMapReduce">&lt;p&gt;上图为世界地图（World Map），实在找不到Reduce了。&lt;/p&gt;

&lt;p&gt;谷歌在2003到2006年间发表了三篇论文，《MapReduce: Simplified Data Processing on Large Clusters》，《Bigtable: A Distributed Storage System for Structured Data》和《The Google File System》介绍了Google如何对大规模数据进行存储和分析。这三篇论文开启了工业界的大数据时代，被称为Google的三驾马车。本文介绍MapReduce的相关内容。&lt;/p&gt;

&lt;h2 id=&quot;背景介绍&quot;&gt;背景介绍&lt;/h2&gt;

&lt;p&gt;在21世纪初，互联网上的内容，大多数企业需要存储的数据量并不大。但是Google不同，Google的搜索引擎的数据基于爬虫，而由于网页的大量增加，爬虫得到的数据也随之急速膨胀，单机或简单的分布式方案已经不能满足业务的需求，所以Google必须设计新的数据存储系统，其产物就是Google File System（GFS）。不过，在Google的设计中，为了尽可能的解耦，GFS仅负责数据存储而不提供类似数据库的服务。也就是说，GFS只存数据，而对数据的具体内容一无所知，自然也就不能提供基于内容的检索功能。所以，更进一步，Google开发了Bigtable作为数据库，向上层服务提供基于内容的各种功能。此外，Google 的搜索结果依赖于PageRank算法的排序，而该算法又需要一些额外的数据，比如某网页的被引用次数，所以他们还开发了对于的数据处理工具MapReduce，在读取了Bigtable数据的技术上，根据业务需求，对数据内容进行运算。其总体架构如下，GFS能充分利用多个Linux服务器的磁盘，并向上掩盖分布式系统的细节。Bigtable在GFS的基础上对数据内容进行识别和存储，向上提供类似数据库的各种操作。MapReduce则使用Bigtable中的数据进行运算，再提供给具体的业务使用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Google Troika.png&quot; alt=&quot;Troika&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;mapreduce&quot;&gt;MapReduce&lt;/h2&gt;

&lt;p&gt;MapReduce本来是函数式编程中的两个函数，在尝试解决利用大数据进行计算时，Jeff Dean和Sanjay Ghemawat想到了使用这种思想简化计算模型。&lt;/p&gt;

&lt;h3 id=&quot;基本思想&quot;&gt;基本思想&lt;/h3&gt;

&lt;p&gt;MapReduce把所有的计算都拆分成两个基本的计算操作，即Map和Reduce。其中Map函数以一系列键值对作为输入，然后输出一个中间文件（Intermediate）。这个中间态是另一种形式的键值对。然后，Reduce函数将这个中间态作为输入，计算得出结果。其中，Map函数和Reduce函数的逻辑都是由开发人员自行定义的。一种经典的逻辑如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/MapReduce Model.png&quot; alt=&quot;MapReduce Model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以WordCount为例，准备要统计一本书中所有单词出现的次数。在Map函数中，我们每遇到一个单词W，就往中间文件中写入（W，1）。然后，在Reduce函数中，把所有（W，1）出现的次数相加，就能得到W的出现次数V。&lt;/p&gt;

&lt;h3 id=&quot;分布式mapreduce流程&quot;&gt;分布式MapReduce流程&lt;/h3&gt;

&lt;p&gt;上面提到的模型和思想都是单机的，想要在分布式系统中实现，还需要一些改动。在MapReduce中，他们选择将大任务拆分成小任务分配给多台机器，以此充分利用分布式系统的性能。下图是论文中展示的MapReduce的流程图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/MapReduce Overview.png&quot; alt=&quot;MapReduce Overview&quot; /&gt;&lt;/p&gt;

&lt;p&gt;具体的流程如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;MapReduce客户端会将输入的文件会分为M个片段，每个片段的大小通常在 16~64 MB 之间。然后在多个机器上开始运行MapReduce程序。&lt;/li&gt;
    &lt;li&gt;系统中会有一个机器被选为Master节点，整个 MapReduce 计算包含M个Map 任务和R个 Reduce 任务。Master节点会为空闲的 Worker节点分配Map任务和 Reduce 任务&lt;/li&gt;
    &lt;li&gt;执行Map任务的 Worker开始读入自己对应的片段并将读入的数据解析为输入键值对。然后调用由用户定义的 Map任务。最后，Worker会将Map任务输出的结果存在内存中。&lt;/li&gt;
    &lt;li&gt;在执行Map的同时，Map Worker根据Partition 函数将产生的中间结果分为R个部分，然后定期将内存中的中间文件存入到自己的本地磁盘中。任务完成时，Mapper 便会将中间文件在其本地磁盘上的存放位置报告给 Master。&lt;/li&gt;
    &lt;li&gt;Master会将中间文件存放位置通知给Reduce Work。Reduce Worker接收到这些信息后便会通过RPC读取中间文件。在读取完毕后，Reduce Worker会对读取到的数据进行排序，保证拥有相同键的键值对能够连续分布。&lt;/li&gt;
    &lt;li&gt;最后，Reduce Worker会为每个键收集与其关联的值的集合，并调用用户定义的Reduce 函数。Reduce 函数的结果会被放入到对应的结果文件。&lt;/li&gt;
    &lt;li&gt;当所有Map和Reduce都结束后，程序会换新客户端并返回结果。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;整个流程非常清晰。首先，将输入文件分割成M个个片段，然后每个Map Worker读取对应的片段并执行Map函数，将结果存入中间文件。Reduce Work则通过Master得知中间文件的位置，然后读取其对应中间文件的内容并运行Reduce函数，最后把结果输出到结果文件中。&lt;/p&gt;

&lt;p&gt;这里值得说明的是，无论是输入文件到Map Worker的映射还是中间文件到Reduce Worker的映射都可以通过自定义的哈希函数来确定，论文中默认使用&lt;strong&gt;Hash(key) mod R&lt;/strong&gt;来确定。另外，M和R的值都是由用户指定的，应当比实际的机器数量要多一些，以此实现均衡负载。&lt;/p&gt;

&lt;h3 id=&quot;fault-tolerance&quot;&gt;Fault-Tolerance&lt;/h3&gt;

&lt;p&gt;因为使用了分布式系统，所以不可避免地要考虑容错的问题，在MapReduce中，容错也考虑Master和Work两种情况。&lt;/p&gt;

&lt;p&gt;Master节点会定期地将当前运行状态存为快照，当Master节点崩溃，就从最近的快照恢复然后重新执行任务。&lt;/p&gt;

&lt;p&gt;Master节点会定期地Ping每个Work节点，一旦发现Work节点不可达，针对其当前执行的是Map还是Reduce任务，会有不同的策略。&lt;/p&gt;

&lt;p&gt;如果是Map任务，无论任务已完成或是未完成，都会废除当前节点的任务。。之后，Master会将任务重新分配给其他节点，同时由于已经生成的中间文件不可访问，还会通知还未拿到中间文件的Reduce Worker去新的节点拿数据。&lt;/p&gt;

&lt;p&gt;如果是Reduce任务，由于结果文件存在GFS中，文件的可用性和一致性由GFS保证，所以Master仅将未完成的任务重新分配。&lt;/p&gt;

&lt;h3 id=&quot;优化&quot;&gt;优化&lt;/h3&gt;

&lt;p&gt;如果集群中有某个 Worker 花了特别长的时间来完成最后的几个 Map 或 Reduce 任务，整个 MapReduce 计算任务的耗时就会因此被拖长，这样的 Worker 也就成了落后者。MapReduce 在整个计算完成到一定程度时就会将剩余的任务即同时将其分配给其他空闲 Worker 来执行，并在其中一个 Worker 完成后将该任务视作已完成。&lt;/p&gt;

&lt;p&gt;这里论文中还提出了其他一些策略，但是我认为不是十分重要也就不再提及。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;MapReduce是一个相当简单的计算模型，它尝试将所有的计算任务都拆分成基础的Map和Reduce，以此降低实现的复杂度。但是，这恰恰提高了编程逻辑的复杂度。我看过使用MapReduce实现Join功能的代码，十分地巧妙灵活。但是看似巧妙的背后，是模型过于简单而导致复杂度转移到了代码逻辑的层面。&lt;/p&gt;

&lt;p&gt;另一方面，MapReduce的程序类似于批处理程序，需要完整的输入程序才能开始运算，而且每次运算都要至少写入两次磁盘。这就导致每次运算都要等待很长的时间，完全不能实现需要快速响应的业务场景的需求。&lt;/p&gt;

&lt;p&gt;以上两个方面，一个引出了支持类SQL的计算工具，另一个引出了支持流式计算的工具，而这两个特性正是今天流行的计算工具的热点。&lt;/p&gt;

&lt;p&gt;总得来说，虽然MapReduce在今天几乎抛弃了，但是在当初那个年代以及谷歌的业务需求看来，是相当合适的。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">谷歌在2003到2006年间发表了三篇论文，《MapReduce:Simplified Data Processing on Large Clusters》，《Bigtable:A Distributed Storage System for Structured Data》和《The Google File System》介绍了Google如何对大规模数据进行存储和分析。这三篇论文开启了工业界的大数据时代，被称为Google的三驾马车。本文介绍MapReduce的相关内容。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">浅谈Google的三驾马车之Bigtable</title>
      <link href="http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BBigtable" rel="alternate" type="text/html" title="浅谈Google的三驾马车之Bigtable" />
      <published>2020-08-10T18:00:00+08:00</published>
      <updated>2020-08-10T18:00:00+08:00</updated>
      <id>http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BBigtable</id>
      <content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BBigtable">&lt;p&gt;上图为真·大桌子&lt;/p&gt;

&lt;p&gt;谷歌在2003到2006年间发表了三篇论文，《MapReduce: Simplified Data Processing on Large Clusters》，《Bigtable: A Distributed Storage System for Structured Data》和《The Google File System》介绍了Google如何对大规模数据进行存储和分析。这三篇论文开启了工业界的大数据时代，被称为Google的三驾马车。本文介绍Bigtable的相关内容。&lt;/p&gt;

&lt;h2 id=&quot;背景介绍&quot;&gt;背景介绍&lt;/h2&gt;

&lt;p&gt;在21世纪初，互联网上的内容，大多数企业需要存储的数据量并不大。但是Google不同，Google的搜索引擎的数据基于爬虫，而由于网页的大量增加，爬虫得到的数据也随之急速膨胀，单机或简单的分布式方案已经不能满足业务的需求，所以Google必须设计新的数据存储系统，其产物就是Google File System（GFS）。不过，在Google的设计中，为了尽可能的解耦，GFS仅负责数据存储而不提供类似数据库的服务。也就是说，GFS只存数据，而对数据的具体内容一无所知，自然也就不能提供基于内容的检索功能。所以，更进一步，Google开发了Bigtable作为数据库，向上层服务提供基于内容的各种功能。此外，Google 的搜索结果依赖于PageRank算法的排序，而该算法又需要一些额外的数据，比如某网页的被引用次数，所以他们还开发了对于的数据处理工具MapReduce，在读取了Bigtable数据的技术上，根据业务需求，对数据内容进行运算。其总体架构如下，GFS能充分利用多个Linux服务器的磁盘，并向上掩盖分布式系统的细节。Bigtable在GFS的基础上对数据内容进行识别和存储，向上提供类似数据库的各种操作。MapReduce则使用Bigtable中的数据进行运算，再提供给具体的业务使用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Google troika.png&quot; alt=&quot;Troika&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;bigtable&quot;&gt;Bigtable&lt;/h2&gt;

&lt;p&gt;Bigtable实现在Google File System的基础上，它关心数据的内容，根据的数据的内容建立数据模型，对外提供读写数据的接口。&lt;/p&gt;

&lt;h3 id=&quot;数据模型&quot;&gt;数据模型&lt;/h3&gt;

&lt;p&gt;Bigtable基本的数据结构和关系型数据库类似，都是以行列构成的表，但是，它还另外增加了新的维度——时间。也就是说，在行列确定的情况下，一个单元格（Cell）中有多个以事件为版本的数据。Bigtable用(row:string, column:string, time:int64) → string表示映射关系。下图为论文中给出的一个例子。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Bigtable example.png&quot; alt=&quot;Bigtable example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果想要在表中查询指定版本的内容，我们需要指出行，列，及版本。比如（“com.cnn.www”，“anchor：cnnsi.com”，t9）→ “CNN”。我个人猜想，增加时间这个维度是因为“三驾马车”被设计出来的时候主要是为了支持搜索引擎，搜索引擎可能需要保留多个时间段的网页数据，而GFS也使用追加（Append）作为数据的主要修改方式，所以增加时间戳作为版本既充分利用了GFS的特性，也能满足业务的需求。&lt;/p&gt;

&lt;p&gt;另外，Bigtable还把多个Column Keys并入到被称为Column Family的集合中，并将Column Family作为访问控制的基础单元。我认为，这种方案其实是一种事务（Transaction）的实现方案。传统的事务以行为基本操作单位，在读写时对行上锁以实现隔离，而Bigtable则是以Column Family为单位，这里的访问控制其实就是锁的思想。&lt;/p&gt;

&lt;h3 id=&quot;相关组件&quot;&gt;相关组件&lt;/h3&gt;

&lt;p&gt;在介绍系统的整体架构之前，我们要对Bigtable用到的两个重要组件有一些了解。由于Bigtable是分布式的数据库，在节点之间的协调上需要额外的处理，这里，Bigtable使用了Google内部的Chubby。我们可以把Chubby看做是Zookeeper，因为Zookeeper本质也就是Chubby的开源版本。另一方面，为了加快数据的查找和存储效率，Bigtable在存储数据之前都进行了排序，而此处用到的存储文件文论称之为SSTable（Sorted String Table）。&lt;/p&gt;

&lt;p&gt;在Bigtable中，由于单个表（Table）存储的数据可能相当地多，那么读写的效率就会十分低下，于是Bigtable将Table分割为固定大小的Tablet，将其作为数据存储和查找的基本单位。每当Table增加了这里要说明的是，tablet是数据存储的基本单元，是用户感知不到的。而Column Family则是访问的基本单元，是编程时指定的，两者一前一后，不是一个概念。&lt;/p&gt;

&lt;h3 id=&quot;tablet-定位&quot;&gt;Tablet 定位&lt;/h3&gt;

&lt;p&gt;因为是在分布式系统中，那么每个Tablet所在的机器不同，需要记录相关信息（METADATA）对其进行管理。而存储这些METADATA又需要分布式的系统，所以Bigtable又将这些METADATA的METADATA记录在一个文件中，并将这个文件的位置保存在Chubby中。总结一下，Bigtable有以下三层结构：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;在Chubby中保存着Root Tablet的位置&lt;/li&gt;
    &lt;li&gt;Root Tablet中保存着METADATA Table中所有 Tablet 的位置&lt;/li&gt;
    &lt;li&gt;METADATA Table中保存着所有存储数据的Tablet的位置&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Tablet Location.png&quot; alt=&quot;Tablet Location&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这其中有几点值得注意。由于Root Tablet的特殊性，哪怕它的数据量再大，它也不允许被分割。METADATA tables被读取到内存中以加快速度，其中存储的是以开始和结尾的Row Key作为键，tablet位置作为值的映射。&lt;/p&gt;

&lt;p&gt;如果客户端希望读取特定的数据，那么它会以此读取Chubby中的文件，Root Tablet，METADATA Tablet，最后读取存储改数据的Tablet。同时，为了加快读取的速度，它会将这些信息缓存到本地，直到信息失效。&lt;/p&gt;

&lt;h3 id=&quot;tablet分配&quot;&gt;Tablet分配&lt;/h3&gt;

&lt;p&gt;在谈Table分配之前，论文先讨论了怎么处理成员变更的问题。类似于GFS，Bigtable使用Master节点来管理这些相关的事情。&lt;/p&gt;

&lt;p&gt;首先，Bigtable使用Chubby来检测Tablet Server的变化。这里的操作和Zookeeper的用法类似，当有新节点加入时，它需要在Chubby中新建一个对应的文件，并获取该文件的锁。由于所有的节点在Chubby中都有对应的文件，那么Master可以通过监听Chubby来获取所有Tablet Server的信息。这里有两种节点失效的情况，一种是仅仅回收了锁但是文件还在，这种情况很可能是节点崩溃了。由于节点不能自己退出，所以在Master节点得到该文件的锁后，它会将文件删除，以此表示节点退出。另一种情况是，文件已经被删除，这种情况说明节点是主动退出系统，那么可以直接重新分配Tablet给其他节点即可。&lt;/p&gt;

&lt;p&gt;在正常的情况下，系统中会有大量数据写入，Master需要负责将这些数据分配到合适的Tablet Server。Bigtable并没有明确指出分配所使用的的算法，但是它提出了一个要求。为了保证数据的一致性，同一时间，一个 Tablet只能被分配给一个Tablet Server。Master通过向 Tablet Server 发送载入请求来分配 Tablet。如果该载入请求被Tablet Server接收到前Master仍是有效的，那么就可以认为此次 Tablet 分配操作已成功。&lt;/p&gt;

&lt;p&gt;在这里，我们还要考虑Master崩溃的情况，论文中描述了Master恢复的步骤如下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;在 Chubby 上获取 Master 独有的锁，确保不会有另一个 Master 同时启动&lt;/li&gt;
    &lt;li&gt;从 Chubby 了解在工作的 Tablet Server&lt;/li&gt;
    &lt;li&gt;从各个 Tablet Server 处获取其所负责的 Tablet 列表，并向其表明自己作为新 Master 的身份，确保 Tablet Server 的后续通信能发往这个新 Master&lt;/li&gt;
    &lt;li&gt;Master 确保 Root Tablet 及 &lt;code class=&quot;highlighter-rouge&quot;&gt;METADATA&lt;/code&gt; 表的 Tablet 已完成分配&lt;/li&gt;
    &lt;li&gt;Master 扫描 &lt;code class=&quot;highlighter-rouge&quot;&gt;METADATA&lt;/code&gt; 表获取集群中的所有 Tablet，并对未分配的 Tablet 重新进行分配&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;其中，第四步是为了第五步的正确执行。&lt;/p&gt;

&lt;h3 id=&quot;读写tablet&quot;&gt;读写Tablet&lt;/h3&gt;

&lt;p&gt;上面我们谈了Bigtable的数据模型，如何寻找和分配Tablet，那么数据是怎么以（row，column，time）的格式被组织成Tablet的呢？论文中给出的流程图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Tablet Operation.png&quot; alt=&quot;Tablet Operation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;每个Tablet由若干个位于 GFS 上的 SSTable、一个位于内存内的MemTable以及一份Tablet Log组成。&lt;/p&gt;

&lt;p&gt;我们来解释一下这张图。为了保证系统可恢复，Google首先使用Table Log（即WAL）将客户端发出的写操作请求记录在磁盘中，那么，一旦系统崩溃，仍然可以从磁盘读取数据，继续执行命令。然后，相关的数据被放入位于内存中的Memtable中，因为内存的速度相当快，那么执行排序等操作就要快得多。当Memtable的大小达到设定的值后，它就会以SSTable的形式被存储到GFS中，这被称为Minor Compaction。&lt;/p&gt;

&lt;p&gt;客户端的读操作请求则要综合考虑Memtable和SSTable中的数据，如果Memtable中已经有需要读的数据，就无需读取SSTable。由于Memtable和SSTable都是有序的，所以读取的速度都相当快。&lt;/p&gt;

&lt;p&gt;在这里，虽然论文没有明确指出，我认为Memtable和SSTable的大小很可能是64MB。因为GFS将单个Chunk设置为64MB，那么为了最大化地利用磁盘空间，Memtable和SSTable的大小设置为这个值是相当合理的。&lt;/p&gt;

&lt;p&gt;由于SSTable中的数据有可能被标记为删除，那么我们需要定期对其进行处理，Bigtable将其称为Major Compaction。在这个过程中，Bigtable会将过期或者被删除的数据删除，并合并多个SSTable。这里似乎和GFS的Garbage Collection有点类似，但是我认为这可能是两个层面的活动。Bigtable清理的是单个Chunk中的数据，而GFS清理的是磁盘中的单个Chunk。&lt;/p&gt;

&lt;h3 id=&quot;优化&quot;&gt;优化&lt;/h3&gt;

&lt;p&gt;论文中提到，仅靠上述这些方法还不能达到要求的速度，因此，Bigtable还做了一些优化。&lt;/p&gt;

&lt;p&gt;第一，为了提高读取的速度，Bigtable使用布隆过滤器判断数据是否在某个SSTable中。&lt;/p&gt;

&lt;p&gt;第二，Tablet Server使用Scan Cache缓存SSTable返回的数据，在重复读时提高效率。使用Block Cache缓存从GFS读取的SSTable，这样在读取附近的数据时就无需从磁盘读取。&lt;/p&gt;

&lt;p&gt;第三，Bigtable把所有的写入操作都写入到同一个Bigtable Log文件中，而不是每个Server分配一个。同时，因为这个文件相当大，恢复起来很费事。Bigtable会对其进行排序并进行切分，每个Tablet Sever只需读取自己的那部分就可以了。&lt;/p&gt;

&lt;p&gt;第四，Bigtable允许针对特定的Column Family生成SSTable，同时进行压缩，以提高读取的效率。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;Bigtable重要的贡献是证明了在分布式的系统中，针对超大规模的数据量，使用排序大表的来设计数据库是可行的。这直接带动了LSM Tree的流行，在后来的HBase，LevelDB中都使用了这种方式处理数据。另外，Bigtable系统中Chubby的使用，还告诉工业界分布式协调组件的重要性，这也引导了Zookeeper的设计实现，而其仍然是今天的分布式系统中重要的组件。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">谷歌在2003到2006年间发表了三篇论文，《MapReduce:Simplified Data Processing on Large Clusters》，《Bigtable:A Distributed Storage System for Structured Data》和《The Google File System》介绍了Google如何对大规模数据进行存储和分析。这三篇论文开启了工业界的大数据时代，被称为Google的三驾马车。本文介绍Bigtable的相关内容。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">浅谈Google的三驾马车之GFS</title>
      <link href="http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BGFS" rel="alternate" type="text/html" title="浅谈Google的三驾马车之GFS" />
      <published>2020-08-08T18:00:00+08:00</published>
      <updated>2020-08-08T18:00:00+08:00</updated>
      <id>http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BGFS</id>
      <content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BGFS">&lt;p&gt;上图为古天乐——真·高富帅（GFS）&lt;/p&gt;

&lt;p&gt;谷歌在2003到2006年间发表了三篇论文，《MapReduce: Simplified Data Processing on Large Clusters》，《Bigtable: A Distributed Storage System for Structured Data》和《The Google File System》介绍了Google如何对大规模数据进行存储和分析。这三篇论文开启了工业界的大数据时代，被称为Google的三驾马车。本文介绍Google File System的相关内容。&lt;/p&gt;

&lt;h2 id=&quot;背景介绍&quot;&gt;背景介绍&lt;/h2&gt;

&lt;p&gt;在21世纪初，互联网上的内容，大多数企业需要存储的数据量并不大。但是Google不同，Google的搜索引擎的数据基于爬虫，而由于网页的大量增加，爬虫得到的数据也随之急速膨胀，单机或简单的分布式方案已经不能满足业务的需求，所以Google必须设计新的数据存储系统，其产物就是Google File System（GFS）。不过，在Google的设计中，为了尽可能的解耦，GFS仅负责数据存储而不提供类似数据库的服务。也就是说，GFS只存数据，而对数据的具体内容一无所知，自然也就不能提供基于内容的检索功能。所以，更进一步，Google开发了Bigtable作为数据库，向上层服务提供基于内容的各种功能。此外，Google 的搜索结果依赖于PageRank算法的排序，而该算法又需要一些额外的数据，比如某网页的被引用次数，所以他们还开发了对于的数据处理工具MapReduce，在读取了Bigtable数据的技术上，根据业务需求，对数据内容进行运算。其总体架构如下，GFS能充分利用多个Linux服务器的磁盘，并向上掩盖分布式系统的细节。Bigtable在GFS的基础上对数据内容进行识别和存储，向上提供类似数据库的各种操作。MapReduce则使用Bigtable中的数据进行运算，再提供给具体的业务使用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Google troika.png&quot; alt=&quot;Troika&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;google-file-system&quot;&gt;Google File System&lt;/h2&gt;

&lt;p&gt;GFS是三驾马车中最底层的组件，当然也是最复杂的，因为他直接和分布式的系统接触。在具体探讨实现细节之前，Google给出了一些设计前提和设计目标。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;由于使用的机器是廉价的商业化机器，那么机器崩溃被认为是一种常态。&lt;/li&gt;
    &lt;li&gt;系统存储以大文件为主，但也支持小文件。文件大小通常在100MB左右并且需要高效的操作几个GB的文件。&lt;/li&gt;
    &lt;li&gt;系统需要支持大规模的连续读取和小规模的随机读取，以及大规模的追加写。&lt;/li&gt;
    &lt;li&gt;高性能稳定的网络带宽比延迟更重要。&lt;/li&gt;
    &lt;li&gt;以及最重要的，能在分布式的系统上运行。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;在下面，我们根据具体的措施讨论如何实现以上目标。&lt;/p&gt;

&lt;h3 id=&quot;总体架构&quot;&gt;总体架构&lt;/h3&gt;

&lt;p&gt;首先我们来看看GFS的总体架构。在这个架构中，GFS采用了单Master（Single Master）的设计来简化系统的复杂度。Master负责两点，一是存储和维护Chunk Server和数据块的相关信息，二是处理客户端的请求。也就是说，Master并不存储任何具体的数据，这些数据被存在被称为Chunk Server的数据节点上。其中Chunk就是指数据块，在GFS中被固定为64MB大小，我想着可能跟第二个目标相关。每当数据需要被写入时，就更新GFS中的信息，并把数据封装成Chunk写入到数据库中，具体流程在下面会仔细介绍。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/GFS Architecture.png&quot; alt=&quot;GFS Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图演示了应用如何调用GSF进行读操作的具体流程&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;应用调用GFS Client的函数，要求其读取具体的文件/foo/bar，假设大小为50MB。&lt;/li&gt;
    &lt;li&gt;GFS Client根据Chunk的固定大小计算出/foo/bar的Chunk的Index，即64/50向上取整，Chunk Index=1。以及其在Chunk内的偏移量Byte Range[0，50]，并将File Name和Chunk Index作为参数发送给GFS Master&lt;/li&gt;
    &lt;li&gt;Master返回了对应的Chunk Handle（也就是Chunk的ID，上图中的2ef0）和Chunk Locations（Chunk Server和其副本的IP）&lt;/li&gt;
    &lt;li&gt;GFS Client根据返回的Chunk Locations找到最近的Chunk Server，然后根据Chunk Handle找到对应的Chunk，最后按照这个文件在Chunk中偏移量Byte Range读取文件。&lt;/li&gt;
    &lt;li&gt;Chunk Server按照其要求返回文件数据。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;整个的流程相当清晰，客户端负责将文件在File Namespace的位置交给Master，Master根据其位置返回对应的Chunk和Chunk Server，然后客户端再根据这些信息去拿数据。但是，这里有很巧妙的设计，就是GFS将所有数据传输的压力都放到Chunk Server上，而不是Master。假设由Master根据Chunk Handle去找数据并返回给客户端，那么这里就有会造成系统带宽压力增大。如果按照假设设计，Chunk Server将数据传回给Master后，Master还要将数据传回给客户端，也就是64MB的流量陡然翻倍成128MB。而实际的GFS不仅降低了Master带宽的压力，还把读取数据的压力均摊到每个Chunk Server上，降低了整个系统的压力。这些设计明显有助于实现目标三。&lt;/p&gt;

&lt;p&gt;通过对读取过程的分析，可以发现，GFS已经完全实现目标二中的大规模连续读和小规模随机读的要求。&lt;/p&gt;

&lt;h3 id=&quot;single-master&quot;&gt;Single Master&lt;/h3&gt;

&lt;p&gt;Master中保存三种信息（MATEDATA）&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;文件和chunk的namespace，即文件树形式的命名方式。&lt;/li&gt;
    &lt;li&gt;文件到chunk的映射，即每个文件需要哪几个Chunk来记录。&lt;/li&gt;
    &lt;li&gt;每一个chunk的具体位置。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;这些信息都平时都存在内存中，以此提高响应速度。这似乎只能存储很少的信息，但是，Master只需要64byte的空间就能记录64MB的Chunk的相关信息，也就是说，128MB的内存能存储1PB的数据的相关信息。不过，为了保证Master能在崩溃后恢复，在执行改变前两种信息的操作前需要使用WAL的形式定期地保存到磁盘上。这样，在Master恢复的时候，就能通过磁盘上的WAL来重建前两种信息。而Chunk的位置则可以通过Heartbeats的形式查询并更新，这样不仅加速了Master的恢复，也能使GFS的配置更加灵活。&lt;/p&gt;

&lt;h3 id=&quot;写数据write&quot;&gt;写数据（Write）&lt;/h3&gt;

&lt;p&gt;由于每个Chunk Server都有备份的副本，所以写操作要比读操作稍微复杂一点。具体的流程如下图所示&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/GFS Write.png&quot; alt=&quot;GFS Write&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;客户端向 Master 询问目前哪个Chunk Server持有该Chunk的Lease&lt;/li&gt;
    &lt;li&gt;Master 向客户端返回Primary Replica和其他Replica的位置&lt;/li&gt;
    &lt;li&gt;客户端将数据推送到所有的Replica上。Chunk Server会把这些数据保存在缓冲区中，等待所有 Replica 都接收到数据。&lt;/li&gt;
    &lt;li&gt;客户端发送写请求给 Primary，Primary 为来自各个客户端的修改操作选定执行序列号，并按顺序地应用于其本地存储的数据。&lt;/li&gt;
    &lt;li&gt;Primary 将写请求转发给其他 Secondary Replica，Replica 们按照相同的顺序应用这些修改&lt;/li&gt;
    &lt;li&gt;Secondary Replica 响应 Primary，示意自己已经完成操作。&lt;/li&gt;
    &lt;li&gt;Primary 响应客户端，并返回该过程中发生的错误&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里要说明的是，每个Chunk被复制到多个Chunk Server中，以此避免单个节点崩溃可能造成的数据损失。而这些Chunk Server中负责相应写操作的Chunk Server被称为Primary Replica，其余的被称为Secondary Replica，接受来自Primary Replica的请求。而为了保证写入数据的一致性，只能有一个Primary Replica，这里的唯一性就由Lease来实现。当需要写入数据时，Master会将特定的Lease分配给某个Replica，拿到Lease的这个Replica就称为了Primary Replica。&lt;/p&gt;

&lt;p&gt;我们再仔细分析整个写的的流程。我们可以注意到为了保证数据的一致性，一次写入只能有一个Primary Replica，同样地，在正式写入数据之前要求所有Secondary Replica缓存数据也是为了一致性。这里的思想类似于WAL，将要执行的操作先保存下来，这样万一崩溃了也能从磁盘读入数据，继续执行未完成的操作。等待所有Secondary Replica完成操作后再相应客户端也是为了保证数据的一致性。&lt;/p&gt;

&lt;p&gt;和读操作类似，在写操作中，为了降低Master的压力，所有的数据由客户端发向Chunk Server。&lt;/p&gt;

&lt;h3 id=&quot;追加append&quot;&gt;追加（Append）&lt;/h3&gt;

&lt;p&gt;为了提升性能，GFS提供并推荐使用追加操作修改文件。它与写操作不同之处仅仅在于它向文件尾端添加数据而不是覆盖，它的流程也he 写操作大同小异&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;客户端将数据推送到每个 Replica，然后将请求发往 Primary&lt;/li&gt;
    &lt;li&gt;Primary 首先判断将数据追加到该块后是否会令块的大小超过上限：如果是，那么 Primary 会为该块写入填充至其大小达到上限，并通知其他 Replica 执行相同的操作，再响应客户端，通知其应在下一个块上重试该操作&lt;/li&gt;
    &lt;li&gt;如果数据能够被放入到当前块中，那么 Primary 会把数据追加到自己的 Replica 中，拿到追加成功返回的偏移值，然后通知其他 Replica 将数据写入到该偏移位置中&lt;/li&gt;
    &lt;li&gt;最后 Primary 再响应客户端&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;特别值得一提的是，GFS保证追加操作至少被执行一次（at least once），这意味着追加操作可能被执行多次。当追加操作失败时，为了保证偏移量，GFS会在对应的位置填充重复的数据，然后重试追加。也就是说，GFS不保证在每个副本中的数据完全一致，而仅仅保证数据被写入了。&lt;/p&gt;

&lt;h3 id=&quot;数据一致性&quot;&gt;数据一致性&lt;/h3&gt;

&lt;p&gt;在GFS中，由于分布式系统的原因，不同节点间处理请求和存储数据速度不一致导致了客户算读取数据时可能出现各种不同的情况。&lt;/p&gt;

&lt;p&gt;文件的数据修改则相对复杂。在讲述接下来的内容前，首先我们先明确，在文件的某一部分被修改后，它可能进入以下三种状态的其中之一：&lt;/p&gt;

&lt;p&gt;在文件的某一部分被修改后，它可能进入以下三种状态的其中之一：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;客户端读取不同的 Replica 时可能会读取到不同的内容，那这部分文件是不一致的（Inconsistent）。&lt;/li&gt;
    &lt;li&gt;所有客户端无论读取哪个 Replica 都会读取到相同的内容，那这部分文件就是一致的（Consistent）。&lt;/li&gt;
    &lt;li&gt;所有客户端都能看到上一次修改的所有完整内容，且这部分文件是一致的，那么我们说这部分文件是确定的（Defined）。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;也就是说，在一致性和强度上，Defined&amp;gt;Consistent&amp;gt;Inconsistent。&lt;/p&gt;

&lt;p&gt;在修改后，一个文件的当前状态将取决于此次修改的类型以及修改是否成功。具体来说：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;如果一次写入操作成功且没有与其他并发的写入操作发生重叠，那这部分的文件是确定的（同时也是一致的）。&lt;/li&gt;
    &lt;li&gt;如果有若干个写入操作并发地执行成功，那么这部分文件会是一致的但会是不确定的：在这种情况下，客户端所能看到的数据通常不能直接体现出其中的任何一次修改。也就是说，操作成功执行了，但是有的操作的数据改变被覆盖了，客户端看不到被覆盖的数据改变。&lt;/li&gt;
    &lt;li&gt;失败的写入操作会让文件进入不一致的状态。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;论文中给出了总结的表格：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/consistency.png&quot; alt=&quot;consistency&quot; /&gt;&lt;/p&gt;

&lt;p&gt;至于为什么追加（Append）是Defined但是有可能是不一致是因为：在追加写操作失败时，为了保证数据的偏移，可能为填充重复的数据，此时导致了不一致。但是失败的操作会被再次执行，此时又保证了数据的一致性。而由于使用的是追加，所以任何数据的改动都可以观察到，所以是Defined。&lt;/p&gt;

&lt;h3 id=&quot;快照snapshot&quot;&gt;快照（Snapshot）&lt;/h3&gt;

&lt;p&gt;这里的快照的目的不同于Raft中的压缩，它仅仅驶出为了生成一个新的Replica，可以看做是一个简单的复制操作。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;在 Master 接收到快照请求后，它首先会撤回相关Chunk  Server的 Lease，保证在创建快照的过程中，客户端对不会对相关的Chunk Server进行写操作或者追加操作。&lt;/li&gt;
    &lt;li&gt;当Lease收回后，Master会先将相关的改动写入日志，然后对自己管理的命名空间进行复制操作，复制产生的新记录指向原本的 Chunk。&lt;/li&gt;
    &lt;li&gt;当有客户端尝试对新的Chunk Server进行写入时，Master 会注意到这个 Chunk 的引用计数大于1（可能是一个标记）。此时，Master 会为要读取的Chunk生成一个Handle，然后通知所有持有这些 Chunk 的 Chunk Server 在本地复制并使用出新的 Chunk，然后再返回给客户端&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;我想GFS提供快照的原因可能是为了在一个副本损坏时，从Primary Replica或者其他副本复制数据，然后用新的节点代替损坏的节点。&lt;/p&gt;

&lt;h3 id=&quot;垃圾回收&quot;&gt;垃圾回收&lt;/h3&gt;

&lt;p&gt;当GFS收到删除文件的请求时，它并不直接删除文件，而是给文件打上删除的时间戳并将其命名为掩藏文件（文件开头加”.”）。在周期性扫描过程中，当发现文件的删除时间超过设定期限后，才真正地将文件删除。Google认为其有以下优点&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;对于大规模的分布式系统来说，这样的机制更为&lt;strong&gt;可靠&lt;/strong&gt;：在 Chunk 创建时，创建操作可能在某些 Chunk Server 上成功了，在其他 Chunk Server 上失败了，这导致某些 Chunk Server 上可能存在 Master 不知道的 Replica。除此以外，删除 Replica 的请求可能会发送失败，Master 会需要记得尝试重发。相比之下，由 Chunk Server 主动地删除 Replica 能够以一种更为统一的方式解决以上的问题&lt;/li&gt;
    &lt;li&gt;这样的删除机制将存储回收过程与 Master 日常的周期扫描过程合并在了一起，这就使得这些操作可以以批的形式进行处理，以减少资源损耗；除外，这样也得以让 Master 选择在相对空闲的时候进行这些操作&lt;/li&gt;
    &lt;li&gt;用户发送删除请求和数据被实际删除之间的延迟也有效避免了用户误操作的问题&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;高可用&quot;&gt;高可用&lt;/h3&gt;

&lt;p&gt;论文中主要探讨了三个方面的高可用，分别是Master，Chunk Server和数据完整性。&lt;/p&gt;

&lt;p&gt;当Master崩溃时，有两种情况，一是进程崩溃但是服务器没有，这种情况下，重开一个进程即可。另一种情况是整个机器崩溃了，在GFS还有被称为Shadow Master的机器，复制Master节点的信息。当Master机器崩溃后，Shadow Master会接替Master进行服务，但是仅提供读取操作的服务，不能更改信息。&lt;/p&gt;

&lt;p&gt;当Chunk Server崩溃时，Master会安排新的Replica代替它，从其他Replica复制原始数据。而当Chunk Server恢复时，由于数据不同步，不应该提供服务，Master就需要区别新旧Chunk Server。GFS使用版本号来标记这个信息，每分配以此Lease，版本号就会增加并同步给其他Replica，而由于Chunk Server崩溃后不能更新，我们就能从版本号上辨别新旧。&lt;/p&gt;

&lt;p&gt;由于写操作和追加操作可能不成功，所以数据可能会损坏。GFS使用检验和检查是否损坏，每次客户端读取数据时，Chunk Server都会检查检验和，一旦发现损坏，就会向Master报告。Master则会将请求发送给其他Replica，并从其他Replica复制数据到该机器。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;2003年GFS的横空出世具有划时代的意义，它标志着学术上的分布式理论和一些实验性质的尝试在工业界有了大规模商用的案例，尤其还是在Google这样的公司。它的系统设计在后续的系统中被屡次参考复用，而其设计确实有独到之处，比如Master负责控制流而完全将数据流从其剥离，这样的设计不能不说是优雅。这篇论文催生了HDFS，至今仍被许多公司使用，足以可见其影响力之大。哪怕这篇论文距离今天已经有快20年的时间，GFS在Google内部迭代多次，但是其设计仍然值得每一个分布式程序员学习。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">谷歌在2003到2006年间发表了三篇论文，《MapReduce:Simplified Data Processing on Large Clusters》，《Bigtable:A Distributed Storage System for Structured Data》和《The Google File System》介绍了Google如何对大规模数据进行存储和分析。这三篇论文开启了工业界的大数据时代，被称为Google的三驾马车。本文介绍Google File System的相关内容。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">浅谈Amazon Aurora</title>
      <link href="http://localhost:4000/%E6%B5%85%E8%B0%88Amazon-Aurora" rel="alternate" type="text/html" title="浅谈Amazon Aurora" />
      <published>2020-08-02T18:00:00+08:00</published>
      <updated>2020-08-02T18:00:00+08:00</updated>
      <id>http://localhost:4000/%E6%B5%85%E8%B0%88Amazon%20Aurora</id>
      <content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Amazon-Aurora">&lt;p&gt;上图为极地极光&lt;/p&gt;

&lt;p&gt;Amazon在2017年的SIGMOD上发表了《Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases》并在对Amazon Aurora进行了介绍，简要描述了他们由于对传统MySQL性能的不满，而设计了Aurora来代替，其性能有相当大的提升。从时间和公司我们就可以看出，这是比较新的工业界的解决方案，有很高的学习参考价值。&lt;/p&gt;

&lt;h2 id=&quot;aurora的总体架构&quot;&gt;Aurora的总体架构&lt;/h2&gt;

&lt;p&gt;虽然论文中在结尾时才对其作出总结，但是在开头就点名其架构，再步步深入会更加合理。下面是Aurora的总体架构图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Aurora Architecture.png&quot; alt=&quot;Aurora Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;需要指出的是，由于Aurora是为了代替MySQL，而MySQL用于关系型数据库，所以Aurora仅负责处理关系型数据库的服务，即RDS（Relational Database Service）。我们其实可以从图中看出相当多的信息，Aurora仅有Primary RW（Read/Write） DB一个主节点用于处理写请求，而其余的则为从节点Secondary RO（Read-Only） DB用于处理读请求，论文中指出Secondary RO DB可以多达15个。另外，每个Aurora配备六个存储节点，其中有两个节点使用Amazon Simple Storage Service（S3）存储技术进行备份，而剩余4个节点则直接存储在本地的SSD上。&lt;/p&gt;

&lt;p&gt;用户的应用通过Customer VPC接入，然后可以读写位于不同AZ（Availability Zone）的数据库。而不同的AZ分布于全球的不同的Region中。当用户的请求发送到Primary RW DB时，RDS HM（Host Manager）会检测到请求，并调用Aurora进行相应的操作。如果是写操作，则将相关信息发送给Secondary RO DB进行备份，同时将命令写入存储节点。如果是读操作，则直接从存储节点读取数据返回。&lt;/p&gt;

&lt;h2 id=&quot;使用传统mysql遇到的问题&quot;&gt;使用传统MySQL遇到的问题&lt;/h2&gt;

&lt;p&gt;Amazon在日常开发和维护中发现，计算能力和存储性能已经不再是其工作的瓶颈了，取而代之的是网络的流量。其实对于Amazon来说，只要有钱，CPU能用最好的就能解决计算能力的问题，机械硬盘不够用固态硬盘，固态硬盘不够就上内存，存储性能也解决了，但是网络的延迟靠大带宽是很难解决的，而拉近机房位置也是有上限的，必须要从业务逻辑和服务组件上找问题。所以他们发现了MySQL在分布式系统中消耗了大量的流量，还提高了延迟。具体如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/MySQL network.png&quot; alt=&quot;MySQL network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从图中我们可以看出，传统的MySQL如果想要执行一次写入操作必须经历以下几步：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;主节点将数据写入EBS1&lt;/li&gt;
    &lt;li&gt;EBS1将数据写入备份镜像EBS2&lt;/li&gt;
    &lt;li&gt;主节点将相关数据发送给从节点&lt;/li&gt;
    &lt;li&gt;从节点将数据写入EBS3&lt;/li&gt;
    &lt;li&gt;EBS3将数据写入EBS4&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;其中，第1,3,5步是串行的，也就是说，只有第1步完成了，才能执行第3步，第3步完成了才能执行第5步。这无疑增加了服务器返回数据的延迟。另外传统的MySQL在写入和传输数据时还需要很多的额外信息，这又增加了网络带宽的消耗。也就是说，MySQL的使用在分布式系统产生了两个问题&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;应答延迟太高&lt;/li&gt;
    &lt;li&gt;消耗网络带宽太多&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;所以当Amazon发现使用传统MySQL的弊端之后，决定设计新的组件来代替MySQL以解决上述两个问题。&lt;/p&gt;

&lt;h2 id=&quot;the-log-is-the-database&quot;&gt;The Log Is The Database&lt;/h2&gt;

&lt;p&gt;上面我们提到，MySQL在同步数据的过程中发送的信息太多，这该怎么办呢？Amazon也算是家大业大，直接自己重新设计标准，以往的数据库是真的数据库，现在他们用WAL也就是Log来整合所有有用的信息并删去无用的信息，既减少了数据传输量又保证了需要保留的信息。同时，他们使用了链式复制结构代替主从结构，简化了保证数据一致性的复杂度。具体架构如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Aurora Network.png&quot; alt=&quot;Aurora Network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以三个副本为例，当位于AZ1的主节点收到写请求后，它将请求的相关数据直接写入六个存储节点中，然后，将数据和一些额外的信息通过链式复制结构传递给位于AZ2和AZ3其他节点。和上图进行对比，明显可以看到主从节点之间网络通信中传输的数据减少了，主节点向存储节点写入数据时也从五种数据变为一种。这里要特别指出的是，此处的数据已经从MySQ定义的Log变为Amazon为Aurora量身定制的Log。由于需要传输数据量的减少，同步所消耗的网络带宽也大幅地减少了。&lt;/p&gt;

&lt;p&gt;另外，因为主节点负责将Log写入存储节点，而从节点仅存储Log不需要负责写入存储节点，这样就减少了在MySQL中额外的第四步和第五步操作的时间。而MySQL中的两级EBS存储操作也由一级Quorum的代替，就像上一篇文章提到的，两级存储的时间是两次操作的时间之和，而一级的Quorum操作的时间则是取决于Quorum中最长的应答时间。这样，Aurora也优化了应答延迟的时间。&lt;/p&gt;

&lt;p&gt;在上一篇文章中我们提到，链式复制仅仅适用于节点较少且物理位置较近的情况。很巧的是，Amazon提供的服务中副本不会超过15个，而经典的情况仅有3个，而虽然不同AZ可能会跨节点，但是Amazon实在有钱，能让AZ之间的延迟低于2ms。在这种情况下，使用链式复制实在合适不过，还大大降低了保证共识的复杂度，简直是完美的设计。&lt;/p&gt;

&lt;h2 id=&quot;storage-node&quot;&gt;Storage Node&lt;/h2&gt;

&lt;p&gt;上面，我们提到主节点将Redo Log写入存储节点。但是，此时Redo Log还未执行，需要在存储节点中执行相应的操作后才算真正完成。下面，我们再来看看Redo Log到达存储节点以后需要进行哪些操作。论文中给出的流程图如下。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Aurora Storage Node.png&quot; alt=&quot;Aurora Storage Node&quot; /&gt;&lt;/p&gt;

&lt;p&gt;具体的流程解释如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;存储节点通过Incoming Queue接受主节点的Log。&lt;/li&gt;
    &lt;li&gt;存储节点将Log存到本地硬盘后向主节点发送ACK，用以确认Quorum。&lt;/li&gt;
    &lt;li&gt;由于网络的不可靠和Quorum机制，当前存储节点可能缺失了部分Log。在这一步，它将Log排序并找出缺失的Log。&lt;/li&gt;
    &lt;li&gt;通过和其他存储节点进行交换信息，将缺失的Log复制到本地，将所有Log填充完整。&lt;/li&gt;
    &lt;li&gt;到目前为止，系统中存储的仍是Log而非用户需要数据，这一步执行Log对应的操作，并写入数据库中。&lt;/li&gt;
    &lt;li&gt;定期地将数据存为快照并存入Amazon S3中。&lt;/li&gt;
    &lt;li&gt;定期地进行垃圾收集，删除过期数据。&lt;/li&gt;
    &lt;li&gt;用CRC定期检验数据。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;从流程中我们可以看到，只有第一步和第二步可能影响应答延迟，其余的步骤都由存储节点在后台执行。这样一来，因为无需等待执行完毕，应答延迟就进一步降低了。&lt;/p&gt;

&lt;h2 id=&quot;读写操作&quot;&gt;读写操作&lt;/h2&gt;

&lt;p&gt;Amazon在设计Log时，为了实现一些功能给它添加了一些标记，具体如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;LSN：Log Sequence Number，相当于Log的自增主键，类似于Raft中的Index。&lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;VCL：Volume Complete LSN，受到Quorum承认的最大LSN。&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;CPLs：Consistenc yPoint LSNs，单个存储节点中已经收到ACK的最大LSN，所以每个节点各一个&lt;/li&gt;
    &lt;li&gt;VDL：Volume Durable LSN，已经持久化最大的LSN，也就是CPLs中最大的LSN&lt;/li&gt;
    &lt;li&gt;SCL：Segment Complete LSN，由每个段维护，代表段中已经持久化的最大LSN&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;在读写，复制和提交等操作中，Aurora会使用这些标记实现对应功能。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;写操作&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们在之前的文章中提到，如果未执行的Log积压过多会产生很不好的后果。所以在写操作时，Aurora会设置VDL+N作为未分配LSN的上限，通过设置N的值来限制未写入磁盘的Log的条数。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;读操作&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;为了提高效率，Aurora会在缓存中先查找是否有需要读取的数据，如果没有，再置换页面。在这里，Aurora要求置换页面中的LSN&amp;gt;VDL以确保数据为最新版本。这保证了所有页面的更新都已经持久化到日志并且在缓存区没有该数据页的情况下，可以根据 VDL 获取最新版本数据。&lt;/p&gt;

&lt;h2 id=&quot;fault-tolerance&quot;&gt;Fault-Tolerance&lt;/h2&gt;

&lt;p&gt;Aurora 将数据库文件切分成 10GB 大小的段（Segment）。在崩溃恢复的时候，Aurora要通过Quorum读得到VDL，并将大于此的Log阶段。由于在写操作时设置了LSN的上限，所以可以将需要Redo Log的LSN上限设置为VDL+N。然后重做已经标记的Log，就能恢复到初始状态，Aurora实验显示这个过程相当地快。&lt;/p&gt;

&lt;h2 id=&quot;不一样的quorum&quot;&gt;不一样的Quorum&lt;/h2&gt;

&lt;p&gt;我们上面提过，Aurora的六个存储节点部署在3个AZ中，每个AZ运行两个存储节点。Amazon考虑到可能整个AZ挂掉，导致两个存储节点崩溃，而AZ又有可能在同一个Region中，所以Aurora考虑的最坏情况是一个AZ崩溃加上一个存储节点崩溃，即AZ+1。&lt;/p&gt;

&lt;p&gt;Aurora提出了以下两个要求：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;在AZ+1崩溃的情况下不丢失数据，也就是保证读数据能力。&lt;/li&gt;
    &lt;li&gt;在AZ崩溃的情况下保证写数据能力。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;于是Aurora提出了读写两种情况的Quorum。在写情况下，需要六个节点中的四张票，即4/6。在读情况下，仅需要六个节点中的三张票，即3/6.&lt;/p&gt;

&lt;p&gt;很明显，写操作的Quorum和我们之前在Paxos和Raft中讨论的Quorum是一致的，也是2f+1需要f+1张票。而由于写操作每次至少写入四个节点，那么根据抽屉原理，每两次写操作至少有一个节点重复，那么读操作无论读哪一半都能在三个节点中读取到最新的全部数据。以此类推，哪怕一半的节点崩溃，Aurora也能读取到最新最全的数据。&lt;/p&gt;

&lt;p&gt;但是，要特别指出的是，读Quorum的要求仅仅在恢复时才使用，正常读是不需要的。&lt;/p&gt;

&lt;p&gt;从这里来看，其实这个Quorum也就是Raft中2f+1个节点容许f个崩溃的另一种说法。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;Amazon Aurora中描述的技术看起似乎很通用，使用WAL代替MySQL的信息，在存储节点执行命令而不是在本机执行，使用Chain Replication等等。但是能将这些技术恰到好处地使用在实际的系统中，并进行优化才是大厂的技术底蕴。比如Log的设计这一块，论文中就介绍地相当模糊，读写操作的细节也没有纰漏。这篇论文恐怕只算是对Aurora的惊鸿一瞥，真的想了解实现细节还得去Amazon内部看看。毕竟，Aurora在每个事务的IO花费的1/8，而事务处理量是MySQL的35倍，这可不是简单的系统设计就能完成的。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">Amazon在2017年的SIGMOD上发表了论文对Amazon Aurora进行了介绍，简要描述了他们由于对传统MySQL性能的不满，而设计了Aurora来代替，其性能有相当大的提升。从时间和公司我们就可以看出，这是比较新的工业界的解决方案，有很高的学习参考价值。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">浅谈Primary-Back Replication和Chain Replication</title>
      <link href="http://localhost:4000/%E6%B5%85%E8%B0%88Primary-Back-Replication%E5%92%8CChain-Replication" rel="alternate" type="text/html" title="浅谈Primary-Back Replication和Chain Replication" />
      <published>2020-07-26T18:00:00+08:00</published>
      <updated>2020-07-26T18:00:00+08:00</updated>
      <id>http://localhost:4000/%E6%B5%85%E8%B0%88Primary-Back%20Replication%E5%92%8CChain%20Replication</id>
      <content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Primary-Back-Replication%E5%92%8CChain-Replication">&lt;p&gt;上图为Google提供的贪吃蛇游戏&lt;/p&gt;

&lt;p&gt;前面两篇文章，我们讨论了分布式系统中为了维护一致性所使用的共识性算法Paxos和Raft。这些算法保证了各机器之间数据的一致性，但是Paxos算法针对的是平等的Replication策略，而Raft算法针对的是Primary-Back Replication策略。Replication策略和共识性算法的目的不同，它的设计是为了实现容错（Fault-Tolerance），即在一部分机器不可用后，仍能保证正常提供服务。下面，我将简要描述一下6.824中讨论的两种复制模型Primary-Back Replication和Chain Replication。&lt;/p&gt;

&lt;h2 id=&quot;机器同步&quot;&gt;机器同步&lt;/h2&gt;

&lt;p&gt;为了能够实现集群能够及时接管服务，并提供相同的服务，集群需要保证不同机器之间的数据一致性。我们上面提到，要实现共识需要使用Paxos或者Raft算法，而我们需要对什么东西达成共识呢？这就要提到实现机器同步的两种方法，分别是State Transfer和Replicated state machine，其中差别如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;State Transfer：主机器将本身的状态变化全部传送给其他机器。&lt;/p&gt;

  &lt;p&gt;Replicated State Machine：由于业务的很多情况符合状态机的定义，所以只要保证所有机器初试状态相同，那么以相同的顺序执行相同指令后，它们的状态也是相同的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;举个例子，客户端发送{X=1，Y=X}的指令给集群。在State Transfer中，主机器执行这个操作后，将它发生变化的数据和值发送给其他机器，也就是{X=1，Y=1}，然后其他机器将自身的数据修改为对应的值，以此实现各机器的数据一致性。而在Replication State Machine中，主机器发送{X=1，Y=X}的指令给其他机器，要求其他机器也执行这一操作，按照状态机的状态变化，也能实现数据一致性。&lt;/p&gt;

&lt;p&gt;State Transfer的优点是不会消耗其他机器的计算能力，但是在实际情况下，发生变化的数据量比较大，它对集群的带宽和延迟要求都很搞。Replicated State Machine由于只发送指令，所以传输的数据量比较小，但是它要求各机器在本地执行指令，所以对机器的计算能力有一定要求。根据实际情况，一般集群中各机器的性能相当，所以计算能力不会成为瓶颈，而为了保证快速响应客户要求，必须尽量减少传输的延迟，State Transfer由于一次性传输的数据量大，所以在网络的传输延迟上很难实现要求。因此Replicated State Machine成为实际上的解决方案。&lt;/p&gt;

&lt;h2 id=&quot;vmft&quot;&gt;VM—FT&lt;/h2&gt;

&lt;p&gt;VMware在&lt;a href=&quot;http://nil.csail.mit.edu/6.824/2017/papers/vm-ft.pdf&quot;&gt;《The Design of a Practical System for Fault-Tolerant Virtual Machines》&lt;/a&gt;中介绍了基于虚拟机实现的Primary-Backup Replication的容错解决方案。其本质就是使用Replicated State Machine实现主从的数据一致性。&lt;/p&gt;

&lt;h3 id=&quot;deterministic-replay&quot;&gt;Deterministic Replay&lt;/h3&gt;

&lt;p&gt;在VM-FT中，它们将指令在副机器的执行称为Deterministic Replay。由于和数据库软件不同，有一些系统命令和时间息息相关，比如中断和时间戳，考虑到传输延迟，这就会使命令执行的结果不同，从而导致数据的不一致。所以这就给系统设计带来一些问题，论文将此分为三个目标&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;正确收集所有的非确定（Non-Deterministic）事件和操作，保证有在执行时有足够的信息在Replay时来修正操作&lt;/li&gt;
    &lt;li&gt;正确地在副机器上Replay非确定的事件和操作&lt;/li&gt;
    &lt;li&gt;保证以上两点不会降低机器性能&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;由于这个系统运行在VMware上，所以VMware可以收集所有它想要的信息并将其存储在文件中，能够正确地实现第一个目标。然后，主机器通过Logging Channel将需要执行的操作和相关的补充信息传递给副机器，因此可以保证第二个目标。在实际的测试中，VMware的实现也能够保证机器的性能，满足了第三点。&lt;/p&gt;

&lt;h3 id=&quot;ft-protocol&quot;&gt;FT Protocol&lt;/h3&gt;

&lt;p&gt;由于主从机器通过Logging Channel进行通信，所以其中会产生一定的延迟而导致问题。假设主机器在收到请求后，在本地执行了该请求并将其传送给副机器，同时将结果反馈给客户端，但是副机器由于一些问题没有成功执行该指令，此时主从机器的数据是不一致的。如果主机器在此时崩溃，集群交由副机器负责，那么客户端再次查询数据就会得到不一样的结果。&lt;/p&gt;

&lt;p&gt;因此，VMware提出了如下的要求：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;输出规则&lt;/strong&gt;：主机器只有在副机器已经接收并返回了该指令的ACK 后，才能回复客户端。其中，副机器必须成功执行了该指令后才能回复ACK。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其实就是要求主机器在副机器也执行完指令后，实现了整个系统的一致性，再回复客户端。其具体流程如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/FT Protocol.png&quot; alt=&quot;FT Protocol&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;错误检测和恢复&quot;&gt;错误检测和恢复&lt;/h3&gt;

&lt;p&gt;为了检测错误，VM-FT使用了以下两种手段&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;使用定期发送UDP来实现各机器间的Heartbeating&lt;/li&gt;
    &lt;li&gt;由于整个系统基于VMware，所以可以通过检测Logging Channel的流量来查看是否正常工作&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;第一种方法和Raft中使用的类似，都是通过周期性地Heartbeats来检测是否活跃。第二种方法基于系统不断接受客户端请求的事实，只要客户发送了请求，为了实现主从同步，两者必须通过Logging Channel发送指令或者返回ACK，而如果相当长的时间内没有流量的话，就说明至少主机器可能已经崩溃。&lt;/p&gt;

&lt;p&gt;一旦检测到主机器崩溃，那么副机器就会将Logging Channel中所有的指令执行完毕，以此到达主机器崩溃前的状态，然后在这个状态下回复客户端。而如果副机器崩溃了，主机器仅仅需要不再发送同步消息即可。&lt;/p&gt;

&lt;h3 id=&quot;logging-channel优化&quot;&gt;Logging Channel优化&lt;/h3&gt;

&lt;p&gt;为了保证主从的一致性，VM-FT还对Logging Channel进行了优化。如果副机器执行很慢，那么Logging Channel中的留存的信息过多，甚至填满了预留的空间，那么收到客户端请求时，主机器必须等待有空余空间才能发出请求，这使得客户端的体验很差。另一方面，如果主机器崩溃，副机器为了赶上主机器进度而需要花费的时间过多也会影响系统。所以VM-FT在通信中添加额外的信息，以此检测主从机器在执行同一指令上产生的延迟。如果这个延迟过大，VM-FT会减少分配给主机器的CPU算力，增加副机器的CPU算力，以此保证主从机器间的相对同步。&lt;/p&gt;

&lt;h3 id=&quot;brain-split&quot;&gt;Brain-Split&lt;/h3&gt;

&lt;p&gt;假设两台机器都能正常工作，但是由于通信原因，他们不能通过Heartbeat来确认对方正常工作，那么主机器仍然保持主机器的角色，而副机器则会将自己晋升为主机器，此时系统中有两个主机器，产生了脑裂（Brain-Split）现象。很不幸，这种情况不能通过系统自身解决，因为他们不能正常沟通，而在VM-FT中也没有使用基于Quorum的选举机制，所以只能通过外部解决。&lt;/p&gt;

&lt;p&gt;在VM-FT中，由于主从共享磁盘，所以可以通过磁盘这个中介来解决。可以在磁盘中记录当前是否有主机器，如果有机器想要成为主机器，那么它必须访问磁盘，读取这个字段，确认当前没有主机器，才能成为主机器。在论文中，这个过程被称为test-and-set 。而在分布式的系统中，则必须通过第三方服务器才能实现这一功能。&lt;/p&gt;

&lt;h3 id=&quot;vm-ft总结&quot;&gt;VM-FT总结&lt;/h3&gt;

&lt;p&gt;这篇论文发表于2010年，虽然距离现在有一段时间了，但是仍能看到后续设计的一些影子。比如Logging Channel中设计了Buffer来保存指令，这个设计其实就相当于WAL，它对Brain-Split的解决方案在今天仍然实用。不过，由于整体的实现是基于VMware，所以和实际的分布式系统仍然有一点差别。总而言之，我们对这篇论文最大的学习是对主从设计系统的简单了解。&lt;/p&gt;

&lt;p&gt;遗憾的是，这篇文章没有讨论多个副机器的情况，也就是说，没有使用Paxos算法实现多个副机器的一致性。&lt;/p&gt;

&lt;h2 id=&quot;craq&quot;&gt;CRAQ&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://pdos.csail.mit.edu/6.824/papers/craq.pdf&quot;&gt;《Object Storage on CRAQ High-throughput chain replication for read-mostly workloads》&lt;/a&gt;使用了一种类似于链表的复制模型Chain Replication。和Raft以及上面提到的主从模型不同，这个模型用很简单的方法就保证了数据的一致性。&lt;/p&gt;

&lt;h3 id=&quot;chain-replication&quot;&gt;Chain Replication&lt;/h3&gt;

&lt;p&gt;Chain Replication将每个结点连接成链表。其中写请求只能由头结点处理，读请求只能由尾节点负责。具体流程如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cr.png&quot; alt=&quot;cr&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;写请求：&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;头节点接受写请求，执行完毕后将其转发给后一个节点&lt;/li&gt;
    &lt;li&gt;所有节点都执行相关指令，同时将请求转发到下一个节点&lt;/li&gt;
    &lt;li&gt;等到尾节点也执行完相关指令后，会给上一个节点发送ACK&lt;/li&gt;
    &lt;li&gt;所有节点反向发送ACK，直到头节点&lt;/li&gt;
    &lt;li&gt;头节点收到ACK后，将结果返回给客户端&lt;/li&gt;
  &lt;/ol&gt;

  &lt;p&gt;读请求：&lt;/p&gt;

  &lt;p&gt;​	读请求只能由尾节点处理，尾节点返回当前数据&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们可以看到，只有当所有节点都执行完相关操作后，写请求才能得到结果，能保证写的一致性。由于尾节点是最后更新数据的节点，所以尾节点中的数据就是当前系统最新的一致性数据，读请求总能得到已经达成一致的数据。这个模型很简单地实现了读写的数据一致性，但是问题是，所有的读操作都由尾节点来处理，相当于整个系统的读取压力都来到了尾节点，尾节点很可能崩溃。所以这篇论文提出了一些改进。&lt;/p&gt;

&lt;h3 id=&quot;chain-replication-with-apportioned-queriescraq&quot;&gt;Chain Replication with Apportioned Queries（CRAQ）&lt;/h3&gt;

&lt;p&gt;这篇论文为了减少尾节点的压力，允许所有结点处理读请求，也就是名字中Apportioned Queries的含义。但是，由于读请求和写请求时并发的，有可能出现结点数据不一致时的读请求。所以这里还需要额外的处理。&lt;/p&gt;

&lt;p&gt;首先考虑最简单的情况，即整个系统数据保持一致的情况。这时，无论从哪个节点读取，数据都是正常的。示意图如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/craq clean read.png&quot; alt=&quot;craq&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其次，我们来考虑在处理写请求的同时，处理读请求的情况。其示意图如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/craq dirty read.png&quot; alt=&quot;craq read&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图中，头节点接受了写请求，并将其传送给第二个节点，第二个节点已经处理完毕。但此时，第三个节点和尾节点都还未进行相应的处理，导致前两个节点和后两个节点的数据不一致。CRAQ通过为数据添加版本并存储多个版本的数据来解决这一问题。&lt;/p&gt;

&lt;p&gt;一次完整的写请求完成后，整个系统仍然能保证数据一致性，唯一不能保证的就是上图中写到一半的情况。其中，已经处理写请求的节点和尾节点数据不一致，而未处理写请求的节点则保持一致。所以，CRAQ要求处理完写请求的节点把自己标记为Dirty，而未处理的则无需标记。另一方面，尾节点始终存储能被整个系统承认的数据，即最新的保持了一致性的数据。所以，为了确定当前应该返回的数据，Dirty节点应该询问尾节点应当返回数据的版本。&lt;/p&gt;

&lt;p&gt;这张图中的绿色圆柱表示数据库，我们可以看到，已经处理完写请求的两个节点的数据库中存有两个版本V1和V2的数据K，而后两个节点则只有V1版本的数据K。此时，第二个节点收到读请求，由于其数据库内有两个版本的数据K，而且本身标记为Dirty，所以它需要向尾节点发送询问请求。尾节点回复其应返回版本V1的数据K，此节点就按照尾节点的指示回复客户端V1版本的数据K。&lt;/p&gt;

&lt;p&gt;综上所述，由于无论在CR还是CRAQ中，尾节点所存储的数据都是最新的具有一致性的数据，那么在可能出现的并发读写情况下，只需要向尾节点询问相关数据的版本，就可以确定应该回复的数据。&lt;/p&gt;

&lt;h3 id=&quot;membership-change-和-failure-recovery&quot;&gt;Membership Change 和 Failure Recovery&lt;/h3&gt;

&lt;p&gt;由于CRAQ中节点的结构类似于双向链表，所以Failure Recovery的策略其实就是Membership Change中的删除节点类似，也就是双向链表中节点的删除。设要删除的节点为D，某节点为N，其具体情况如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果D是N的后继：N需要把其所有数据传送给其新的后继。因为D有可能是上一节图中第二个节点的情况，那么写请求就不能传送给后继节点。如果D是尾节点，那么N需要D把所有ACK都反向传输完毕后再删除节点。&lt;/p&gt;

  &lt;p&gt;如果D是N的前驱：N需要把数据传送给其新的前驱。同时，如果D是头节点，N需要成为新的头结点。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;节点的添加大同小异，只需要在新节点A是头结点和尾节点时进行特殊处理即可。&lt;/p&gt;

&lt;h3 id=&quot;craq总结&quot;&gt;CRAQ总结&lt;/h3&gt;

&lt;p&gt;CRAQ对原有的Chain Replication进行了改进，平摊了读请求的压力，还用简单的模型保证了数据的强一致性。但是另一方面，从写请求的流程来看，需要线性地流经每一个节点势必增加系统的延迟。而Raft等算法则取决于单个机器的最长时间，如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/compare.png&quot; alt=&quot;compare&quot; /&gt;&lt;/p&gt;

&lt;p&gt;假设使用RAFT和CRAQ的系统中都有三个节点，黄色表示向非主节点写入指令需要的时间，蓝色表示该节点返回ACK花费的时间。可以看到，由于RAFT算法是并行的，它的实际时间取决于单个节点花费的最长时间。而CRAQ是串行的，它的时间是所有结点花费的时间总和。&lt;/p&gt;

&lt;p&gt;如果想要使用CRAQ的模型进行备份，那么节点数量一定不能太多，最好节点不要跨机房，否则在网络传输上花费的时间就十分庞大了。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">前面两篇文章，我们讨论了分布式系统中为了维护一致性所使用的共识性算法Paxos和Raft。这些算法保证了各机器之间数据的一致性，但是Paxos算法针对的是平等的Replication策略，而Raft算法针对的是Primary-Back Replication策略。Replication策略和共识性算法的目的不同，它的设计是为了实现容错（Fault-Tolerance），即在一部分机器不可用后，仍能保证正常提供服务。下面，我将简要描述一下6.824中讨论的两种复制模型Primary-Back Replication和Chain Replication。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">浅谈Raft</title>
      <link href="http://localhost:4000/%E8%B0%88%E8%B0%88Raft" rel="alternate" type="text/html" title="浅谈Raft" />
      <published>2020-07-19T18:00:00+08:00</published>
      <updated>2020-07-19T18:00:00+08:00</updated>
      <id>http://localhost:4000/%E8%B0%88%E8%B0%88Raft</id>
      <content type="html" xml:base="http://localhost:4000/%E8%B0%88%E8%B0%88Raft">&lt;p&gt;上图为Steam同名游戏———RAFT&lt;/p&gt;

&lt;p&gt;Raft算法是由Diego Ongaro和John Ousterhout于2014年提出的共识性算法。在斯坦福当助教时，他发现学生很难理解Paxos算法，所以他希望能用一种更简单易懂的算法来代替Paxos，以此为契机，他把便于理解作为目的，提出了Raft算法。&lt;/p&gt;

&lt;h2 id=&quot;paxos算法的不足&quot;&gt;Paxos算法的不足&lt;/h2&gt;

&lt;p&gt;了解过Paxos算法的朋友都知道，Paxos算法最后提出了2PC的机制来保证共识。但是，我们也提到了，两段式的请求消耗过大，而且多Proposer的前提也很难实现，Proposal的数据结构也未提及。总之，Basic Paxos算法在工程上有非常多的实现困难。另一方面，也就是上文提到的，非常难于理解。而Raft正是为了解决这两大问题，它提出并总结了系统中节点的角色，各角色的功能和职责，需要使用的RPC及其参数和逻辑，甚至连数据结构都清楚地给出了定义。这简直就像手把手教开发人员实现算法。而也正是由于清晰详细的介绍，Raft算法理解起来也相当地容易，顺带一提，可能是为了“便于理解”，论文本身的行文和用词也相当地简单。&lt;/p&gt;

&lt;h2 id=&quot;raft&quot;&gt;Raft&lt;/h2&gt;

&lt;p&gt;Raft算法使用Leader代替Paxos中的多个Proposer，使用Log Entry代替Proposal，以此简化问题。也因此，Raft算法将整个共识性问题划分为Leader Election，Log Replication两大块，同时，为了解决在崩溃时可能出现的问题，还提出了Safety的问题。此外，Raft还提供了Snapshot和Membership Change的解决方案。&lt;/p&gt;

&lt;p&gt;Raft将系统中的角色划分为三种，Leader，Candidate和Follower。整个系统只有三种种通信方式，AppendEntryies PRC，RequestVote RPC和InstallSnapshot RPC。其中，AppendEntries RPC只能由Leader发出，用于向Follower追加Log Entry或者广播Heartbeats，RequestVote只能由Candidate发出，用于发起Leader Election，InstallSnapshot RPC只能由Leader发出，用于向Follower发送快照。&lt;/p&gt;

&lt;h3 id=&quot;leader-election&quot;&gt;Leader Election&lt;/h3&gt;

&lt;p&gt;Raft将系统中的角色划分为三种，Leader，Candidate和Follower。所有机器初试为Follower，他们三者的转化关系如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/state transfer.png&quot; alt=&quot;state transfer&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Follower-&amp;gt;Candidate：当自身的计时器超时后，将自身转变为Candidate&lt;/p&gt;

  &lt;p&gt;Candidate-&amp;gt;Leader：当Candidate获得Quorum的选票时，成为Leader&lt;/p&gt;

  &lt;p&gt;Candidate-&amp;gt;Follower：当Candidate在RequestVote RPC中收到的Term大于该自己的Term，或者收到Term大于等于该自己Term的AppendEntries PRC时，就变为Follower&lt;/p&gt;

  &lt;p&gt;Leader-&amp;gt;Follower：当在RPC中收到的Term大于该机器的Term时，自动变为Follower&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;要特别说明的是，只有Candidate才能成为Leader，所以只有Candidate才能发起Leader Election。下面，我们逐条来分析这么做的理由。&lt;/p&gt;

&lt;p&gt;Follower-&amp;gt;Candidate：首先，我们要声明的是，Raft规定了一个时限，如果在这个时限内没有收到Heartbeat（即不携带信息的AppendEnyties RPC），那么就认为超时了。假设在初始系统中，所有的机器角色都是Follower，但是只有Candidate才能发起选举，那么要怎么才能得到Leader呢？于是，这一条的作用就出现了。由于系统中没有Leader，那么就不会有Heartbeats，也就是说必定有一个Follower会超时，按着这一条的转换关系，此Follower会转变成Candidate，然后发出Leader Election请求，直到系统中出现Leader。还有一种情况，本来正常工作的系统的Leader崩溃了，同样地，由于收不到Heartbeats，其中的Follower会转变成Candidate发起Leader Election，保证系统正常运行。&lt;/p&gt;

&lt;p&gt;Candidate-&amp;gt;Leader：这一条其实无需多言，按照Paxos中提出的Quorum机制，获得多数承认的机器可以成为Leader。&lt;/p&gt;

&lt;p&gt;Candidate-&amp;gt;Follower：不同的Term代表不同的Leader周期，而且Term是自增的。Candidate会在发起Leader Election之前将自己的Term加一，代表自己比之前的Leader更新。而如果收到了一个大于自己的Term的RequestVote RPC，这就意味有机器在时间上比自己更新，所以就应该主动退出竞争。如果仅仅收到和自己Term相同的RequestVote RPC，这意味着在这一轮选举中有竞争者，谁能成为Leader就要看网络的连通情况，这就无需变为Follower了。而如果它收到了一个AppendEntries RPC且Term大于等于自己，而此RPC仅可能由Leader发出，这就意味本轮已经选出Leader或者有更新的Leader了，此时就应该退出竞争。&lt;/p&gt;

&lt;p&gt;Leader-&amp;gt;Follower：我们知道Term的大小代表Leader的新旧，也就是说，当在RPC中接收到的Term大于自己时，有一个新的Leader被选出了。这种情况有可能由于暂时的网络故障，使得此Leader不能与其他机器连通，而当剩余机器选出Leader后网络恢复了，那么此机器中存储的数据很可能不够完整，所以应该让位给新的Leader。&lt;/p&gt;

&lt;p&gt;有了以上的要求，整个系统的角色转换已经可以顺利流转了。还需要关注的，是RequestVote RPC中的投票逻辑。在Raft算法中，使用“先来先投票”的原则，这个原则暗示了在多个Candidate竞争的过程中，与其他节点通信良好的会更有可能赢得选举。也就是说，选举出来的Leader与其他节点的通信延迟低，从而提升了系统的性能。但是，当本机器的Term大于RequestVote RPC中的Term时，说明本机器比发送请求的机器更新，此时应当拒绝请求。&lt;/p&gt;

&lt;p&gt;不过，假如有一种极端情况：在某种条件下，所有的节点同时成为Candidate。而按照上述策略，它们首先会投自己一票，从而产生人均一票的情况，然后重新开始选举，如此往复。为了避免这种情况，Raft使用等待随机时间后再发起选举的解决方案。&lt;/p&gt;

&lt;h3 id=&quot;log-replication&quot;&gt;Log Replication&lt;/h3&gt;

&lt;p&gt;Raft中的Log Entry对应于Paxos中的Proposal，但是它远比Propsal具体实际，也比Propsal强大。它是Write-ahead Log（WAL），也就是说，它在执行之前就被存储在可靠的磁盘上了，这种方案被广泛地用在实际的分布式系统中，以此提供一定的容错能力。&lt;/p&gt;

&lt;p&gt;在Raft中，Log Entry使用Term和Index来标记自己，以此区别不同Log Entry并比较其新旧程度。其中，Term代表不同的Leader周期，而Index代表在同一个Leader周期中的不同Log Entry。Log Entry的整个生命周期如下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Client向Leader发送请求{x-&amp;gt;1}，Leader将其包装成Log Entry，追加到自己的Log Entries尾部，此时该log Entry的状态为Append&lt;/li&gt;
    &lt;li&gt;Leader使用AppendEntries RPC将这条Log Entry广播给所有的Follower，要求Follower也将其追加到自己的Log Entries的尾部&lt;/li&gt;
    &lt;li&gt;当Follower完成后，会送回AppendEntries RPC的回答，这个回答可能是成功，也可能是失败。&lt;/li&gt;
    &lt;li&gt;如果Leader收到Quorum个的成功的回应，他会将其状态设为Commited，表示已经在Quorum个Follower中写入该请求。如果没有，则重新发送。&lt;/li&gt;
    &lt;li&gt;在将该Log Entry设为Commited后，Leader就可以在机器中执行{x-&amp;gt;1}，同时将结果返回给Client。此时，该Log Entry的状态为Applied。各Follower则会在后台选择时间执行该命令。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;综上所述，Log Entry一共有三种状态，Append，Commited和Applied。&lt;/p&gt;

&lt;p&gt;Append状态表示仅仅被追加到了Leader的Log Entries中，此时既未被其他机器承认，也未被执行。唯一的好消息是，它被存储在Leader的磁盘中了，哪怕Leader暂时崩溃了，只要系统恢复后该机器仍然是Leader，也能读取其中内容，继续向下执行操作。&lt;/p&gt;

&lt;p&gt;Commited状态表示此Log Entry已经得到了系统中Quorum个机器的共识，在这个状态下，它能够承受2f+1个机器中f个机器的崩溃，一旦系统恢复，它可以被安全地执行。&lt;/p&gt;

&lt;p&gt;Applied状态表示它已经被执行过了，只要处于该状态，那么在形成快照时，它就可以被删除。不过，由于各机器都在后台执行命令，Raft仅保证Log Entry最后能被执行但不保证执行的一致性。&lt;/p&gt;

&lt;p&gt;在追加Log Entry和维护Log Entry的过程中，Raft必须维护一下两条准则&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;如果在两个不同机器上的Log Entry拥有同样的Term和Index，那么这个Log Entry包含的Command相同&lt;/li&gt;
    &lt;li&gt;如果在两个不同机器上的Log Entry拥有同样的Term和Index，那么在这个Log Entry以前所有Log Entry都相同&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;由于每个Term仅有一个Leader，而Leader给每个Log Entry标记的Index不同，这就意味着Log Entry的Term和Index构成了全局唯一性。这个特性保证了第一条准则。&lt;/p&gt;

&lt;p&gt;第二个准则由AppendEntries RPC的内部逻辑保证。AppendEntries PRC的请求中带有PrevLogIndex和PrevLogTerm两个参数用来验证Follower中最新的Log Entry是否和Leader一致，这两个参数是AppendEntries RPC中Log Entry的前一个Log Entry的Index和Term。一旦发现不一致，Follower被要求删掉此Log Entry并返回失败，在一下次AppendEntries RPC时，Leader会将这两个参数改为当前Log Entry的前两个Log Entry的Index和Term，以此类推。直到找到Follower和Leader匹配的位置后，Leader会把从匹配位置开始直到最后的Log Entry全部发送给Follower。Leader通过这种强制更换的方式实现了第二条准则。这里要特别指出的是，在正常情况下不会出现Follower和Leader不匹配的情况，只有在机器崩溃的时候才可能出现，而这种强制更换掉的Log Entry中有两种情况，被删除和位置变更。其中被删除的Log Entry只有可能处于Append状态，位置变更的Log Entry则处于Commited状态。&lt;/p&gt;

&lt;p&gt;除此之外，AppendEntries RPC还要喝RequestVote RPC一样检查当前Term是否小于请求中的Term，否则将拒绝请求。在Leader的CommitedIndex大于本机器的CommitedIndex时，还应将本机器的CommitedIndex设为LeaderCommitedIndex和本机最新Index中的小的值，从而能跟上Leader的处理节奏。&lt;/p&gt;

&lt;h3 id=&quot;safety&quot;&gt;Safety&lt;/h3&gt;

&lt;p&gt;由于系统中的机器有可能发生故障和崩溃，而此时会产生许多意外的情况，所以Raft给Leader Election和Log Replication增加了一些限制来解决这些情况。&lt;/p&gt;

&lt;h4 id=&quot;election-resitriction&quot;&gt;Election Resitriction&lt;/h4&gt;

&lt;p&gt;如果按照原来的Leader Election的要求，那么我们可能会出现下面的情况。有一台原来网络状况不是那么好的机器，它能赶得上当前的Term，但是不能收到最新的Log Entry，假设它在同Term中总比其他机器少一个Log Entry。此时由于某种原因，它的网络状况改善了并按照选举原则拿到多数票，赢得了选举，成为Leader。那么，按照AppendEntries RPC的要求，它会删除掉其他机器中比它快的那一个Log Entry，然而这个Log Entry已经被Commited了，这就出现了问题，这个系统虽然保证了一致性但是却损失了信息。所以Raft在RequestVote RPC中添加了一个逻辑：通过和AppendEntries RPC一样的方式进行比较，如果Candidate的Log Entry比自己的旧，就投拒绝票，只有Candidate和自己一样新或者比自己更新才投同意票。&lt;/p&gt;

&lt;p&gt;基于这个约束和AppendEntries RPC中的Quorum原则，如果2f+1个中有f个机器未收到最新的Log Entry，那么在选举中，他们最多只能拿到f票，从而无法成为Leader。我们可以得出结论，只有拥有最新的Log Entry的Candidate才能成为Leader。&lt;/p&gt;

&lt;h4 id=&quot;committing-entries-from-previous-terms&quot;&gt;Committing Entries From Previous Terms&lt;/h4&gt;

&lt;p&gt;假设有如图情况&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/overwrite.png&quot; alt=&quot;overwrite&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中，黑框代表Leader，Log Entry中的数字代表Term。由图可知，在Term（2）中，S1是Leader，它向S1和S2写入了Log Entry但是由于某些原因宕机了。此时，S5得到S3，S4和自己的投票成为Term（3）的Leader。但是，仅仅把Log Entry追加到本机上后，在向其他服务器发送Log Entry之前就崩溃了，注意，此时该Log Entry未被Commited。此时，S1又成为了Term（4）的Leader，在它把一个Log Entry复制给S3后，由于某些原因又崩溃了。按照上述的选举原则，S5仍然可以拿到S2，S3，S4的投票，当他成为Term（5）的Leader后，它还未收到任何Client的请求，于是它按照AppendEntries的的逻辑使用强制替换保证了一致性。但是，值得注意的是，它使用Term（3）的Log Entry覆盖了Term（4）的Log Entry。这种情况是不被允许的，旧的数据是不能覆盖新的数据的。于是，Raft提出了约束：只有和当前Term相同的Log Entry才能被Commited。&lt;/p&gt;

&lt;p&gt;通过这个约束，上述的蓝色Log Entry就不会在重启后重新发送给其他机器，而客户端也永远不会收到它想要的答复。此时，如果Client需要此命令被执行，那么它需要重新发送请求。&lt;/p&gt;

&lt;h3 id=&quot;membership-change&quot;&gt;Membership Change&lt;/h3&gt;

&lt;p&gt;上面的三个部分其实已经完成了Raft算法的主体介绍，这一部分讨论的是Raft算法如何解决成员变更的问题，成员的组成在Raft中被称为Configuration。成员变更意味着Quorum的总人数和成员的变化，所以无论是对Leader Election还是Log Replication都有很大的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/membership.png&quot; alt=&quot;membership&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示，在指定时刻，S1，S2在旧的Configuration中，S3，S4，S5在新的成员中，但此时整个的成员交接还未完成。所以，对于旧的Configuration来说，S1和S2组成了Quorum，对于新的Configuration来说S3，S4，S5组成了Quorum。那么此时就出现了两个Quorum，可以实现两种决策，这是Raft算法必须要避免的。&lt;/p&gt;

&lt;p&gt;Raft算法使用Log Entry实现成员变更。当需要进行成员变更时，Leader将包含新Configuration的Quorum以及旧Configuration的Quorum的Log Entry发送给Follower，等待Quorum的回复以此将其设为Commited。在此过程中，由于有部分成员已经收到此Configuration，所以所有的决策都需要在此条件下进行，而由于Leader包含这个Configuration，所以能确保这一点。由于中间态的Configuration既包含旧的Quorum又包含新的Quorum，所以所有的决策都需要两方的同意，这就保证的在这个阶段的决策能符合新旧Configuration的要求。然后，新的Configuration才被广播给所有Follower。同样地，由于至少有一个成员在新的Configuration下工作，所以一旦被广播给Follower，所有决策都要在新的Configuration下进行。这样，就完成了成员变更。&lt;/p&gt;

&lt;p&gt;总得来说，Raft引入了一个中间态，在这个中间态中，既包含旧Configuration的Quorum，又包含新Configuration的Quorum，以此满足在此期间决策的正确性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/membershipChange.png&quot; alt=&quot;membershipChange&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;snapchat&quot;&gt;Snapchat&lt;/h3&gt;

&lt;p&gt;在Log Replication中我们说，所有Commited的Log Entry都要被记录在磁盘上，但是磁盘的容量是有限的，如果达到上限了怎么办？很明显地，Raft使用Snapshot记录下机器中此时的状态，再将最新执行的的Log Entry以前的Log Entry全部删除就可以了。&lt;/p&gt;

&lt;p&gt;详细地说，Raft算法允许每台机器各自执行Snapshot，而Snapshot必须记录它最后包含的Log Entry的Term和Index作为LastIncludedEntry和LastIncludedIndex来标记自己记录的位置。然后将这个Log Entry及其之前的全部删除就能达到清理空间的效果。&lt;/p&gt;

&lt;p&gt;但是，有一个情况我们必须考虑。假如一台机器网络情况和运行状况都很差，它最新的Log Entry甚至没有其他机器的快照新，这时候使用AppendEntries RPC是不能帮助它的，因为对应的Log Entry已经被删掉了。这里就需要Leader使用新的RPC，InstallSnapshot RPC来帮助它。InstallSnapshot RPC包含Leader的Snapshot的数据，LastIncludedEntry和LastIncludedIndex等信息，当这个Follower使用Snapshot更新自己的状态后，它就需要使用LastIncludedEntry和LastIncludedIndex来更新自己记录的信息，然后Leader可以再使用AppendEntryies RPC去更新它的Log Entry。&lt;/p&gt;

&lt;h2 id=&quot;总结和思考&quot;&gt;总结和思考&lt;/h2&gt;

&lt;p&gt;Raft相对于Basic Paxos做了很大的改进。&lt;/p&gt;

&lt;p&gt;首先，它使用Leader代替了多Proposer，跳过了Prepare阶段，减少了网络请求的消耗。&lt;/p&gt;

&lt;p&gt;其次，它显式地明确了使用WSL来作为Propsal的载体，不仅降低了开发难度，还提供了相当的容错能力。&lt;/p&gt;

&lt;p&gt;最后，它对容错的场景进行了深入地考虑并且给出了相应的解决方案。&lt;/p&gt;

&lt;p&gt;最重要的，Raft在保证共识性的同时明确地实现了数据的一致性，这对于算法落地来说实在难能可贵。&lt;/p&gt;

&lt;p&gt;但是，Raft使用的成员变更看起来并不是一个很好的方案。我认为在实践中使用Zookeeper作为成员变更的中间工具可能更加合适易行。&lt;/p&gt;

&lt;p&gt;总得来说，Raft算法实至名归，对得起Practical和Understandable的主旨。在GitHub上，有相当部分的项目都使用Raft算法作为他们的核心算法。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">Raft算法是由Diego Ongaro和John Ousterhout于2014年提出的共识性算法。在斯坦福当助教时，他发现学生很难理解Paxos算法，所以他希望能用一种更简单易懂的算法来代替Paxos，以此为契机，他把便于理解作为目的，提出了Raft算法。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">浅谈Paxos</title>
      <link href="http://localhost:4000/%E6%B5%85%E8%B0%88Paxos" rel="alternate" type="text/html" title="浅谈Paxos" />
      <published>2020-07-13T04:00:00+08:00</published>
      <updated>2020-07-13T04:00:00+08:00</updated>
      <id>http://localhost:4000/%E6%B5%85%E8%B0%88Paxos</id>
      <content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Paxos">&lt;p&gt;图为真·Paxos———位于希腊的岛屿&lt;/p&gt;

&lt;p&gt;如果想要在后端开发上更进一步而不是局限于SSM框架，那么分布式是一个不那么坏的发展方向。如果要研究分布式系统，Paxos算法是绝对绕不过，也不能绕过的知识点。&lt;/p&gt;

&lt;p&gt;Leslie Lamport在1998年在《The Part-Time Parliament》中提出了Paxos算法，单单从论文名字看也知道这是篇不怎么正经的论文，这一结论在看完论文后又得到了印证。作者在论文中虚构了一个叫做Paxos的希腊城邦，这个城邦以议会作为最高权利机构，每条法令都需要在此议会中通过后方可实施，作者又对其中的细节作了一些描述和规定，以此符合分布式系统的实际模型。但是，这篇寓言性质的论文对于母语中文的我来说实在过于难懂，哪怕参考了许多资料和解释以后，我仍然很难理解这种模型，就算是读了《Paxos Made Simple》，也很难将其与这个故事一一对应起来。所以，我决定跳过这篇论文，从《Paxos Made Simple》开始说起。&lt;/p&gt;

&lt;h2 id=&quot;为什么需要paxos&quot;&gt;为什么需要Paxos&lt;/h2&gt;

&lt;p&gt;在开始谈Paxos细节之前，我想有必要谈谈为什么我们需要Paxos。几乎所有的资料都在说Paxos是一种共识性（Consensus）算法那么什么是共识呢，共识性和一致性（Consistency）有什么差别，为什么我们需要共识呢？我希望通过下面的例子解决这个问题。&lt;/p&gt;

&lt;p&gt;假设我们开了家“肥宅”奶茶店，为了简化模型，店里只卖珍珠奶茶，我们的配方为奶茶之比为1:1。如果只有一家店，自然是我们说了算。但是有人看我们开得不错，提出要入伙一起干，也不管我们同不同意，总之我们现在有两家店了。作为商业机密，配方是不能透露的，这家店老板也没多想，配方就定了奶茶比为2:1。那么此时，分歧就出现了，也就是说，两家店没有在奶茶的配方上达成“共识”。在这种情况下，由于没有“共识”，消费者在两家“肥宅”奶茶店买到的奶茶竟然味道不同，这就产生了“一致性”的问题。&lt;/p&gt;

&lt;p&gt;将上述的奶茶店换成计算机，配方换成提议，奶茶换成数据，就变成了分布式系统的模型。在奶茶店模型中，消费者喝到味道不同的奶茶倒是小事，但是如果在银行系统中，一台机器上余额是一百万，一台是负一百万那问题就大了。而且，由于现在绝大多数业务都需要保证数据一致性，那么保证提议的共识性就显得格外重要了。&lt;/p&gt;

&lt;h2 id=&quot;basic-paxos&quot;&gt;Basic Paxos&lt;/h2&gt;

&lt;p&gt;鉴于有很多朋友也像我一样无法理解《The Part-Time Parliament》，作者在2001年又发了《Paxos Made Simple》重新解释Paxos算法。作者在这篇文章中，终于用能看得懂的英语解释了Paxos算法是怎么运作的。&lt;/p&gt;

&lt;p&gt;通读全文，我们可以知道Paxos有两个目标安全性（Safety）和活跃性（Liveness）。其期望分别如下（此处采用Raft作者的理解）&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;安全性&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;每次提议仅有一个值被选定&lt;/li&gt;
    &lt;li&gt;服务器在值被接受前不会知道该值已被选定&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;活跃性&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;在一些提议中，最后必定有值会被选定&lt;/li&gt;
    &lt;li&gt;如果值被选定了，那么服务器最后总会知道该值&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;为了实现以上目标Paxos提出了两条约束，并通过数学证明：任何遵循这两条约束的系统都能保证共识性，此处我们仅讨论在文中作者如何推导出两条约束，不讨论数学上如何证明。该算法中，提议（Proposal）具有两个属性，编号（Number）和值（Value），算法中的规定的角色如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;提议者（Proposer）:负责提出提议&lt;/p&gt;

  &lt;p&gt;接受者（Acceptor）:负责审阅提议并决定是否批准&lt;/p&gt;

  &lt;p&gt;学习者（Learner）  :不参与议案过程仅学习通过的提案&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;考虑最简单的情况，我们仅使用一台机器作为Acceptor且接受Number相同的第一个Proposal，那么所有的提议都会由它审阅，并且只会有一种结果，能够保证共识。但是一旦这台机器宕机，那么整个系统就无法继续运行。所以使用多个Acceptor是必要的。此时，仅当Proposal被大于一半的Acceptor接受，该Proposal才被视为通过。虽然在文中此条件没有被显式地列为约束，但我认为其重要程度与后两者相当，因此我将其列为P0&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P0：当且仅当Proposal被大多数Acceptor接受（Accepted），该Proposal才被视为选定（Chosen）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;为了能保证Paxos 在一个Proposer和一个Acceptor的情况下工作，即符合活跃性的第一条要求，我们提出下面的方案作为约束：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P1：Acceptor必须接受其收到的第一个Proposal&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然而此约束存在一个问题，假如几个Proposal同时提出，且被分别发送发不同的Acceptor，每个Acceptor都接受一个Proposal，那么就会出现人均一票的情况，无法形成符合P0的情况，Paxos并未提出如何解决这一问题，而Raft使用随机等待的办法解决此问题。&lt;/p&gt;

&lt;p&gt;由P0的约束可知，需要大多数Acceptor接受提案，而P1则要求Acceptor接受其收到的第一个Proposal，那么这就要求每个Acceptor需要接受多个Proposal。这里，我们使用Proposal的Number作为区分。而为了保证被选中的Proposal具有相同的Value，我们提出以下约束：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P2：如果Number为n，Value为v的Proposal被选中，那么所有被选中且Number &amp;gt; n的Proposal都具有Value v&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;由于Proposal被选中意味着Proposal被大多数Acceptor接受，所以可以进一步约束为&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P2a：如果Number为n，Value为v的Proposal被选中，那么所有被接受且Number &amp;gt; n的Proposal都具有Value v&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;现在，我们假设一个新的 Proposer 刚刚从崩溃中恢复或加入此系统,并且发送了一个带有不同 Value 且Number更高的Proposal。P1要求Acceptor接受这个 proposal，但是却违背了 P2a。为了处理这种情况，需要继续加强P2a，于是我们得到&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P2b：如果一个Value为v的Proposal 被选中，那么之后每个 Proposer 提出的具有更大Number的Proposal都有Value v&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;P2b通过强制要求新的Proposal的值中含有Value解决了上述的问题。而作者通过数学工具，证明以下约束能够满足P2b&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P2c：对于任意的Value v和Number n，如果Value为v且Number为n的Proposal被提出，那么一定有多数Acceptor集合S满足：（a）S中不存在Acceptor已经接受任何Number小于n的Proposal，或者（b）S中的Acceptor所接受的Proposal中Number最高的具备Value v。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;文中根据P2c将整个流程划分为两段：准备（Prepare）和接受（Accept），但是主要是针对Proposer的，为了适应这个流程，进一步将P1约束如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P1a：如果Acceptor没有在Prepare阶段回复过Number大于n的请求，那么在Accept阶段，它可以接受Number为n的Proposal&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在得到上述的两个约束后，我们就能够以此为依据，得到具体的算法流程。&lt;/p&gt;

&lt;h3 id=&quot;2-phase-commit&quot;&gt;2 Phase Commit&lt;/h3&gt;

&lt;p&gt;上文提到整个算法流程分为两个阶段，下面我们将介绍具体流程是怎么样的。&lt;/p&gt;

&lt;p&gt;阶段一为准备阶段，具体如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Proposer 选择一个Number n，然后向大多数Acceptor发送Number 为n的Prepare request。&lt;/li&gt;
    &lt;li&gt;如果一个Acceptor接收到Number为n的Prepare request，并且n大于任何它已经回复的Prepare request的Number，那么它将承诺不再接受任何Number 小于 n的proposal，并且回复已经接受的最大Number的 proposal。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;阶段二为接受阶段，具体如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;如果Proposer 接受了来自大多数Acceptor对它的Prepare request 的回 复，那么接下来它将给这些 Acceptor发送Number为n，Value为v的 Proposal作为Accept request。其中v是收到的回复中最大 Number 的Proposal的Value，或者如果回复中没有Proposal的话，就可以是它自己选的任意值。&lt;/li&gt;
    &lt;li&gt;如果 Acceptor 收到一个Number 为n的Accept request，如果它没有对Number 大于n的Prepare request进行过回复，那么就接受该Accept request。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Raft作者的这张图形象地展示了Paxos的整个流程&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/paxos.png&quot; alt=&quot;Paxos&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;对paxos的一些思考&quot;&gt;对Paxos的一些思考&lt;/h2&gt;

&lt;p&gt;上述内容本质是对《Paxos Made Simple》的翻译和复述，下面我想谈谈我对这个算法一些思考。&lt;/p&gt;

&lt;p&gt;我仍然用上文奶茶店的例子，共识指的是两者对同一奶茶的配方认可，一致是奶茶的味道相同。初始奶茶店希望分店和自己采用相同的配方，这个过程就是寻求共识，如果分店同意，那么他们就达成了共识，这一阶段对应于提出Proposal。得到配方后的分店按配方制作奶茶，就能得到和初始奶茶店一样的味道，就相当于保证了数据的一致性。如果采用复制状态机的方案，奶茶配方在计算机系统中就是指令，两台初始完全相同的机器，在以同样顺序执行相同的指令后，就会得到一致的数据。也就是说，只要保证各机器对执行指令和顺序的共识，那么我们就能保证数据的一致性。&lt;/p&gt;

&lt;h3 id=&quot;为什么paxos需要p0&quot;&gt;为什么Paxos需要P0？&lt;/h3&gt;

&lt;p&gt;这个约束有两个作用，一是在拥有2f+1台机器的系统中，它能够允许f台机器同时崩溃，而仍能正常处理请求。其次，我们来考虑有2f+1个机器，而我们仅需要f个机器接受请求，那么假如所有的请求和数据都由前f个机器处理，而后f+1个机器没有任何数据。此时，前f个机器崩溃了，那么，我们的系统中就没有任何的数据了。但是，假如是要求f+1个机器接受请求，那么任何两次请求都必然有至少一个机器接受了两次请求，也就是说，它存有全部的最新数据。使用数学归纳法可以得知，无论接受了多少次请求，f+1个机器中所有的存储的数据可以拼接成完整的全部数据，那么，即使f个机器崩溃了，也能保证数据的完整。&lt;/p&gt;

&lt;p&gt;总得来说，Quorum机制能保证在2f+1台机器中f台机器崩溃的情况下保证Paxos协议的正常运行和数据的完整性。&lt;/p&gt;

&lt;h3 id=&quot;为什么basic-paxos只能处理single-decree&quot;&gt;为什么Basic Paxos只能处理“Single Decree”？&lt;/h3&gt;

&lt;p&gt;我们可以看到，如果Proposal希望能被接受，那么他必须包含之前所有被接受过的Proposal的Value，这在实践中是不可能实现的。另外，由于Basic Paxos允许多个Proposer，那么每个Proposal的Number大概率是不一致的，在跨事件的情况下，不能根据自增的Number来判断是否多个Proposal是对一个事件的共识。最重要的是，恐怕作者这篇论文的目的也仅仅是为了解决单个事件的共识。&lt;/p&gt;

&lt;p&gt;以前看这篇的论文的时候，由于之前了解的都是有Leader的系统，Proposal的不同Number的代表对不同Value的共识，然后在读Paxos时也代入了这种想法，就不能很好理解Paxos算法，现在从这个角度来看就容易理解得多。同时也可以解释为什么P2a和P2b中要求大Number应包含小Number有的Value，因为此时并不是两个事件，而是对同一事件的修改。比如两次的Value分别是{x=1}和{x=1，y=2}，那么实际上后者只是对前者的补充而已。&lt;/p&gt;

&lt;h3 id=&quot;为什么需要prepare阶段&quot;&gt;为什么需要Prepare阶段?&lt;/h3&gt;

&lt;p&gt;Prepare阶段主要是为了实现Proposer的共识，这也是和有Leader系统非常不同的一点。&lt;/p&gt;

&lt;p&gt;假设三个Proposer有三个版本的Proposal，具体如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Proposer1：{Number：1，Value：[X=1] }&lt;/p&gt;

  &lt;p&gt;Proposer2：{Number：2，Value：[X=1，y=2] }&lt;/p&gt;

  &lt;p&gt;Proposer3：{Number：3，Value：[X=1]，y=2，z=3] }&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果Proposer3最先到达Acceptor，那么根据Prepare阶段的要求，由于Proposer1和Proposer2的Number小于Proposer3，他们会被一直拒绝直到其内容和Proposer3相同。如果Proposer3最迟到达Acceptor，那么Proposer1和Proposer会在Acceptor阶段被拒绝。这里我们假设了Proposer3的Proposal是被选定的情况，如果它不是最终版本，那么，很有可能会出现Value中继续添加值的情况，然后Proposer3也会在第Accept阶段被拒绝，要求它重新提案。&lt;/p&gt;

&lt;p&gt;总之，由于Basic Paxos允许多个Proposer存在，所以需要Prepare阶段保证提案的一致性。&lt;/p&gt;

&lt;h3 id=&quot;为什么我们不使用basic-paxos&quot;&gt;为什么我们不使用Basic Paxos？&lt;/h3&gt;

&lt;p&gt;由于Basic Paxos需要Prepare阶段保证提案的一致性，而且一次算法的运行只能允许完成单次操作，所以如果直接使用Basic Paxos在性能上很可能会达不到我们的要求。因此，学界提出了更加符合实际的Multi-Paxos，使用Basic Paxos选举Leader，让Leader直接提案代替Prepare阶段，从而大大缩短了一次算法运行需要的时间。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">如果想要在后端开发上更进一步而不是局限于SSM框架，那么分布式是一个不那么坏的发展方向。如果要研究分布式系统，Paxos算法是绝对绕不过，也不能绕过的知识点。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">我的2019</title>
      <link href="http://localhost:4000/%E6%88%91%E7%9A%842019" rel="alternate" type="text/html" title="我的2019" />
      <published>2020-02-04T18:18:00+08:00</published>
      <updated>2020-02-04T18:18:00+08:00</updated>
      <id>http://localhost:4000/%E6%88%91%E7%9A%842019</id>
      <content type="html" xml:base="http://localhost:4000/%E6%88%91%E7%9A%842019">&lt;p&gt;​2019悄然而过，过去的一年，大大小小事情许多，而站在2020回望，似乎除了考研，与我有关的，实在不多。适逢新型冠状病毒肆虐，诸事皆歇，得空搭起博客，也算整理千万思绪，展望一下迷茫的未来。&lt;/p&gt;

&lt;h2 id=&quot;考研&quot;&gt;考研&lt;/h2&gt;

&lt;p&gt;​考研的复习始于面试受挫，这么说恐怕不对，因为真正的面试甚至都还没开始。看到牛客网上的面经，拿到大厂offer的兄弟几乎都是985或是211的硕士。面试的主要知识点说不上多么深入，无非是老生常谈的那几本书，那几个知识点，但是，要谈项目经验的话，我实在是没有能拿得出手的的东西，倘若硬要尝试大厂，恐怕只会撞得头破血流。再以本科杭电的身份自处，两相比较，最终还是选择的考研。现在回头想想，哪里会有公司要求一个大三的学生有光鲜亮丽的实际项目？然而话又说回来，现在无论是BAT还是公职单位，没有硕士的文凭，上升空间实在是小之又小，几乎埋下了35岁被清理的引子。当然强的不像话的我们另说，比说A校CM队的同学，有拿到阿里，微软实习的，有保研浙大的，实在不是能够与之同台竞技。&lt;/p&gt;

&lt;p&gt;​复习过程乏善可陈，大约与996无异。早上八点半起床，九点到图书馆，复习两个半小时，中午十一点半去吃饭，小睡一会，下午一点半起床去图书馆，复习到五点钟，吃完晚饭，去寝室休息到六点左右出门，到图书馆复习到九点或九点半，然后结束一天复习，回寝室准备睡觉。平心而论，我的复习时间不算长，但效率要比绝大多数人强上不少，几乎不停笔，算是复习卓有成效，可惜天不如人愿。&lt;/p&gt;

&lt;p&gt;​值得一提的，是九月中旬考研报名。在浙大和中科大犹豫不决。本来是首选浙大计算机，但是，实在是太难了。清北的计算机自主命题，严格控制分数线，所以公认最难，而浙大则是公版的数一英一408，但是浙大的分数线之高令人发指，我个人觉得难度几乎与清北齐平。中科大软院招人多，分数低，但是没导师，认可度没浙大高，虽然难度比浙大低一档，但是考上后的加成也要低一档。马克思说要结合实际，具体问题具体分析，还是得看自身情况呀。通过以下四点考虑，我最终选择了中科大的软院：1.上浙大可以说只有一层把握，考不上研究生过去一年的努力就等于白费。2.我准备的方向是后端，做中间件或者分布式，这些方向的学习资源非常丰富，完全可以自学成才。3.没导师确实少了很多帮助，但另一方面，没了导师可以说未来所有时间都是自己的。想做什么方向，想怎么安排生活自主权都在自己手里，或许更适合我。4.科软再菜也是C9呀，虽然不比浙大的牌子响亮，但是比杭电总归要强得多。&lt;/p&gt;

&lt;p&gt;​然而，一切都终结于2019年12月22号上午。数学，我最有自信的一科，炸了，血炸。考到一半的时候，我慌了，犯了大忌，尽力想冷静下来却办不到，那一刻，我的考研就结束了。无论我其他三门怎么发挥，恐怕也难以挽回这科的劣势，甚至于单科线都很难过。刚刚查了文件，大概还有十天出分，希望渺茫，虽然一路走来早已做好了最坏的打算，但八个月的努力白费总归是意难平。&lt;/p&gt;

&lt;p&gt;​只能是尽人事，听天命。&lt;/p&gt;

&lt;h2 id=&quot;创新实践&quot;&gt;创新实践&lt;/h2&gt;

&lt;p&gt;​这门课实在是麻烦得要命。但总归能学到一些东西。&lt;/p&gt;

&lt;p&gt;​大学所谓的teamwork其实是solo-teamwork，大学四年，除了一门课的teamwork划水划过去，其他的我几乎都干了70%以上的事。有趣的是，我那一次的划水竟然被大佬喷了，虽然能体会那位大佬的干了90%的活的心情，可是当时我等了一个半个小时也等不到回复信息，以为没有问题就睡觉了，结果被喷就难受了。言归正传，这次teamwork我干的活大概也在90%左右，10次ppt，8篇paper的资料，实验重现几乎一力完成，但是付出总是有回报的。大学里，同学间，师生间大多留点情面，但是毕业了，遇到这种事却不能马马虎虎，多带一个名字就过去了。这就要说到展现自我的能力了。&lt;/p&gt;

&lt;p&gt;​以前总认为闷声发大财，是金子总会发光的。但现在看来，发大财的时候肯定是不能到处讲的，免得遭人嫉妒，而后者则是所谓怀才不遇的人的自我安慰。到了大学，终于明白信息不对称随处都在，尤其是到了求职的时候。你说你厉害，我怎么信你？总是要拿出一些东西来证明的。挑战杯之类的比赛水吗？水，真的水。但是在面试官眼中，你没有ACM的奖项就算了，但这么水的挑战杯也没有，那岂不是更水吗？简历里什么都没，怎么让人相信你水平高呢？有能力，要展现出来，多参加比赛，拿个小奖，奖不是奖励，是证明，证明你的能力高于同龄人。同理，以后团队工作，要让上级看到自己的工作量，不然别人就要以为你是混日子的。&lt;/p&gt;

&lt;p&gt;​这门课充分锻炼了我的看论文的能力。以前看到全英文，很容易怀疑人生，感觉看完几乎是不可能的任务。但是这么多论文看下来，终于养成了看论文的能力，现在总算能拿到论文不慌，知道重点在哪些段落，哪些细节需要注意，把论文从头到尾看完，还能再做一个报告。这个能力我以后应该是要受用无穷的。但是，前几天找的分布式存储的论文还一个字没看呢。&lt;/p&gt;

&lt;p&gt;​这门课暴露的我的一个不足，就是表达能力不行。做汇报的时候，往往会觉得力不从心，懂得说不好，表达不出来，不能很自如地边讲话边思考。以前看演讲，觉得这个不行那个不行，没想到最不行的是我自己。追根溯源，是两点，1.本身对报告的内容不够熟悉，细节掌握的不够充分，没有一个清晰的思路。2.演讲能力的不足，这个不仅仅是需要锻炼，最好有稿子，最不济要有一个大纲，然后要提前准备，反复练习。&lt;/p&gt;

&lt;p&gt;​千言万语，这门课终于结束了，天知道有多push。&lt;/p&gt;

&lt;h2 id=&quot;gsoc&quot;&gt;GSoC&lt;/h2&gt;

&lt;p&gt;​申请两次，被拒两次。那感觉就像是精心准备的表白，被女神无情拒绝。&lt;/p&gt;

&lt;p&gt;​第一次申请时的我实在是too young too naive。我单以为只有我一个人申请就只能给我，没想到还能谁都不给。这次申请的是小组织的小项目，更新下API而已，哪里有什么难度。可惜当时无知，proposal写得跟官话一样，满口空话，实际项目相关压根没写，怕是换了我自己也是要拒绝的。&lt;/p&gt;

&lt;p&gt;​第二次申请的真是大项目呀，阿里的RPC框架Dubbo，要是能给它贡献代码，BAT躺着也能进去了，可惜没拿到。一开始的项目是加一个Consul实现的register，虽然有难度，可是说不上完成不了，花点时间怎么也能写出来。整了大半个月，proposal都快写完了，结果说社区里有人实现了，当时真的傻了。硬着头皮换了项目，用gRPC做RPC的第三方协议，看着很合理，写起来完全的另一回事。gRPC的API和dubbo的思路完全不同，最后还得回到Netty上，我真没研究过Netty。也知道多半是申不上的，但还是磕磕绊绊交了proposal。当然，没过。&lt;/p&gt;

&lt;p&gt;​话虽如此，但还是能学到不少东西的。我学会用Mailing List了，看得懂Apache的issue管理了，能写英文邮件了（直接导致我英文邮件写得比中文顺）。重要的是两点，1.不怕所谓的开源社区和开源项目。其本质就是一帮在世界各地的人，给同一个项目写代码，想参加进来的前提是把与自己相关的代码看懂。2.敢去看大型项目的源码，尝试去理解其用意。之前从没有真正地看过这些项目的代码，但这两次是实实在在地去阅读代码，将其个部分联系起来，并在本地调试。这是程序员进阶的必要技能，这次算是点了一级，来日方长，慢慢升级吧。&lt;/p&gt;

&lt;p&gt;​这三件事算得上我去年最重要的事，都不算圆满，但多少能从中学到一些经验。&lt;/p&gt;

&lt;p&gt;​但愿今年，健康平安，劳有所得。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="essay" />
      

      
        <summary type="html">2019悄然而过，过去的一年，大大小小事情许多，而站在2020回望，似乎除了考研，与我有关的，实在不多。适逢武汉病毒肆虐，诸事皆歇，得空搭起博客，也算整理千万思绪，展望一下迷茫的未来。</summary>
      

      
      
    </entry>
  
</feed>
