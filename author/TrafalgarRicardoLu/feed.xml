<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="http://localhost:4000/author/TrafalgarRicardoLu/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2020-08-08T18:57:13+08:00</updated>
  <id>http://localhost:4000/author/TrafalgarRicardoLu/feed.xml</id>

  
  
  

  
    <title type="html">Ghost | </title>
  

  
    <subtitle>The professional publishing platform</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">浅谈Google的三驾马车之GFS</title>
      <link href="http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BGFS" rel="alternate" type="text/html" title="浅谈Google的三驾马车之GFS" />
      <published>2020-08-08T18:00:00+08:00</published>
      <updated>2020-08-08T18:00:00+08:00</updated>
      <id>http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BGFS</id>
      <content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BGFS">&lt;p&gt;上图为真·三驾马车&lt;/p&gt;

&lt;p&gt;谷歌在2003到2006年间发表了三篇论文，《MapReduce: Simplified Data Processing on Large Clusters》，《Bigtable: A Distributed Storage System for Structured Data》和《The Google File System》介绍了Google如何对大规模数据进行存储和分析。这三篇论文开启了工业界的大数据时代，被称为Google的三驾马车。&lt;/p&gt;

&lt;h2 id=&quot;背景介绍&quot;&gt;背景介绍&lt;/h2&gt;

&lt;p&gt;在21世纪初，互联网上的内容，大多数企业需要存储的数据量并不大。但是Google不同，Google的搜索引擎的数据基于爬虫，而由于网页的大量增加，爬虫得到的数据也随之急速膨胀，单机或简单的分布式方案已经不能满足业务的需求，所以Google必须设计新的数据存储系统，其产物就是Google File System（GFS）。不过，在Google的设计中，为了尽可能的解耦，GFS仅负责数据存储而不提供类似数据库的服务。也就是说，GFS只存数据，而对数据的具体内容一无所知，自然也就不能提供基于内容的检索功能。所以，更进一步，Google开发了Bigtable作为数据库，向上层服务提供基于内容的各种功能。此外，Google 的搜索结果依赖于PageRank算法的排序，而该算法又需要一些额外的数据，比如某网页的被引用次数，所以他们还开发了对于的数据处理工具MapReduce，在读取了Bigtable数据的技术上，根据业务需求，对数据内容进行运算。其总体架构如下，GFS能充分利用多个Linux服务器的磁盘，并向上掩盖分布式系统的细节。Bigtable在GFS的基础上对数据内容进行识别和存储，向上提供类似数据库的各种操作。MapReduce则使用Bigtable中的数据进行运算，再提供给具体的业务使用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Google troika.png&quot; alt=&quot;Troika&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;google-file-system&quot;&gt;Google File System&lt;/h2&gt;

&lt;p&gt;GFS是三驾马车中最底层的组件，当然也是最复杂的，因为他直接和分布式的系统接触。在具体探讨实现细节之前，Google给出了一些设计前提和设计目标。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;由于使用的机器是廉价的商业化机器，那么机器崩溃被认为是一种常态。&lt;/li&gt;
    &lt;li&gt;系统存储以大文件为主，但也支持小文件。文件大小通常在100MB左右并且需要高效的操作几个GB的文件。&lt;/li&gt;
    &lt;li&gt;系统需要支持大规模的连续读取和小规模的随机读取，以及大规模的追加写。&lt;/li&gt;
    &lt;li&gt;高性能稳定的网络带宽比延迟更重要。&lt;/li&gt;
    &lt;li&gt;以及最重要的，能在分布式的系统上运行。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;在下面，我们根据具体的措施讨论如何实现以上目标。&lt;/p&gt;

&lt;h3 id=&quot;总体架构&quot;&gt;总体架构&lt;/h3&gt;

&lt;p&gt;首先我们来看看GFS的总体架构。在这个架构中，GFS采用了单Master（Single Master）的设计来简化系统的复杂度。Master负责两点，一是存储和维护Chunk Server和数据块的相关信息，二是处理客户端的请求。也就是说，Master并不存储任何具体的数据，这些数据被存在被称为Chunk Server的数据节点上。其中Chunk就是指数据块，在GFS中被固定为64MB大小，我想着可能跟第二个目标相关。每当数据需要被写入时，就更新GFS中的信息，并把数据封装成Chunk写入到数据库中，具体流程在下面会仔细介绍。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/GFS Architecture.png&quot; alt=&quot;GFS Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图演示了应用如何调用GSF进行读操作的具体流程&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;应用调用GFS Client的函数，要求其读取具体的文件/foo/bar，假设大小为50MB。&lt;/li&gt;
    &lt;li&gt;GFS Client根据Chunk的固定大小计算出/foo/bar的Chunk的Index，即64/50向上取整，Chunk Index=1。以及其在Chunk内的偏移量Byte Range[0，50]，并将File Name和Chunk Index作为参数发送给GFS Master&lt;/li&gt;
    &lt;li&gt;Master返回了对应的Chunk Handle（也就是Chunk的ID，上图中的2ef0）和Chunk Locations（Chunk Server和其副本的IP）&lt;/li&gt;
    &lt;li&gt;GFS Client根据返回的Chunk Locations找到最近的Chunk Server，然后根据Chunk Handle找到对应的Chunk，最后按照这个文件在Chunk中偏移量Byte Range读取文件。&lt;/li&gt;
    &lt;li&gt;Chunk Server按照其要求返回文件数据。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;整个的流程相当清晰，客户端负责将文件在File Namespace的位置交给Master，Master根据其位置返回对应的Chunk和Chunk Server，然后客户端再根据这些信息去拿数据。但是，这里有很巧妙的设计，就是GFS将所有数据传输的压力都放到Chunk Server上，而不是Master。假设由Master根据Chunk Handle去找数据并返回给客户端，那么这里就有会造成系统带宽压力增大。如果按照假设设计，Chunk Server将数据传回给Master后，Master还要将数据传回给客户端，也就是64MB的流量陡然翻倍成128MB。而实际的GFS不仅降低了Master带宽的压力，还把读取数据的压力均摊到每个Chunk Server上，降低了整个系统的压力。这些设计明显有助于实现目标三。&lt;/p&gt;

&lt;p&gt;通过对读取过程的分析，可以发现，GFS已经完全实现目标二中的大规模连续读和小规模随机读的要求。&lt;/p&gt;

&lt;h3 id=&quot;single-master&quot;&gt;Single Master&lt;/h3&gt;

&lt;p&gt;Master中保存三种信息（MATEDATA）&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;文件和chunk的namespace，即文件树形式的命名方式。&lt;/li&gt;
    &lt;li&gt;文件到chunk的映射，即每个文件需要哪几个Chunk来记录。&lt;/li&gt;
    &lt;li&gt;每一个chunk的具体位置。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;这些信息都平时都存在内存中，以此提高响应速度。这似乎只能存储很少的信息，但是，Master只需要64byte的空间就能记录64MB的Chunk的相关信息，也就是说，128MB的内存能存储1PB的数据的相关信息。不过，为了保证Master能在崩溃后恢复，在执行改变前两种信息的操作前需要使用WAL的形式定期地保存到磁盘上。这样，在Master恢复的时候，就能通过磁盘上的WAL来重建前两种信息。而Chunk的位置则可以通过Heartbeats的形式查询并更新，这样不仅加速了Master的恢复，也能使GFS的配置更加灵活。&lt;/p&gt;

&lt;h3 id=&quot;写数据write&quot;&gt;写数据（Write）&lt;/h3&gt;

&lt;p&gt;由于每个Chunk Server都有备份的副本，所以写操作要比读操作稍微复杂一点。具体的流程如下图所示&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/GFS Write.png&quot; alt=&quot;GFS Write&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;客户端向 Master 询问目前哪个Chunk Server持有该Chunk的Lease&lt;/li&gt;
    &lt;li&gt;Master 向客户端返回Primary Replica和其他Replica的位置&lt;/li&gt;
    &lt;li&gt;客户端将数据推送到所有的Replica上。Chunk Server会把这些数据保存在缓冲区中，等待所有 Replica 都接收到数据。&lt;/li&gt;
    &lt;li&gt;客户端发送写请求给 Primary，Primary 为来自各个客户端的修改操作选定执行序列号，并按顺序地应用于其本地存储的数据。&lt;/li&gt;
    &lt;li&gt;Primary 将写请求转发给其他 Secondary Replica，Replica 们按照相同的顺序应用这些修改&lt;/li&gt;
    &lt;li&gt;Secondary Replica 响应 Primary，示意自己已经完成操作。&lt;/li&gt;
    &lt;li&gt;Primary 响应客户端，并返回该过程中发生的错误&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里要说明的是，每个Chunk被复制到多个Chunk Server中，以此避免单个节点崩溃可能造成的数据损失。而这些Chunk Server中负责相应写操作的Chunk Server被称为Primary Replica，其余的被称为Secondary Replica，接受来自Primary Replica的请求。而为了保证写入数据的一致性，只能有一个Primary Replica，这里的唯一性就由Lease来实现。当需要写入数据时，Master会将特定的Lease分配给某个Replica，拿到Lease的这个Replica就称为了Primary Replica。&lt;/p&gt;

&lt;p&gt;我们再仔细分析整个写的的流程。我们可以注意到为了保证数据的一致性，一次写入只能有一个Primary Replica，同样地，在正式写入数据之前要求所有Secondary Replica缓存数据也是为了一致性。这里的思想类似于WAL，将要执行的操作先保存下来，这样万一崩溃了也能从磁盘读入数据，继续执行未完成的操作。等待所有Secondary Replica完成操作后再相应客户端也是为了保证数据的一致性。&lt;/p&gt;

&lt;p&gt;和读操作类似，在写操作中，为了降低Master的压力，所有的数据由客户端发向Chunk Server。&lt;/p&gt;

&lt;h3 id=&quot;追加append&quot;&gt;追加（Append）&lt;/h3&gt;

&lt;p&gt;为了提升性能，GFS提供并推荐使用追加操作修改文件。它与写操作不同之处仅仅在于它向文件尾端添加数据而不是覆盖，它的流程也he 写操作大同小异&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;客户端将数据推送到每个 Replica，然后将请求发往 Primary&lt;/li&gt;
    &lt;li&gt;Primary 首先判断将数据追加到该块后是否会令块的大小超过上限：如果是，那么 Primary 会为该块写入填充至其大小达到上限，并通知其他 Replica 执行相同的操作，再响应客户端，通知其应在下一个块上重试该操作&lt;/li&gt;
    &lt;li&gt;如果数据能够被放入到当前块中，那么 Primary 会把数据追加到自己的 Replica 中，拿到追加成功返回的偏移值，然后通知其他 Replica 将数据写入到该偏移位置中&lt;/li&gt;
    &lt;li&gt;最后 Primary 再响应客户端&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;特别值得一提的是，GFS保证追加操作至少被执行一次（at least once），这意味着追加操作可能被执行多次。当追加操作失败时，为了保证偏移量，GFS会在对应的位置填充重复的数据，然后重试追加。也就是说，GFS不保证在每个副本中的数据完全一致，而仅仅保证数据被写入了。&lt;/p&gt;

&lt;h3 id=&quot;数据一致性&quot;&gt;数据一致性&lt;/h3&gt;

&lt;p&gt;在GFS中，由于分布式系统的原因，不同节点间处理请求和存储数据速度不一致导致了客户算读取数据时可能出现各种不同的情况。&lt;/p&gt;

&lt;p&gt;文件的数据修改则相对复杂。在讲述接下来的内容前，首先我们先明确，在文件的某一部分被修改后，它可能进入以下三种状态的其中之一：&lt;/p&gt;

&lt;p&gt;在文件的某一部分被修改后，它可能进入以下三种状态的其中之一：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;客户端读取不同的 Replica 时可能会读取到不同的内容，那这部分文件是不一致的（Inconsistent）。&lt;/li&gt;
    &lt;li&gt;所有客户端无论读取哪个 Replica 都会读取到相同的内容，那这部分文件就是一致的（Consistent）。&lt;/li&gt;
    &lt;li&gt;所有客户端都能看到上一次修改的所有完整内容，且这部分文件是一致的，那么我们说这部分文件是确定的（Defined）。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;也就是说，在一致性和强度上，Defined&amp;gt;Consistent&amp;gt;Inconsistent。&lt;/p&gt;

&lt;p&gt;在修改后，一个文件的当前状态将取决于此次修改的类型以及修改是否成功。具体来说：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;如果一次写入操作成功且没有与其他并发的写入操作发生重叠，那这部分的文件是确定的（同时也是一致的）。&lt;/li&gt;
    &lt;li&gt;如果有若干个写入操作并发地执行成功，那么这部分文件会是一致的但会是不确定的：在这种情况下，客户端所能看到的数据通常不能直接体现出其中的任何一次修改。也就是说，操作成功执行了，但是有的操作的数据改变被覆盖了，客户端看不到被覆盖的数据改变。&lt;/li&gt;
    &lt;li&gt;失败的写入操作会让文件进入不一致的状态。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;论文中给出了总结的表格：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/consistency.png&quot; alt=&quot;consistency&quot; /&gt;&lt;/p&gt;

&lt;p&gt;至于为什么追加（Append）是Defined但是有可能是不一致是因为：在追加写操作失败时，为了保证数据的偏移，可能为填充重复的数据，此时导致了不一致。但是失败的操作会被再次执行，此时又保证了数据的一致性。而由于使用的是追加，所以任何数据的改动都可以观察到，所以是Defined。&lt;/p&gt;

&lt;h3 id=&quot;快照snapshot&quot;&gt;快照（Snapshot）&lt;/h3&gt;

&lt;p&gt;这里的快照的目的不同于Raft中的压缩，它仅仅驶出为了生成一个新的Replica，可以看做是一个简单的复制操作。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;在 Master 接收到快照请求后，它首先会撤回相关Chunk  Server的 Lease，保证在创建快照的过程中，客户端对不会对相关的Chunk Server进行写操作或者追加操作。&lt;/li&gt;
    &lt;li&gt;当Lease收回后，Master会先将相关的改动写入日志，然后对自己管理的命名空间进行复制操作，复制产生的新记录指向原本的 Chunk。&lt;/li&gt;
    &lt;li&gt;当有客户端尝试对新的Chunk Server进行写入时，Master 会注意到这个 Chunk 的引用计数大于1（可能是一个标记）。此时，Master 会为要读取的Chunk生成一个Handle，然后通知所有持有这些 Chunk 的 Chunk Server 在本地复制并使用出新的 Chunk，然后再返回给客户端&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;我想GFS提供快照的原因可能是为了在一个副本损坏时，从Primary Replica或者其他副本复制数据，然后用新的节点代替损坏的节点。&lt;/p&gt;

&lt;h3 id=&quot;垃圾回收&quot;&gt;垃圾回收&lt;/h3&gt;

&lt;p&gt;当GFS收到删除文件的请求时，它并不直接删除文件，而是给文件打上删除的时间戳并将其命名为掩藏文件（文件开头加”.”）。在周期性扫描过程中，当发现文件的删除时间超过设定期限后，才真正地将文件删除。Google认为其有以下优点&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;对于大规模的分布式系统来说，这样的机制更为&lt;strong&gt;可靠&lt;/strong&gt;：在 Chunk 创建时，创建操作可能在某些 Chunk Server 上成功了，在其他 Chunk Server 上失败了，这导致某些 Chunk Server 上可能存在 Master 不知道的 Replica。除此以外，删除 Replica 的请求可能会发送失败，Master 会需要记得尝试重发。相比之下，由 Chunk Server 主动地删除 Replica 能够以一种更为统一的方式解决以上的问题&lt;/li&gt;
    &lt;li&gt;这样的删除机制将存储回收过程与 Master 日常的周期扫描过程合并在了一起，这就使得这些操作可以以批的形式进行处理，以减少资源损耗；除外，这样也得以让 Master 选择在相对空闲的时候进行这些操作&lt;/li&gt;
    &lt;li&gt;用户发送删除请求和数据被实际删除之间的延迟也有效避免了用户误操作的问题&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;高可用&quot;&gt;高可用&lt;/h3&gt;

&lt;p&gt;论文中主要探讨了三个方面的高可用，分别是Master，Chunk Server和数据完整性。&lt;/p&gt;

&lt;p&gt;当Master崩溃时，有两种情况，一是进程崩溃但是服务器没有，这种情况下，重开一个进程即可。另一种情况是整个机器崩溃了，在GFS还有被称为Shadow Master的机器，复制Master节点的信息。当Master机器崩溃后，Shadow Master会接替Master进行服务，但是仅提供读取操作的服务，不能更改信息。&lt;/p&gt;

&lt;p&gt;当Chunk Server崩溃时，Master会安排新的Replica代替它，从其他Replica复制原始数据。而当Chunk Server恢复时，由于数据不同步，不应该提供服务，Master就需要区别新旧Chunk Server。GFS使用版本号来标记这个信息，每分配以此Lease，版本号就会增加并同步给其他Replica，而由于Chunk Server崩溃后不能更新，我们就能从版本号上辨别新旧。&lt;/p&gt;

&lt;p&gt;由于写操作和追加操作可能不成功，所以数据可能会损坏。GFS使用检验和检查是否损坏，每次客户端读取数据时，Chunk Server都会检查检验和，一旦发现损坏，就会向Master报告。Master则会将请求发送给其他Replica，并从其他Replica复制数据到该机器。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;2003年GFS的横空出世具有划时代的意义，它标志着学术上的分布式理论和一些实验性质的尝试在工业界有了大规模商用的案例，尤其还是在Google这样的公司。它的系统设计在后续的系统中被屡次参考复用，而其设计确实有独到之处，比如Master负责控制流而完全将数据流从其剥离，这样的设计不能不说是优雅。这篇论文催生了HDFS，至今仍被许多公司使用，足以可见其影响力之大。哪怕这篇论文距离今天已经有快20年的时间，GFS在Google内部迭代多次，但是其设计仍然值得每一个分布式程序员学习。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">谷歌在2003到2006年间发表了三篇论文，《MapReduce:Simplified Data Processing on Large Clusters》，《Bigtable:A Distributed Storage System for Structured Data》和《The Google File System》介绍了Google如何对大规模数据进行存储和分析。这三篇论文开启了工业界的大数据时代，被称为Google的三驾马车。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">浅谈Amazon Aurora</title>
      <link href="http://localhost:4000/%E6%B5%85%E8%B0%88Amazon-Aurora" rel="alternate" type="text/html" title="浅谈Amazon Aurora" />
      <published>2020-08-02T18:00:00+08:00</published>
      <updated>2020-08-02T18:00:00+08:00</updated>
      <id>http://localhost:4000/%E6%B5%85%E8%B0%88Amazon%20Aurora</id>
      <content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Amazon-Aurora">&lt;p&gt;上图为极地极光&lt;/p&gt;

&lt;p&gt;Amazon在2017年的SIGMOD上发表了《Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases》并在对Amazon Aurora进行了介绍，简要描述了他们由于对传统MySQL性能的不满，而设计了Aurora来代替，其性能有相当大的提升。从时间和公司我们就可以看出，这是比较新的工业界的解决方案，有很高的学习参考价值。&lt;/p&gt;

&lt;h2 id=&quot;aurora的总体架构&quot;&gt;Aurora的总体架构&lt;/h2&gt;

&lt;p&gt;虽然论文中在结尾时才对其作出总结，但是在开头就点名其架构，再步步深入会更加合理。下面是Aurora的总体架构图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Aurora Architecture.png&quot; alt=&quot;Aurora Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;需要指出的是，由于Aurora是为了代替MySQL，而MySQL用于关系型数据库，所以Aurora仅负责处理关系型数据库的服务，即RDS（Relational Database Service）。我们其实可以从图中看出相当多的信息，Aurora仅有Primary RW（Read/Write） DB一个主节点用于处理写请求，而其余的则为从节点Secondary RO（Read-Only） DB用于处理读请求，论文中指出Secondary RO DB可以多达15个。另外，每个Aurora配备六个存储节点，其中有两个节点使用Amazon Simple Storage Service（S3）存储技术进行备份，而剩余4个节点则直接存储在本地的SSD上。&lt;/p&gt;

&lt;p&gt;用户的应用通过Customer VPC接入，然后可以读写位于不同AZ（Availability Zone）的数据库。而不同的AZ分布于全球的不同的Region中。当用户的请求发送到Primary RW DB时，RDS HM（Host Manager）会检测到请求，并调用Aurora进行相应的操作。如果是写操作，则将相关信息发送给Secondary RO DB进行备份，同时将命令写入存储节点。如果是读操作，则直接从存储节点读取数据返回。&lt;/p&gt;

&lt;h2 id=&quot;使用传统mysql遇到的问题&quot;&gt;使用传统MySQL遇到的问题&lt;/h2&gt;

&lt;p&gt;Amazon在日常开发和维护中发现，计算能力和存储性能已经不再是其工作的瓶颈了，取而代之的是网络的流量。其实对于Amazon来说，只要有钱，CPU能用最好的就能解决计算能力的问题，机械硬盘不够用固态硬盘，固态硬盘不够就上内存，存储性能也解决了，但是网络的延迟靠大带宽是很难解决的，而拉近机房位置也是有上限的，必须要从业务逻辑和服务组件上找问题。所以他们发现了MySQL在分布式系统中消耗了大量的流量，还提高了延迟。具体如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/MySQL network.png&quot; alt=&quot;MySQL network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从图中我们可以看出，传统的MySQL如果想要执行一次写入操作必须经历以下几步：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;主节点将数据写入EBS1&lt;/li&gt;
    &lt;li&gt;EBS1将数据写入备份镜像EBS2&lt;/li&gt;
    &lt;li&gt;主节点将相关数据发送给从节点&lt;/li&gt;
    &lt;li&gt;从节点将数据写入EBS3&lt;/li&gt;
    &lt;li&gt;EBS3将数据写入EBS4&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;其中，第1,3,5步是串行的，也就是说，只有第1步完成了，才能执行第3步，第3步完成了才能执行第5步。这无疑增加了服务器返回数据的延迟。另外传统的MySQL在写入和传输数据时还需要很多的额外信息，这又增加了网络带宽的消耗。也就是说，MySQL的使用在分布式系统产生了两个问题&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;应答延迟太高&lt;/li&gt;
    &lt;li&gt;消耗网络带宽太多&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;所以当Amazon发现使用传统MySQL的弊端之后，决定设计新的组件来代替MySQL以解决上述两个问题。&lt;/p&gt;

&lt;h2 id=&quot;the-log-is-the-database&quot;&gt;The Log Is The Database&lt;/h2&gt;

&lt;p&gt;上面我们提到，MySQL在同步数据的过程中发送的信息太多，这该怎么办呢？Amazon也算是家大业大，直接自己重新设计标准，以往的数据库是真的数据库，现在他们用WAL也就是Log来整合所有有用的信息并删去无用的信息，既减少了数据传输量又保证了需要保留的信息。同时，他们使用了链式复制结构代替主从结构，简化了保证数据一致性的复杂度。具体架构如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Aurora Network.png&quot; alt=&quot;Aurora Network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以三个副本为例，当位于AZ1的主节点收到写请求后，它将请求的相关数据直接写入六个存储节点中，然后，将数据和一些额外的信息通过链式复制结构传递给位于AZ2和AZ3其他节点。和上图进行对比，明显可以看到主从节点之间网络通信中传输的数据减少了，主节点向存储节点写入数据时也从五种数据变为一种。这里要特别指出的是，此处的数据已经从MySQ定义的Log变为Amazon为Aurora量身定制的Log。由于需要传输数据量的减少，同步所消耗的网络带宽也大幅地减少了。&lt;/p&gt;

&lt;p&gt;另外，因为主节点负责将Log写入存储节点，而从节点仅存储Log不需要负责写入存储节点，这样就减少了在MySQL中额外的第四步和第五步操作的时间。而MySQL中的两级EBS存储操作也由一级Quorum的代替，就像上一篇文章提到的，两级存储的时间是两次操作的时间之和，而一级的Quorum操作的时间则是取决于Quorum中最长的应答时间。这样，Aurora也优化了应答延迟的时间。&lt;/p&gt;

&lt;p&gt;在上一篇文章中我们提到，链式复制仅仅适用于节点较少且物理位置较近的情况。很巧的是，Amazon提供的服务中副本不会超过15个，而经典的情况仅有3个，而虽然不同AZ可能会跨节点，但是Amazon实在有钱，能让AZ之间的延迟低于2ms。在这种情况下，使用链式复制实在合适不过，还大大降低了保证共识的复杂度，简直是完美的设计。&lt;/p&gt;

&lt;h2 id=&quot;storage-node&quot;&gt;Storage Node&lt;/h2&gt;

&lt;p&gt;上面，我们提到主节点将Redo Log写入存储节点。但是，此时Redo Log还未执行，需要在存储节点中执行相应的操作后才算真正完成。下面，我们再来看看Redo Log到达存储节点以后需要进行哪些操作。论文中给出的流程图如下。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Aurora Storage Node.png&quot; alt=&quot;Aurora Storage Node&quot; /&gt;&lt;/p&gt;

&lt;p&gt;具体的流程解释如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;存储节点通过Incoming Queue接受主节点的Log。&lt;/li&gt;
    &lt;li&gt;存储节点将Log存到本地硬盘后向主节点发送ACK，用以确认Quorum。&lt;/li&gt;
    &lt;li&gt;由于网络的不可靠和Quorum机制，当前存储节点可能缺失了部分Log。在这一步，它将Log排序并找出缺失的Log。&lt;/li&gt;
    &lt;li&gt;通过和其他存储节点进行交换信息，将缺失的Log复制到本地，将所有Log填充完整。&lt;/li&gt;
    &lt;li&gt;到目前为止，系统中存储的仍是Log而非用户需要数据，这一步执行Log对应的操作，并写入数据库中。&lt;/li&gt;
    &lt;li&gt;定期地将数据存为快照并存入Amazon S3中。&lt;/li&gt;
    &lt;li&gt;定期地进行垃圾收集，删除过期数据。&lt;/li&gt;
    &lt;li&gt;用CRC定期检验数据。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;从流程中我们可以看到，只有第一步和第二步可能影响应答延迟，其余的步骤都由存储节点在后台执行。这样一来，因为无需等待执行完毕，应答延迟就进一步降低了。&lt;/p&gt;

&lt;h2 id=&quot;读写操作&quot;&gt;读写操作&lt;/h2&gt;

&lt;p&gt;Amazon在设计Log时，为了实现一些功能给它添加了一些标记，具体如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;LSN：Log Sequence Number，相当于Log的自增主键，类似于Raft中的Index。&lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;VCL：Volume Complete LSN，受到Quorum承认的最大LSN。&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;CPLs：Consistenc yPoint LSNs，单个存储节点中已经收到ACK的最大LSN，所以每个节点各一个&lt;/li&gt;
    &lt;li&gt;VDL：Volume Durable LSN，已经持久化最大的LSN，也就是CPLs中最大的LSN&lt;/li&gt;
    &lt;li&gt;SCL：Segment Complete LSN，由每个段维护，代表段中已经持久化的最大LSN&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;在读写，复制和提交等操作中，Aurora会使用这些标记实现对应功能。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;写操作&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们在之前的文章中提到，如果未执行的Log积压过多会产生很不好的后果。所以在写操作时，Aurora会设置VDL+N作为未分配LSN的上限，通过设置N的值来限制未写入磁盘的Log的条数。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;读操作&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;为了提高效率，Aurora会在缓存中先查找是否有需要读取的数据，如果没有，再置换页面。在这里，Aurora要求置换页面中的LSN&amp;gt;VDL以确保数据为最新版本。这保证了所有页面的更新都已经持久化到日志并且在缓存区没有该数据页的情况下，可以根据 VDL 获取最新版本数据。&lt;/p&gt;

&lt;h2 id=&quot;fault-tolerance&quot;&gt;Fault-Tolerance&lt;/h2&gt;

&lt;p&gt;Aurora 将数据库文件切分成 10GB 大小的段（Segment）。在崩溃恢复的时候，Aurora要通过Quorum读得到VDL，并将大于此的Log阶段。由于在写操作时设置了LSN的上限，所以可以将需要Redo Log的LSN上限设置为VDL+N。然后重做已经标记的Log，就能恢复到初始状态，Aurora实验显示这个过程相当地快。&lt;/p&gt;

&lt;h2 id=&quot;不一样的quorum&quot;&gt;不一样的Quorum&lt;/h2&gt;

&lt;p&gt;我们上面提过，Aurora的六个存储节点部署在3个AZ中，每个AZ运行两个存储节点。Amazon考虑到可能整个AZ挂掉，导致两个存储节点崩溃，而AZ又有可能在同一个Region中，所以Aurora考虑的最坏情况是一个AZ崩溃加上一个存储节点崩溃，即AZ+1。&lt;/p&gt;

&lt;p&gt;Aurora提出了以下两个要求：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;在AZ+1崩溃的情况下不丢失数据，也就是保证读数据能力。&lt;/li&gt;
    &lt;li&gt;在AZ崩溃的情况下保证写数据能力。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;于是Aurora提出了读写两种情况的Quorum。在写情况下，需要六个节点中的四张票，即4/6。在读情况下，仅需要六个节点中的三张票，即3/6.&lt;/p&gt;

&lt;p&gt;很明显，写操作的Quorum和我们之前在Paxos和Raft中讨论的Quorum是一致的，也是2f+1需要f+1张票。而由于写操作每次至少写入四个节点，那么根据抽屉原理，每两次写操作至少有一个节点重复，那么读操作无论读哪一半都能在三个节点中读取到最新的全部数据。以此类推，哪怕一半的节点崩溃，Aurora也能读取到最新最全的数据。&lt;/p&gt;

&lt;p&gt;但是，要特别指出的是，读Quorum的要求仅仅在恢复时才使用，正常读是不需要的。&lt;/p&gt;

&lt;p&gt;从这里来看，其实这个Quorum也就是Raft中2f+1个节点容许f个崩溃的另一种说法。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;Amazon Aurora中描述的技术看起似乎很通用，使用WAL代替MySQL的信息，在存储节点执行命令而不是在本机执行，使用Chain Replication等等。但是能将这些技术恰到好处地使用在实际的系统中，并进行优化才是大厂的技术底蕴。比如Log的设计这一块，论文中就介绍地相当模糊，读写操作的细节也没有纰漏。这篇论文恐怕只算是对Aurora的惊鸿一瞥，真的想了解实现细节还得去Amazon内部看看。毕竟，Aurora在每个事务的IO花费的1/8，而事务处理量是MySQL的35倍，这可不是简单的系统设计就能完成的。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">Amazon在2017年的SIGMOD上发表了论文对Amazon Aurora进行了介绍，简要描述了他们由于对传统MySQL性能的不满，而设计了Aurora来代替，其性能有相当大的提升。从时间和公司我们就可以看出，这是比较新的工业界的解决方案，有很高的学习参考价值。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">浅谈Primary-Back Replication和Chain Replication</title>
      <link href="http://localhost:4000/%E6%B5%85%E8%B0%88Primary-Back-Replication%E5%92%8CChain-Replication" rel="alternate" type="text/html" title="浅谈Primary-Back Replication和Chain Replication" />
      <published>2020-07-26T18:00:00+08:00</published>
      <updated>2020-07-26T18:00:00+08:00</updated>
      <id>http://localhost:4000/%E6%B5%85%E8%B0%88Primary-Back%20Replication%E5%92%8CChain%20Replication</id>
      <content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Primary-Back-Replication%E5%92%8CChain-Replication">&lt;p&gt;上图为Google提供的贪吃蛇游戏&lt;/p&gt;

&lt;p&gt;前面两篇文章，我们讨论了分布式系统中为了维护一致性所使用的共识性算法Paxos和Raft。这些算法保证了各机器之间数据的一致性，但是Paxos算法针对的是平等的Replication策略，而Raft算法针对的是Primary-Back Replication策略。Replication策略和共识性算法的目的不同，它的设计是为了实现容错（Fault-Tolerance），即在一部分机器不可用后，仍能保证正常提供服务。下面，我将简要描述一下6.824中讨论的两种复制模型Primary-Back Replication和Chain Replication。&lt;/p&gt;

&lt;h2 id=&quot;机器同步&quot;&gt;机器同步&lt;/h2&gt;

&lt;p&gt;为了能够实现集群能够及时接管服务，并提供相同的服务，集群需要保证不同机器之间的数据一致性。我们上面提到，要实现共识需要使用Paxos或者Raft算法，而我们需要对什么东西达成共识呢？这就要提到实现机器同步的两种方法，分别是State Transfer和Replicated state machine，其中差别如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;State Transfer：主机器将本身的状态变化全部传送给其他机器。&lt;/p&gt;

  &lt;p&gt;Replicated State Machine：由于业务的很多情况符合状态机的定义，所以只要保证所有机器初试状态相同，那么以相同的顺序执行相同指令后，它们的状态也是相同的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;举个例子，客户端发送{X=1，Y=X}的指令给集群。在State Transfer中，主机器执行这个操作后，将它发生变化的数据和值发送给其他机器，也就是{X=1，Y=1}，然后其他机器将自身的数据修改为对应的值，以此实现各机器的数据一致性。而在Replication State Machine中，主机器发送{X=1，Y=X}的指令给其他机器，要求其他机器也执行这一操作，按照状态机的状态变化，也能实现数据一致性。&lt;/p&gt;

&lt;p&gt;State Transfer的优点是不会消耗其他机器的计算能力，但是在实际情况下，发生变化的数据量比较大，它对集群的带宽和延迟要求都很搞。Replicated State Machine由于只发送指令，所以传输的数据量比较小，但是它要求各机器在本地执行指令，所以对机器的计算能力有一定要求。根据实际情况，一般集群中各机器的性能相当，所以计算能力不会成为瓶颈，而为了保证快速响应客户要求，必须尽量减少传输的延迟，State Transfer由于一次性传输的数据量大，所以在网络的传输延迟上很难实现要求。因此Replicated State Machine成为实际上的解决方案。&lt;/p&gt;

&lt;h2 id=&quot;vmft&quot;&gt;VM—FT&lt;/h2&gt;

&lt;p&gt;VMware在&lt;a href=&quot;http://nil.csail.mit.edu/6.824/2017/papers/vm-ft.pdf&quot;&gt;《The Design of a Practical System for Fault-Tolerant Virtual Machines》&lt;/a&gt;中介绍了基于虚拟机实现的Primary-Backup Replication的容错解决方案。其本质就是使用Replicated State Machine实现主从的数据一致性。&lt;/p&gt;

&lt;h3 id=&quot;deterministic-replay&quot;&gt;Deterministic Replay&lt;/h3&gt;

&lt;p&gt;在VM-FT中，它们将指令在副机器的执行称为Deterministic Replay。由于和数据库软件不同，有一些系统命令和时间息息相关，比如中断和时间戳，考虑到传输延迟，这就会使命令执行的结果不同，从而导致数据的不一致。所以这就给系统设计带来一些问题，论文将此分为三个目标&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;正确收集所有的非确定（Non-Deterministic）事件和操作，保证有在执行时有足够的信息在Replay时来修正操作&lt;/li&gt;
    &lt;li&gt;正确地在副机器上Replay非确定的事件和操作&lt;/li&gt;
    &lt;li&gt;保证以上两点不会降低机器性能&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;由于这个系统运行在VMware上，所以VMware可以收集所有它想要的信息并将其存储在文件中，能够正确地实现第一个目标。然后，主机器通过Logging Channel将需要执行的操作和相关的补充信息传递给副机器，因此可以保证第二个目标。在实际的测试中，VMware的实现也能够保证机器的性能，满足了第三点。&lt;/p&gt;

&lt;h3 id=&quot;ft-protocol&quot;&gt;FT Protocol&lt;/h3&gt;

&lt;p&gt;由于主从机器通过Logging Channel进行通信，所以其中会产生一定的延迟而导致问题。假设主机器在收到请求后，在本地执行了该请求并将其传送给副机器，同时将结果反馈给客户端，但是副机器由于一些问题没有成功执行该指令，此时主从机器的数据是不一致的。如果主机器在此时崩溃，集群交由副机器负责，那么客户端再次查询数据就会得到不一样的结果。&lt;/p&gt;

&lt;p&gt;因此，VMware提出了如下的要求：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;输出规则&lt;/strong&gt;：主机器只有在副机器已经接收并返回了该指令的ACK 后，才能回复客户端。其中，副机器必须成功执行了该指令后才能回复ACK。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其实就是要求主机器在副机器也执行完指令后，实现了整个系统的一致性，再回复客户端。其具体流程如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/FT Protocol.png&quot; alt=&quot;FT Protocol&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;错误检测和恢复&quot;&gt;错误检测和恢复&lt;/h3&gt;

&lt;p&gt;为了检测错误，VM-FT使用了以下两种手段&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;使用定期发送UDP来实现各机器间的Heartbeating&lt;/li&gt;
    &lt;li&gt;由于整个系统基于VMware，所以可以通过检测Logging Channel的流量来查看是否正常工作&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;第一种方法和Raft中使用的类似，都是通过周期性地Heartbeats来检测是否活跃。第二种方法基于系统不断接受客户端请求的事实，只要客户发送了请求，为了实现主从同步，两者必须通过Logging Channel发送指令或者返回ACK，而如果相当长的时间内没有流量的话，就说明至少主机器可能已经崩溃。&lt;/p&gt;

&lt;p&gt;一旦检测到主机器崩溃，那么副机器就会将Logging Channel中所有的指令执行完毕，以此到达主机器崩溃前的状态，然后在这个状态下回复客户端。而如果副机器崩溃了，主机器仅仅需要不再发送同步消息即可。&lt;/p&gt;

&lt;h3 id=&quot;logging-channel优化&quot;&gt;Logging Channel优化&lt;/h3&gt;

&lt;p&gt;为了保证主从的一致性，VM-FT还对Logging Channel进行了优化。如果副机器执行很慢，那么Logging Channel中的留存的信息过多，甚至填满了预留的空间，那么收到客户端请求时，主机器必须等待有空余空间才能发出请求，这使得客户端的体验很差。另一方面，如果主机器崩溃，副机器为了赶上主机器进度而需要花费的时间过多也会影响系统。所以VM-FT在通信中添加额外的信息，以此检测主从机器在执行同一指令上产生的延迟。如果这个延迟过大，VM-FT会减少分配给主机器的CPU算力，增加副机器的CPU算力，以此保证主从机器间的相对同步。&lt;/p&gt;

&lt;h3 id=&quot;brain-split&quot;&gt;Brain-Split&lt;/h3&gt;

&lt;p&gt;假设两台机器都能正常工作，但是由于通信原因，他们不能通过Heartbeat来确认对方正常工作，那么主机器仍然保持主机器的角色，而副机器则会将自己晋升为主机器，此时系统中有两个主机器，产生了脑裂（Brain-Split）现象。很不幸，这种情况不能通过系统自身解决，因为他们不能正常沟通，而在VM-FT中也没有使用基于Quorum的选举机制，所以只能通过外部解决。&lt;/p&gt;

&lt;p&gt;在VM-FT中，由于主从共享磁盘，所以可以通过磁盘这个中介来解决。可以在磁盘中记录当前是否有主机器，如果有机器想要成为主机器，那么它必须访问磁盘，读取这个字段，确认当前没有主机器，才能成为主机器。在论文中，这个过程被称为test-and-set 。而在分布式的系统中，则必须通过第三方服务器才能实现这一功能。&lt;/p&gt;

&lt;h3 id=&quot;vm-ft总结&quot;&gt;VM-FT总结&lt;/h3&gt;

&lt;p&gt;这篇论文发表于2010年，虽然距离现在有一段时间了，但是仍能看到后续设计的一些影子。比如Logging Channel中设计了Buffer来保存指令，这个设计其实就相当于WAL，它对Brain-Split的解决方案在今天仍然实用。不过，由于整体的实现是基于VMware，所以和实际的分布式系统仍然有一点差别。总而言之，我们对这篇论文最大的学习是对主从设计系统的简单了解。&lt;/p&gt;

&lt;p&gt;遗憾的是，这篇文章没有讨论多个副机器的情况，也就是说，没有使用Paxos算法实现多个副机器的一致性。&lt;/p&gt;

&lt;h2 id=&quot;craq&quot;&gt;CRAQ&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://pdos.csail.mit.edu/6.824/papers/craq.pdf&quot;&gt;《Object Storage on CRAQ High-throughput chain replication for read-mostly workloads》&lt;/a&gt;使用了一种类似于链表的复制模型Chain Replication。和Raft以及上面提到的主从模型不同，这个模型用很简单的方法就保证了数据的一致性。&lt;/p&gt;

&lt;h3 id=&quot;chain-replication&quot;&gt;Chain Replication&lt;/h3&gt;

&lt;p&gt;Chain Replication将每个结点连接成链表。其中写请求只能由头结点处理，读请求只能由尾节点负责。具体流程如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cr.png&quot; alt=&quot;cr&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;写请求：&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;头节点接受写请求，执行完毕后将其转发给后一个节点&lt;/li&gt;
    &lt;li&gt;所有节点都执行相关指令，同时将请求转发到下一个节点&lt;/li&gt;
    &lt;li&gt;等到尾节点也执行完相关指令后，会给上一个节点发送ACK&lt;/li&gt;
    &lt;li&gt;所有节点反向发送ACK，直到头节点&lt;/li&gt;
    &lt;li&gt;头节点收到ACK后，将结果返回给客户端&lt;/li&gt;
  &lt;/ol&gt;

  &lt;p&gt;读请求：&lt;/p&gt;

  &lt;p&gt;​	读请求只能由尾节点处理，尾节点返回当前数据&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们可以看到，只有当所有节点都执行完相关操作后，写请求才能得到结果，能保证写的一致性。由于尾节点是最后更新数据的节点，所以尾节点中的数据就是当前系统最新的一致性数据，读请求总能得到已经达成一致的数据。这个模型很简单地实现了读写的数据一致性，但是问题是，所有的读操作都由尾节点来处理，相当于整个系统的读取压力都来到了尾节点，尾节点很可能崩溃。所以这篇论文提出了一些改进。&lt;/p&gt;

&lt;h3 id=&quot;chain-replication-with-apportioned-queriescraq&quot;&gt;Chain Replication with Apportioned Queries（CRAQ）&lt;/h3&gt;

&lt;p&gt;这篇论文为了减少尾节点的压力，允许所有结点处理读请求，也就是名字中Apportioned Queries的含义。但是，由于读请求和写请求时并发的，有可能出现结点数据不一致时的读请求。所以这里还需要额外的处理。&lt;/p&gt;

&lt;p&gt;首先考虑最简单的情况，即整个系统数据保持一致的情况。这时，无论从哪个节点读取，数据都是正常的。示意图如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/craq clean read.png&quot; alt=&quot;craq&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其次，我们来考虑在处理写请求的同时，处理读请求的情况。其示意图如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/craq dirty read.png&quot; alt=&quot;craq read&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图中，头节点接受了写请求，并将其传送给第二个节点，第二个节点已经处理完毕。但此时，第三个节点和尾节点都还未进行相应的处理，导致前两个节点和后两个节点的数据不一致。CRAQ通过为数据添加版本并存储多个版本的数据来解决这一问题。&lt;/p&gt;

&lt;p&gt;一次完整的写请求完成后，整个系统仍然能保证数据一致性，唯一不能保证的就是上图中写到一半的情况。其中，已经处理写请求的节点和尾节点数据不一致，而未处理写请求的节点则保持一致。所以，CRAQ要求处理完写请求的节点把自己标记为Dirty，而未处理的则无需标记。另一方面，尾节点始终存储能被整个系统承认的数据，即最新的保持了一致性的数据。所以，为了确定当前应该返回的数据，Dirty节点应该询问尾节点应当返回数据的版本。&lt;/p&gt;

&lt;p&gt;这张图中的绿色圆柱表示数据库，我们可以看到，已经处理完写请求的两个节点的数据库中存有两个版本V1和V2的数据K，而后两个节点则只有V1版本的数据K。此时，第二个节点收到读请求，由于其数据库内有两个版本的数据K，而且本身标记为Dirty，所以它需要向尾节点发送询问请求。尾节点回复其应返回版本V1的数据K，此节点就按照尾节点的指示回复客户端V1版本的数据K。&lt;/p&gt;

&lt;p&gt;综上所述，由于无论在CR还是CRAQ中，尾节点所存储的数据都是最新的具有一致性的数据，那么在可能出现的并发读写情况下，只需要向尾节点询问相关数据的版本，就可以确定应该回复的数据。&lt;/p&gt;

&lt;h3 id=&quot;membership-change-和-failure-recovery&quot;&gt;Membership Change 和 Failure Recovery&lt;/h3&gt;

&lt;p&gt;由于CRAQ中节点的结构类似于双向链表，所以Failure Recovery的策略其实就是Membership Change中的删除节点类似，也就是双向链表中节点的删除。设要删除的节点为D，某节点为N，其具体情况如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果D是N的后继：N需要把其所有数据传送给其新的后继。因为D有可能是上一节图中第二个节点的情况，那么写请求就不能传送给后继节点。如果D是尾节点，那么N需要D把所有ACK都反向传输完毕后再删除节点。&lt;/p&gt;

  &lt;p&gt;如果D是N的前驱：N需要把数据传送给其新的前驱。同时，如果D是头节点，N需要成为新的头结点。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;节点的添加大同小异，只需要在新节点A是头结点和尾节点时进行特殊处理即可。&lt;/p&gt;

&lt;h3 id=&quot;craq总结&quot;&gt;CRAQ总结&lt;/h3&gt;

&lt;p&gt;CRAQ对原有的Chain Replication进行了改进，平摊了读请求的压力，还用简单的模型保证了数据的强一致性。但是另一方面，从写请求的流程来看，需要线性地流经每一个节点势必增加系统的延迟。而Raft等算法则取决于单个机器的最长时间，如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/compare.png&quot; alt=&quot;compare&quot; /&gt;&lt;/p&gt;

&lt;p&gt;假设使用RAFT和CRAQ的系统中都有三个节点，黄色表示向非主节点写入指令需要的时间，蓝色表示该节点返回ACK花费的时间。可以看到，由于RAFT算法是并行的，它的实际时间取决于单个节点花费的最长时间。而CRAQ是串行的，它的时间是所有结点花费的时间总和。&lt;/p&gt;

&lt;p&gt;如果想要使用CRAQ的模型进行备份，那么节点数量一定不能太多，最好节点不要跨机房，否则在网络传输上花费的时间就十分庞大了。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">前面两篇文章，我们讨论了分布式系统中为了维护一致性所使用的共识性算法Paxos和Raft。这些算法保证了各机器之间数据的一致性，但是Paxos算法针对的是平等的Replication策略，而Raft算法针对的是Primary-Back Replication策略。Replication策略和共识性算法的目的不同，它的设计是为了实现容错（Fault-Tolerance），即在一部分机器不可用后，仍能保证正常提供服务。下面，我将简要描述一下6.824中讨论的两种复制模型Primary-Back Replication和Chain Replication。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">浅谈Raft</title>
      <link href="http://localhost:4000/%E8%B0%88%E8%B0%88Raft" rel="alternate" type="text/html" title="浅谈Raft" />
      <published>2020-07-19T18:00:00+08:00</published>
      <updated>2020-07-19T18:00:00+08:00</updated>
      <id>http://localhost:4000/%E8%B0%88%E8%B0%88Raft</id>
      <content type="html" xml:base="http://localhost:4000/%E8%B0%88%E8%B0%88Raft">&lt;p&gt;上图为Steam同名游戏———RAFT&lt;/p&gt;

&lt;p&gt;Raft算法是由Diego Ongaro和John Ousterhout于2014年提出的共识性算法。在斯坦福当助教时，他发现学生很难理解Paxos算法，所以他希望能用一种更简单易懂的算法来代替Paxos，以此为契机，他把便于理解作为目的，提出了Raft算法。&lt;/p&gt;

&lt;h2 id=&quot;paxos算法的不足&quot;&gt;Paxos算法的不足&lt;/h2&gt;

&lt;p&gt;了解过Paxos算法的朋友都知道，Paxos算法最后提出了2PC的机制来保证共识。但是，我们也提到了，两段式的请求消耗过大，而且多Proposer的前提也很难实现，Proposal的数据结构也未提及。总之，Basic Paxos算法在工程上有非常多的实现困难。另一方面，也就是上文提到的，非常难于理解。而Raft正是为了解决这两大问题，它提出并总结了系统中节点的角色，各角色的功能和职责，需要使用的RPC及其参数和逻辑，甚至连数据结构都清楚地给出了定义。这简直就像手把手教开发人员实现算法。而也正是由于清晰详细的介绍，Raft算法理解起来也相当地容易，顺带一提，可能是为了“便于理解”，论文本身的行文和用词也相当地简单。&lt;/p&gt;

&lt;h2 id=&quot;raft&quot;&gt;Raft&lt;/h2&gt;

&lt;p&gt;Raft算法使用Leader代替Paxos中的多个Proposer，使用Log Entry代替Proposal，以此简化问题。也因此，Raft算法将整个共识性问题划分为Leader Election，Log Replication两大块，同时，为了解决在崩溃时可能出现的问题，还提出了Safety的问题。此外，Raft还提供了Snapshot和Membership Change的解决方案。&lt;/p&gt;

&lt;p&gt;Raft将系统中的角色划分为三种，Leader，Candidate和Follower。整个系统只有三种种通信方式，AppendEntryies PRC，RequestVote RPC和InstallSnapshot RPC。其中，AppendEntries RPC只能由Leader发出，用于向Follower追加Log Entry或者广播Heartbeats，RequestVote只能由Candidate发出，用于发起Leader Election，InstallSnapshot RPC只能由Leader发出，用于向Follower发送快照。&lt;/p&gt;

&lt;h3 id=&quot;leader-election&quot;&gt;Leader Election&lt;/h3&gt;

&lt;p&gt;Raft将系统中的角色划分为三种，Leader，Candidate和Follower。所有机器初试为Follower，他们三者的转化关系如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/state transfer.png&quot; alt=&quot;state transfer&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Follower-&amp;gt;Candidate：当自身的计时器超时后，将自身转变为Candidate&lt;/p&gt;

  &lt;p&gt;Candidate-&amp;gt;Leader：当Candidate获得Quorum的选票时，成为Leader&lt;/p&gt;

  &lt;p&gt;Candidate-&amp;gt;Follower：当Candidate在RequestVote RPC中收到的Term大于该自己的Term，或者收到Term大于等于该自己Term的AppendEntries PRC时，就变为Follower&lt;/p&gt;

  &lt;p&gt;Leader-&amp;gt;Follower：当在RPC中收到的Term大于该机器的Term时，自动变为Follower&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;要特别说明的是，只有Candidate才能成为Leader，所以只有Candidate才能发起Leader Election。下面，我们逐条来分析这么做的理由。&lt;/p&gt;

&lt;p&gt;Follower-&amp;gt;Candidate：首先，我们要声明的是，Raft规定了一个时限，如果在这个时限内没有收到Heartbeat（即不携带信息的AppendEnyties RPC），那么就认为超时了。假设在初始系统中，所有的机器角色都是Follower，但是只有Candidate才能发起选举，那么要怎么才能得到Leader呢？于是，这一条的作用就出现了。由于系统中没有Leader，那么就不会有Heartbeats，也就是说必定有一个Follower会超时，按着这一条的转换关系，此Follower会转变成Candidate，然后发出Leader Election请求，直到系统中出现Leader。还有一种情况，本来正常工作的系统的Leader崩溃了，同样地，由于收不到Heartbeats，其中的Follower会转变成Candidate发起Leader Election，保证系统正常运行。&lt;/p&gt;

&lt;p&gt;Candidate-&amp;gt;Leader：这一条其实无需多言，按照Paxos中提出的Quorum机制，获得多数承认的机器可以成为Leader。&lt;/p&gt;

&lt;p&gt;Candidate-&amp;gt;Follower：不同的Term代表不同的Leader周期，而且Term是自增的。Candidate会在发起Leader Election之前将自己的Term加一，代表自己比之前的Leader更新。而如果收到了一个大于自己的Term的RequestVote RPC，这就意味有机器在时间上比自己更新，所以就应该主动退出竞争。如果仅仅收到和自己Term相同的RequestVote RPC，这意味着在这一轮选举中有竞争者，谁能成为Leader就要看网络的连通情况，这就无需变为Follower了。而如果它收到了一个AppendEntries RPC且Term大于等于自己，而此RPC仅可能由Leader发出，这就意味本轮已经选出Leader或者有更新的Leader了，此时就应该退出竞争。&lt;/p&gt;

&lt;p&gt;Leader-&amp;gt;Follower：我们知道Term的大小代表Leader的新旧，也就是说，当在RPC中接收到的Term大于自己时，有一个新的Leader被选出了。这种情况有可能由于暂时的网络故障，使得此Leader不能与其他机器连通，而当剩余机器选出Leader后网络恢复了，那么此机器中存储的数据很可能不够完整，所以应该让位给新的Leader。&lt;/p&gt;

&lt;p&gt;有了以上的要求，整个系统的角色转换已经可以顺利流转了。还需要关注的，是RequestVote RPC中的投票逻辑。在Raft算法中，使用“先来先投票”的原则，这个原则暗示了在多个Candidate竞争的过程中，与其他节点通信良好的会更有可能赢得选举。也就是说，选举出来的Leader与其他节点的通信延迟低，从而提升了系统的性能。但是，当本机器的Term大于RequestVote RPC中的Term时，说明本机器比发送请求的机器更新，此时应当拒绝请求。&lt;/p&gt;

&lt;p&gt;不过，假如有一种极端情况：在某种条件下，所有的节点同时成为Candidate。而按照上述策略，它们首先会投自己一票，从而产生人均一票的情况，然后重新开始选举，如此往复。为了避免这种情况，Raft使用等待随机时间后再发起选举的解决方案。&lt;/p&gt;

&lt;h3 id=&quot;log-replication&quot;&gt;Log Replication&lt;/h3&gt;

&lt;p&gt;Raft中的Log Entry对应于Paxos中的Proposal，但是它远比Propsal具体实际，也比Propsal强大。它是Write-ahead Log（WAL），也就是说，它在执行之前就被存储在可靠的磁盘上了，这种方案被广泛地用在实际的分布式系统中，以此提供一定的容错能力。&lt;/p&gt;

&lt;p&gt;在Raft中，Log Entry使用Term和Index来标记自己，以此区别不同Log Entry并比较其新旧程度。其中，Term代表不同的Leader周期，而Index代表在同一个Leader周期中的不同Log Entry。Log Entry的整个生命周期如下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Client向Leader发送请求{x-&amp;gt;1}，Leader将其包装成Log Entry，追加到自己的Log Entries尾部，此时该log Entry的状态为Append&lt;/li&gt;
    &lt;li&gt;Leader使用AppendEntries RPC将这条Log Entry广播给所有的Follower，要求Follower也将其追加到自己的Log Entries的尾部&lt;/li&gt;
    &lt;li&gt;当Follower完成后，会送回AppendEntries RPC的回答，这个回答可能是成功，也可能是失败。&lt;/li&gt;
    &lt;li&gt;如果Leader收到Quorum个的成功的回应，他会将其状态设为Commited，表示已经在Quorum个Follower中写入该请求。如果没有，则重新发送。&lt;/li&gt;
    &lt;li&gt;在将该Log Entry设为Commited后，Leader就可以在机器中执行{x-&amp;gt;1}，同时将结果返回给Client。此时，该Log Entry的状态为Applied。各Follower则会在后台选择时间执行该命令。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;综上所述，Log Entry一共有三种状态，Append，Commited和Applied。&lt;/p&gt;

&lt;p&gt;Append状态表示仅仅被追加到了Leader的Log Entries中，此时既未被其他机器承认，也未被执行。唯一的好消息是，它被存储在Leader的磁盘中了，哪怕Leader暂时崩溃了，只要系统恢复后该机器仍然是Leader，也能读取其中内容，继续向下执行操作。&lt;/p&gt;

&lt;p&gt;Commited状态表示此Log Entry已经得到了系统中Quorum个机器的共识，在这个状态下，它能够承受2f+1个机器中f个机器的崩溃，一旦系统恢复，它可以被安全地执行。&lt;/p&gt;

&lt;p&gt;Applied状态表示它已经被执行过了，只要处于该状态，那么在形成快照时，它就可以被删除。不过，由于各机器都在后台执行命令，Raft仅保证Log Entry最后能被执行但不保证执行的一致性。&lt;/p&gt;

&lt;p&gt;在追加Log Entry和维护Log Entry的过程中，Raft必须维护一下两条准则&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;如果在两个不同机器上的Log Entry拥有同样的Term和Index，那么这个Log Entry包含的Command相同&lt;/li&gt;
    &lt;li&gt;如果在两个不同机器上的Log Entry拥有同样的Term和Index，那么在这个Log Entry以前所有Log Entry都相同&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;由于每个Term仅有一个Leader，而Leader给每个Log Entry标记的Index不同，这就意味着Log Entry的Term和Index构成了全局唯一性。这个特性保证了第一条准则。&lt;/p&gt;

&lt;p&gt;第二个准则由AppendEntries RPC的内部逻辑保证。AppendEntries PRC的请求中带有PrevLogIndex和PrevLogTerm两个参数用来验证Follower中最新的Log Entry是否和Leader一致，这两个参数是AppendEntries RPC中Log Entry的前一个Log Entry的Index和Term。一旦发现不一致，Follower被要求删掉此Log Entry并返回失败，在一下次AppendEntries RPC时，Leader会将这两个参数改为当前Log Entry的前两个Log Entry的Index和Term，以此类推。直到找到Follower和Leader匹配的位置后，Leader会把从匹配位置开始直到最后的Log Entry全部发送给Follower。Leader通过这种强制更换的方式实现了第二条准则。这里要特别指出的是，在正常情况下不会出现Follower和Leader不匹配的情况，只有在机器崩溃的时候才可能出现，而这种强制更换掉的Log Entry中有两种情况，被删除和位置变更。其中被删除的Log Entry只有可能处于Append状态，位置变更的Log Entry则处于Commited状态。&lt;/p&gt;

&lt;p&gt;除此之外，AppendEntries RPC还要喝RequestVote RPC一样检查当前Term是否小于请求中的Term，否则将拒绝请求。在Leader的CommitedIndex大于本机器的CommitedIndex时，还应将本机器的CommitedIndex设为LeaderCommitedIndex和本机最新Index中的小的值，从而能跟上Leader的处理节奏。&lt;/p&gt;

&lt;h3 id=&quot;safety&quot;&gt;Safety&lt;/h3&gt;

&lt;p&gt;由于系统中的机器有可能发生故障和崩溃，而此时会产生许多意外的情况，所以Raft给Leader Election和Log Replication增加了一些限制来解决这些情况。&lt;/p&gt;

&lt;h4 id=&quot;election-resitriction&quot;&gt;Election Resitriction&lt;/h4&gt;

&lt;p&gt;如果按照原来的Leader Election的要求，那么我们可能会出现下面的情况。有一台原来网络状况不是那么好的机器，它能赶得上当前的Term，但是不能收到最新的Log Entry，假设它在同Term中总比其他机器少一个Log Entry。此时由于某种原因，它的网络状况改善了并按照选举原则拿到多数票，赢得了选举，成为Leader。那么，按照AppendEntries RPC的要求，它会删除掉其他机器中比它快的那一个Log Entry，然而这个Log Entry已经被Commited了，这就出现了问题，这个系统虽然保证了一致性但是却损失了信息。所以Raft在RequestVote RPC中添加了一个逻辑：通过和AppendEntries RPC一样的方式进行比较，如果Candidate的Log Entry比自己的旧，就投拒绝票，只有Candidate和自己一样新或者比自己更新才投同意票。&lt;/p&gt;

&lt;p&gt;基于这个约束和AppendEntries RPC中的Quorum原则，如果2f+1个中有f个机器未收到最新的Log Entry，那么在选举中，他们最多只能拿到f票，从而无法成为Leader。我们可以得出结论，只有拥有最新的Log Entry的Candidate才能成为Leader。&lt;/p&gt;

&lt;h4 id=&quot;committing-entries-from-previous-terms&quot;&gt;Committing Entries From Previous Terms&lt;/h4&gt;

&lt;p&gt;假设有如图情况&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/overwrite.png&quot; alt=&quot;overwrite&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中，黑框代表Leader，Log Entry中的数字代表Term。由图可知，在Term（2）中，S1是Leader，它向S1和S2写入了Log Entry但是由于某些原因宕机了。此时，S5得到S3，S4和自己的投票成为Term（3）的Leader。但是，仅仅把Log Entry追加到本机上后，在向其他服务器发送Log Entry之前就崩溃了，注意，此时该Log Entry未被Commited。此时，S1又成为了Term（4）的Leader，在它把一个Log Entry复制给S3后，由于某些原因又崩溃了。按照上述的选举原则，S5仍然可以拿到S2，S3，S4的投票，当他成为Term（5）的Leader后，它还未收到任何Client的请求，于是它按照AppendEntries的的逻辑使用强制替换保证了一致性。但是，值得注意的是，它使用Term（3）的Log Entry覆盖了Term（4）的Log Entry。这种情况是不被允许的，旧的数据是不能覆盖新的数据的。于是，Raft提出了约束：只有和当前Term相同的Log Entry才能被Commited。&lt;/p&gt;

&lt;p&gt;通过这个约束，上述的蓝色Log Entry就不会在重启后重新发送给其他机器，而客户端也永远不会收到它想要的答复。此时，如果Client需要此命令被执行，那么它需要重新发送请求。&lt;/p&gt;

&lt;h3 id=&quot;membership-change&quot;&gt;Membership Change&lt;/h3&gt;

&lt;p&gt;上面的三个部分其实已经完成了Raft算法的主体介绍，这一部分讨论的是Raft算法如何解决成员变更的问题，成员的组成在Raft中被称为Configuration。成员变更意味着Quorum的总人数和成员的变化，所以无论是对Leader Election还是Log Replication都有很大的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/membership.png&quot; alt=&quot;membership&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示，在指定时刻，S1，S2在旧的Configuration中，S3，S4，S5在新的成员中，但此时整个的成员交接还未完成。所以，对于旧的Configuration来说，S1和S2组成了Quorum，对于新的Configuration来说S3，S4，S5组成了Quorum。那么此时就出现了两个Quorum，可以实现两种决策，这是Raft算法必须要避免的。&lt;/p&gt;

&lt;p&gt;Raft算法使用Log Entry实现成员变更。当需要进行成员变更时，Leader将包含新Configuration的Quorum以及旧Configuration的Quorum的Log Entry发送给Follower，等待Quorum的回复以此将其设为Commited。在此过程中，由于有部分成员已经收到此Configuration，所以所有的决策都需要在此条件下进行，而由于Leader包含这个Configuration，所以能确保这一点。由于中间态的Configuration既包含旧的Quorum又包含新的Quorum，所以所有的决策都需要两方的同意，这就保证的在这个阶段的决策能符合新旧Configuration的要求。然后，新的Configuration才被广播给所有Follower。同样地，由于至少有一个成员在新的Configuration下工作，所以一旦被广播给Follower，所有决策都要在新的Configuration下进行。这样，就完成了成员变更。&lt;/p&gt;

&lt;p&gt;总得来说，Raft引入了一个中间态，在这个中间态中，既包含旧Configuration的Quorum，又包含新Configuration的Quorum，以此满足在此期间决策的正确性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/membershipChange.png&quot; alt=&quot;membershipChange&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;snapchat&quot;&gt;Snapchat&lt;/h3&gt;

&lt;p&gt;在Log Replication中我们说，所有Commited的Log Entry都要被记录在磁盘上，但是磁盘的容量是有限的，如果达到上限了怎么办？很明显地，Raft使用Snapshot记录下机器中此时的状态，再将最新执行的的Log Entry以前的Log Entry全部删除就可以了。&lt;/p&gt;

&lt;p&gt;详细地说，Raft算法允许每台机器各自执行Snapshot，而Snapshot必须记录它最后包含的Log Entry的Term和Index作为LastIncludedEntry和LastIncludedIndex来标记自己记录的位置。然后将这个Log Entry及其之前的全部删除就能达到清理空间的效果。&lt;/p&gt;

&lt;p&gt;但是，有一个情况我们必须考虑。假如一台机器网络情况和运行状况都很差，它最新的Log Entry甚至没有其他机器的快照新，这时候使用AppendEntries RPC是不能帮助它的，因为对应的Log Entry已经被删掉了。这里就需要Leader使用新的RPC，InstallSnapshot RPC来帮助它。InstallSnapshot RPC包含Leader的Snapshot的数据，LastIncludedEntry和LastIncludedIndex等信息，当这个Follower使用Snapshot更新自己的状态后，它就需要使用LastIncludedEntry和LastIncludedIndex来更新自己记录的信息，然后Leader可以再使用AppendEntryies RPC去更新它的Log Entry。&lt;/p&gt;

&lt;h2 id=&quot;总结和思考&quot;&gt;总结和思考&lt;/h2&gt;

&lt;p&gt;Raft相对于Basic Paxos做了很大的改进。&lt;/p&gt;

&lt;p&gt;首先，它使用Leader代替了多Proposer，跳过了Prepare阶段，减少了网络请求的消耗。&lt;/p&gt;

&lt;p&gt;其次，它显式地明确了使用WSL来作为Propsal的载体，不仅降低了开发难度，还提供了相当的容错能力。&lt;/p&gt;

&lt;p&gt;最后，它对容错的场景进行了深入地考虑并且给出了相应的解决方案。&lt;/p&gt;

&lt;p&gt;最重要的，Raft在保证共识性的同时明确地实现了数据的一致性，这对于算法落地来说实在难能可贵。&lt;/p&gt;

&lt;p&gt;但是，Raft使用的成员变更看起来并不是一个很好的方案。我认为在实践中使用Zookeeper作为成员变更的中间工具可能更加合适易行。&lt;/p&gt;

&lt;p&gt;总得来说，Raft算法实至名归，对得起Practical和Understandable的主旨。在GitHub上，有相当部分的项目都使用Raft算法作为他们的核心算法。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">Raft算法是由Diego Ongaro和John Ousterhout于2014年提出的共识性算法。在斯坦福当助教时，他发现学生很难理解Paxos算法，所以他希望能用一种更简单易懂的算法来代替Paxos，以此为契机，他把便于理解作为目的，提出了Raft算法。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">浅谈Paxos</title>
      <link href="http://localhost:4000/%E6%B5%85%E8%B0%88Paxos" rel="alternate" type="text/html" title="浅谈Paxos" />
      <published>2020-07-13T04:00:00+08:00</published>
      <updated>2020-07-13T04:00:00+08:00</updated>
      <id>http://localhost:4000/%E6%B5%85%E8%B0%88Paxos</id>
      <content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Paxos">&lt;p&gt;图为真·Paxos———位于希腊的岛屿&lt;/p&gt;

&lt;p&gt;如果想要在后端开发上更进一步而不是局限于SSM框架，那么分布式是一个不那么坏的发展方向。如果要研究分布式系统，Paxos算法是绝对绕不过，也不能绕过的知识点。&lt;/p&gt;

&lt;p&gt;Leslie Lamport在1998年在《The Part-Time Parliament》中提出了Paxos算法，单单从论文名字看也知道这是篇不怎么正经的论文，这一结论在看完论文后又得到了印证。作者在论文中虚构了一个叫做Paxos的希腊城邦，这个城邦以议会作为最高权利机构，每条法令都需要在此议会中通过后方可实施，作者又对其中的细节作了一些描述和规定，以此符合分布式系统的实际模型。但是，这篇寓言性质的论文对于母语中文的我来说实在过于难懂，哪怕参考了许多资料和解释以后，我仍然很难理解这种模型，就算是读了《Paxos Made Simple》，也很难将其与这个故事一一对应起来。所以，我决定跳过这篇论文，从《Paxos Made Simple》开始说起。&lt;/p&gt;

&lt;h2 id=&quot;为什么需要paxos&quot;&gt;为什么需要Paxos&lt;/h2&gt;

&lt;p&gt;在开始谈Paxos细节之前，我想有必要谈谈为什么我们需要Paxos。几乎所有的资料都在说Paxos是一种共识性（Consensus）算法那么什么是共识呢，共识性和一致性（Consistency）有什么差别，为什么我们需要共识呢？我希望通过下面的例子解决这个问题。&lt;/p&gt;

&lt;p&gt;假设我们开了家“肥宅”奶茶店，为了简化模型，店里只卖珍珠奶茶，我们的配方为奶茶之比为1:1。如果只有一家店，自然是我们说了算。但是有人看我们开得不错，提出要入伙一起干，也不管我们同不同意，总之我们现在有两家店了。作为商业机密，配方是不能透露的，这家店老板也没多想，配方就定了奶茶比为2:1。那么此时，分歧就出现了，也就是说，两家店没有在奶茶的配方上达成“共识”。在这种情况下，由于没有“共识”，消费者在两家“肥宅”奶茶店买到的奶茶竟然味道不同，这就产生了“一致性”的问题。&lt;/p&gt;

&lt;p&gt;将上述的奶茶店换成计算机，配方换成提议，奶茶换成数据，就变成了分布式系统的模型。在奶茶店模型中，消费者喝到味道不同的奶茶倒是小事，但是如果在银行系统中，一台机器上余额是一百万，一台是负一百万那问题就大了。而且，由于现在绝大多数业务都需要保证数据一致性，那么保证提议的共识性就显得格外重要了。&lt;/p&gt;

&lt;h2 id=&quot;basic-paxos&quot;&gt;Basic Paxos&lt;/h2&gt;

&lt;p&gt;鉴于有很多朋友也像我一样无法理解《The Part-Time Parliament》，作者在2001年又发了《Paxos Made Simple》重新解释Paxos算法。作者在这篇文章中，终于用能看得懂的英语解释了Paxos算法是怎么运作的。&lt;/p&gt;

&lt;p&gt;通读全文，我们可以知道Paxos有两个目标安全性（Safety）和活跃性（Liveness）。其期望分别如下（此处采用Raft作者的理解）&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;安全性&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;每次提议仅有一个值被选定&lt;/li&gt;
    &lt;li&gt;服务器在值被接受前不会知道该值已被选定&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;活跃性&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;在一些提议中，最后必定有值会被选定&lt;/li&gt;
    &lt;li&gt;如果值被选定了，那么服务器最后总会知道该值&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;为了实现以上目标Paxos提出了两条约束，并通过数学证明：任何遵循这两条约束的系统都能保证共识性，此处我们仅讨论在文中作者如何推导出两条约束，不讨论数学上如何证明。该算法中，提议（Proposal）具有两个属性，编号（Number）和值（Value），算法中的规定的角色如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;提议者（Proposer）:负责提出提议&lt;/p&gt;

  &lt;p&gt;接受者（Acceptor）:负责审阅提议并决定是否批准&lt;/p&gt;

  &lt;p&gt;学习者（Learner）  :不参与议案过程仅学习通过的提案&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;考虑最简单的情况，我们仅使用一台机器作为Acceptor且接受Number相同的第一个Proposal，那么所有的提议都会由它审阅，并且只会有一种结果，能够保证共识。但是一旦这台机器宕机，那么整个系统就无法继续运行。所以使用多个Acceptor是必要的。此时，仅当Proposal被大于一半的Acceptor接受，该Proposal才被视为通过。虽然在文中此条件没有被显式地列为约束，但我认为其重要程度与后两者相当，因此我将其列为P0&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P0：当且仅当Proposal被大多数Acceptor接受（Accepted），该Proposal才被视为选定（Chosen）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;为了能保证Paxos 在一个Proposer和一个Acceptor的情况下工作，即符合活跃性的第一条要求，我们提出下面的方案作为约束：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P1：Acceptor必须接受其收到的第一个Proposal&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然而此约束存在一个问题，假如几个Proposal同时提出，且被分别发送发不同的Acceptor，每个Acceptor都接受一个Proposal，那么就会出现人均一票的情况，无法形成符合P0的情况，Paxos并未提出如何解决这一问题，而Raft使用随机等待的办法解决此问题。&lt;/p&gt;

&lt;p&gt;由P0的约束可知，需要大多数Acceptor接受提案，而P1则要求Acceptor接受其收到的第一个Proposal，那么这就要求每个Acceptor需要接受多个Proposal。这里，我们使用Proposal的Number作为区分。而为了保证被选中的Proposal具有相同的Value，我们提出以下约束：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P2：如果Number为n，Value为v的Proposal被选中，那么所有被选中且Number &amp;gt; n的Proposal都具有Value v&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;由于Proposal被选中意味着Proposal被大多数Acceptor接受，所以可以进一步约束为&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P2a：如果Number为n，Value为v的Proposal被选中，那么所有被接受且Number &amp;gt; n的Proposal都具有Value v&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;现在，我们假设一个新的 Proposer 刚刚从崩溃中恢复或加入此系统,并且发送了一个带有不同 Value 且Number更高的Proposal。P1要求Acceptor接受这个 proposal，但是却违背了 P2a。为了处理这种情况，需要继续加强P2a，于是我们得到&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P2b：如果一个Value为v的Proposal 被选中，那么之后每个 Proposer 提出的具有更大Number的Proposal都有Value v&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;P2b通过强制要求新的Proposal的值中含有Value解决了上述的问题。而作者通过数学工具，证明以下约束能够满足P2b&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P2c：对于任意的Value v和Number n，如果Value为v且Number为n的Proposal被提出，那么一定有多数Acceptor集合S满足：（a）S中不存在Acceptor已经接受任何Number小于n的Proposal，或者（b）S中的Acceptor所接受的Proposal中Number最高的具备Value v。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;文中根据P2c将整个流程划分为两段：准备（Prepare）和接受（Accept），但是主要是针对Proposer的，为了适应这个流程，进一步将P1约束如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P1a：如果Acceptor没有在Prepare阶段回复过Number大于n的请求，那么在Accept阶段，它可以接受Number为n的Proposal&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在得到上述的两个约束后，我们就能够以此为依据，得到具体的算法流程。&lt;/p&gt;

&lt;h3 id=&quot;2-phase-commit&quot;&gt;2 Phase Commit&lt;/h3&gt;

&lt;p&gt;上文提到整个算法流程分为两个阶段，下面我们将介绍具体流程是怎么样的。&lt;/p&gt;

&lt;p&gt;阶段一为准备阶段，具体如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Proposer 选择一个Number n，然后向大多数Acceptor发送Number 为n的Prepare request。&lt;/li&gt;
    &lt;li&gt;如果一个Acceptor接收到Number为n的Prepare request，并且n大于任何它已经回复的Prepare request的Number，那么它将承诺不再接受任何Number 小于 n的proposal，并且回复已经接受的最大Number的 proposal。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;阶段二为接受阶段，具体如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;如果Proposer 接受了来自大多数Acceptor对它的Prepare request 的回 复，那么接下来它将给这些 Acceptor发送Number为n，Value为v的 Proposal作为Accept request。其中v是收到的回复中最大 Number 的Proposal的Value，或者如果回复中没有Proposal的话，就可以是它自己选的任意值。&lt;/li&gt;
    &lt;li&gt;如果 Acceptor 收到一个Number 为n的Accept request，如果它没有对Number 大于n的Prepare request进行过回复，那么就接受该Accept request。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Raft作者的这张图形象地展示了Paxos的整个流程&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/paxos.png&quot; alt=&quot;Paxos&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;对paxos的一些思考&quot;&gt;对Paxos的一些思考&lt;/h2&gt;

&lt;p&gt;上述内容本质是对《Paxos Made Simple》的翻译和复述，下面我想谈谈我对这个算法一些思考。&lt;/p&gt;

&lt;p&gt;我仍然用上文奶茶店的例子，共识指的是两者对同一奶茶的配方认可，一致是奶茶的味道相同。初始奶茶店希望分店和自己采用相同的配方，这个过程就是寻求共识，如果分店同意，那么他们就达成了共识，这一阶段对应于提出Proposal。得到配方后的分店按配方制作奶茶，就能得到和初始奶茶店一样的味道，就相当于保证了数据的一致性。如果采用复制状态机的方案，奶茶配方在计算机系统中就是指令，两台初始完全相同的机器，在以同样顺序执行相同的指令后，就会得到一致的数据。也就是说，只要保证各机器对执行指令和顺序的共识，那么我们就能保证数据的一致性。&lt;/p&gt;

&lt;h3 id=&quot;为什么paxos需要p0&quot;&gt;为什么Paxos需要P0？&lt;/h3&gt;

&lt;p&gt;这个约束有两个作用，一是在拥有2f+1台机器的系统中，它能够允许f台机器同时崩溃，而仍能正常处理请求。其次，我们来考虑有2f+1个机器，而我们仅需要f个机器接受请求，那么假如所有的请求和数据都由前f个机器处理，而后f+1个机器没有任何数据。此时，前f个机器崩溃了，那么，我们的系统中就没有任何的数据了。但是，假如是要求f+1个机器接受请求，那么任何两次请求都必然有至少一个机器接受了两次请求，也就是说，它存有全部的最新数据。使用数学归纳法可以得知，无论接受了多少次请求，f+1个机器中所有的存储的数据可以拼接成完整的全部数据，那么，即使f个机器崩溃了，也能保证数据的完整。&lt;/p&gt;

&lt;p&gt;总得来说，Quorum机制能保证在2f+1台机器中f台机器崩溃的情况下保证Paxos协议的正常运行和数据的完整性。&lt;/p&gt;

&lt;h3 id=&quot;为什么basic-paxos只能处理single-decree&quot;&gt;为什么Basic Paxos只能处理“Single Decree”？&lt;/h3&gt;

&lt;p&gt;我们可以看到，如果Proposal希望能被接受，那么他必须包含之前所有被接受过的Proposal的Value，这在实践中是不可能实现的。另外，由于Basic Paxos允许多个Proposer，那么每个Proposal的Number大概率是不一致的，在跨事件的情况下，不能根据自增的Number来判断是否多个Proposal是对一个事件的共识。最重要的是，恐怕作者这篇论文的目的也仅仅是为了解决单个事件的共识。&lt;/p&gt;

&lt;p&gt;以前看这篇的论文的时候，由于之前了解的都是有Leader的系统，Proposal的不同Number的代表对不同Value的共识，然后在读Paxos时也代入了这种想法，就不能很好理解Paxos算法，现在从这个角度来看就容易理解得多。同时也可以解释为什么P2a和P2b中要求大Number应包含小Number有的Value，因为此时并不是两个事件，而是对同一事件的修改。比如两次的Value分别是{x=1}和{x=1，y=2}，那么实际上后者只是对前者的补充而已。&lt;/p&gt;

&lt;h3 id=&quot;为什么需要prepare阶段&quot;&gt;为什么需要Prepare阶段?&lt;/h3&gt;

&lt;p&gt;Prepare阶段主要是为了实现Proposer的共识，这也是和有Leader系统非常不同的一点。&lt;/p&gt;

&lt;p&gt;假设三个Proposer有三个版本的Proposal，具体如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Proposer1：{Number：1，Value：[X=1] }&lt;/p&gt;

  &lt;p&gt;Proposer2：{Number：2，Value：[X=1，y=2] }&lt;/p&gt;

  &lt;p&gt;Proposer3：{Number：3，Value：[X=1]，y=2，z=3] }&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果Proposer3最先到达Acceptor，那么根据Prepare阶段的要求，由于Proposer1和Proposer2的Number小于Proposer3，他们会被一直拒绝直到其内容和Proposer3相同。如果Proposer3最迟到达Acceptor，那么Proposer1和Proposer会在Acceptor阶段被拒绝。这里我们假设了Proposer3的Proposal是被选定的情况，如果它不是最终版本，那么，很有可能会出现Value中继续添加值的情况，然后Proposer3也会在第Accept阶段被拒绝，要求它重新提案。&lt;/p&gt;

&lt;p&gt;总之，由于Basic Paxos允许多个Proposer存在，所以需要Prepare阶段保证提案的一致性。&lt;/p&gt;

&lt;h3 id=&quot;为什么我们不使用basic-paxos&quot;&gt;为什么我们不使用Basic Paxos？&lt;/h3&gt;

&lt;p&gt;由于Basic Paxos需要Prepare阶段保证提案的一致性，而且一次算法的运行只能允许完成单次操作，所以如果直接使用Basic Paxos在性能上很可能会达不到我们的要求。因此，学界提出了更加符合实际的Multi-Paxos，使用Basic Paxos选举Leader，让Leader直接提案代替Prepare阶段，从而大大缩短了一次算法运行需要的时间。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">如果想要在后端开发上更进一步而不是局限于SSM框架，那么分布式是一个不那么坏的发展方向。如果要研究分布式系统，Paxos算法是绝对绕不过，也不能绕过的知识点。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">我的2019</title>
      <link href="http://localhost:4000/%E6%88%91%E7%9A%842019" rel="alternate" type="text/html" title="我的2019" />
      <published>2020-02-04T18:18:00+08:00</published>
      <updated>2020-02-04T18:18:00+08:00</updated>
      <id>http://localhost:4000/%E6%88%91%E7%9A%842019</id>
      <content type="html" xml:base="http://localhost:4000/%E6%88%91%E7%9A%842019">&lt;p&gt;​2019悄然而过，过去的一年，大大小小事情许多，而站在2020回望，似乎除了考研，与我有关的，实在不多。适逢新型冠状病毒肆虐，诸事皆歇，得空搭起博客，也算整理千万思绪，展望一下迷茫的未来。&lt;/p&gt;

&lt;h2 id=&quot;考研&quot;&gt;考研&lt;/h2&gt;

&lt;p&gt;​考研的复习始于面试受挫，这么说恐怕不对，因为真正的面试甚至都还没开始。看到牛客网上的面经，拿到大厂offer的兄弟几乎都是985或是211的硕士。面试的主要知识点说不上多么深入，无非是老生常谈的那几本书，那几个知识点，但是，要谈项目经验的话，我实在是没有能拿得出手的的东西，倘若硬要尝试大厂，恐怕只会撞得头破血流。再以本科杭电的身份自处，两相比较，最终还是选择的考研。现在回头想想，哪里会有公司要求一个大三的学生有光鲜亮丽的实际项目？然而话又说回来，现在无论是BAT还是公职单位，没有硕士的文凭，上升空间实在是小之又小，几乎埋下了35岁被清理的引子。当然强的不像话的我们另说，比说A校CM队的同学，有拿到阿里，微软实习的，有保研浙大的，实在不是能够与之同台竞技。&lt;/p&gt;

&lt;p&gt;​复习过程乏善可陈，大约与996无异。早上八点半起床，九点到图书馆，复习两个半小时，中午十一点半去吃饭，小睡一会，下午一点半起床去图书馆，复习到五点钟，吃完晚饭，去寝室休息到六点左右出门，到图书馆复习到九点或九点半，然后结束一天复习，回寝室准备睡觉。平心而论，我的复习时间不算长，但效率要比绝大多数人强上不少，几乎不停笔，算是复习卓有成效，可惜天不如人愿。&lt;/p&gt;

&lt;p&gt;​值得一提的，是九月中旬考研报名。在浙大和中科大犹豫不决。本来是首选浙大计算机，但是，实在是太难了。清北的计算机自主命题，严格控制分数线，所以公认最难，而浙大则是公版的数一英一408，但是浙大的分数线之高令人发指，我个人觉得难度几乎与清北齐平。中科大软院招人多，分数低，但是没导师，认可度没浙大高，虽然难度比浙大低一档，但是考上后的加成也要低一档。马克思说要结合实际，具体问题具体分析，还是得看自身情况呀。通过以下四点考虑，我最终选择了中科大的软院：1.上浙大可以说只有一层把握，考不上研究生过去一年的努力就等于白费。2.我准备的方向是后端，做中间件或者分布式，这些方向的学习资源非常丰富，完全可以自学成才。3.没导师确实少了很多帮助，但另一方面，没了导师可以说未来所有时间都是自己的。想做什么方向，想怎么安排生活自主权都在自己手里，或许更适合我。4.科软再菜也是C9呀，虽然不比浙大的牌子响亮，但是比杭电总归要强得多。&lt;/p&gt;

&lt;p&gt;​然而，一切都终结于2019年12月22号上午。数学，我最有自信的一科，炸了，血炸。考到一半的时候，我慌了，犯了大忌，尽力想冷静下来却办不到，那一刻，我的考研就结束了。无论我其他三门怎么发挥，恐怕也难以挽回这科的劣势，甚至于单科线都很难过。刚刚查了文件，大概还有十天出分，希望渺茫，虽然一路走来早已做好了最坏的打算，但八个月的努力白费总归是意难平。&lt;/p&gt;

&lt;p&gt;​只能是尽人事，听天命。&lt;/p&gt;

&lt;h2 id=&quot;创新实践&quot;&gt;创新实践&lt;/h2&gt;

&lt;p&gt;​这门课实在是麻烦得要命。但总归能学到一些东西。&lt;/p&gt;

&lt;p&gt;​大学所谓的teamwork其实是solo-teamwork，大学四年，除了一门课的teamwork划水划过去，其他的我几乎都干了70%以上的事。有趣的是，我那一次的划水竟然被大佬喷了，虽然能体会那位大佬的干了90%的活的心情，可是当时我等了一个半个小时也等不到回复信息，以为没有问题就睡觉了，结果被喷就难受了。言归正传，这次teamwork我干的活大概也在90%左右，10次ppt，8篇paper的资料，实验重现几乎一力完成，但是付出总是有回报的。大学里，同学间，师生间大多留点情面，但是毕业了，遇到这种事却不能马马虎虎，多带一个名字就过去了。这就要说到展现自我的能力了。&lt;/p&gt;

&lt;p&gt;​以前总认为闷声发大财，是金子总会发光的。但现在看来，发大财的时候肯定是不能到处讲的，免得遭人嫉妒，而后者则是所谓怀才不遇的人的自我安慰。到了大学，终于明白信息不对称随处都在，尤其是到了求职的时候。你说你厉害，我怎么信你？总是要拿出一些东西来证明的。挑战杯之类的比赛水吗？水，真的水。但是在面试官眼中，你没有ACM的奖项就算了，但这么水的挑战杯也没有，那岂不是更水吗？简历里什么都没，怎么让人相信你水平高呢？有能力，要展现出来，多参加比赛，拿个小奖，奖不是奖励，是证明，证明你的能力高于同龄人。同理，以后团队工作，要让上级看到自己的工作量，不然别人就要以为你是混日子的。&lt;/p&gt;

&lt;p&gt;​这门课充分锻炼了我的看论文的能力。以前看到全英文，很容易怀疑人生，感觉看完几乎是不可能的任务。但是这么多论文看下来，终于养成了看论文的能力，现在总算能拿到论文不慌，知道重点在哪些段落，哪些细节需要注意，把论文从头到尾看完，还能再做一个报告。这个能力我以后应该是要受用无穷的。但是，前几天找的分布式存储的论文还一个字没看呢。&lt;/p&gt;

&lt;p&gt;​这门课暴露的我的一个不足，就是表达能力不行。做汇报的时候，往往会觉得力不从心，懂得说不好，表达不出来，不能很自如地边讲话边思考。以前看演讲，觉得这个不行那个不行，没想到最不行的是我自己。追根溯源，是两点，1.本身对报告的内容不够熟悉，细节掌握的不够充分，没有一个清晰的思路。2.演讲能力的不足，这个不仅仅是需要锻炼，最好有稿子，最不济要有一个大纲，然后要提前准备，反复练习。&lt;/p&gt;

&lt;p&gt;​千言万语，这门课终于结束了，天知道有多push。&lt;/p&gt;

&lt;h2 id=&quot;gsoc&quot;&gt;GSoC&lt;/h2&gt;

&lt;p&gt;​申请两次，被拒两次。那感觉就像是精心准备的表白，被女神无情拒绝。&lt;/p&gt;

&lt;p&gt;​第一次申请时的我实在是too young too naive。我单以为只有我一个人申请就只能给我，没想到还能谁都不给。这次申请的是小组织的小项目，更新下API而已，哪里有什么难度。可惜当时无知，proposal写得跟官话一样，满口空话，实际项目相关压根没写，怕是换了我自己也是要拒绝的。&lt;/p&gt;

&lt;p&gt;​第二次申请的真是大项目呀，阿里的RPC框架Dubbo，要是能给它贡献代码，BAT躺着也能进去了，可惜没拿到。一开始的项目是加一个Consul实现的register，虽然有难度，可是说不上完成不了，花点时间怎么也能写出来。整了大半个月，proposal都快写完了，结果说社区里有人实现了，当时真的傻了。硬着头皮换了项目，用gRPC做RPC的第三方协议，看着很合理，写起来完全的另一回事。gRPC的API和dubbo的思路完全不同，最后还得回到Netty上，我真没研究过Netty。也知道多半是申不上的，但还是磕磕绊绊交了proposal。当然，没过。&lt;/p&gt;

&lt;p&gt;​话虽如此，但还是能学到不少东西的。我学会用Mailing List了，看得懂Apache的issue管理了，能写英文邮件了（直接导致我英文邮件写得比中文顺）。重要的是两点，1.不怕所谓的开源社区和开源项目。其本质就是一帮在世界各地的人，给同一个项目写代码，想参加进来的前提是把与自己相关的代码看懂。2.敢去看大型项目的源码，尝试去理解其用意。之前从没有真正地看过这些项目的代码，但这两次是实实在在地去阅读代码，将其个部分联系起来，并在本地调试。这是程序员进阶的必要技能，这次算是点了一级，来日方长，慢慢升级吧。&lt;/p&gt;

&lt;p&gt;​这三件事算得上我去年最重要的事，都不算圆满，但多少能从中学到一些经验。&lt;/p&gt;

&lt;p&gt;​但愿今年，健康平安，劳有所得。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="essay" />
      

      
        <summary type="html">2019悄然而过，过去的一年，大大小小事情许多，而站在2020回望，似乎除了考研，与我有关的，实在不多。适逢武汉病毒肆虐，诸事皆歇，得空搭起博客，也算整理千万思绪，展望一下迷茫的未来。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">写在研究生生活开始之前</title>
      <link href="http://localhost:4000/%E5%86%99%E5%9C%A8%E7%A0%94%E7%A9%B6%E7%94%9F%E7%94%9F%E6%B4%BB%E5%BC%80%E5%A7%8B%E4%B9%8B%E5%89%8D" rel="alternate" type="text/html" title="写在研究生生活开始之前" />
      <published>2020-02-04T18:18:00+08:00</published>
      <updated>2020-02-04T18:18:00+08:00</updated>
      <id>http://localhost:4000/%E5%86%99%E5%9C%A8%E7%A0%94%E7%A9%B6%E7%94%9F%E7%94%9F%E6%B4%BB%E5%BC%80%E5%A7%8B%E4%B9%8B%E5%89%8D</id>
      <content type="html" xml:base="http://localhost:4000/%E5%86%99%E5%9C%A8%E7%A0%94%E7%A9%B6%E7%94%9F%E7%94%9F%E6%B4%BB%E5%BC%80%E5%A7%8B%E4%B9%8B%E5%89%8D">&lt;p&gt;本来是准备把GitPage作为技术博客来使用的，所以第二篇本该是技术相关的文章。但是想想Spring、MyBatis之类的Web框架也没什么东西好写，Paxos和Raft还没吃透，写不出好东西。又正值大学毕业和研究生生活开始之前的暑假，不如就来写写对未来的一些展望和规划。&lt;/p&gt;

&lt;p&gt;之前看到过一篇文章，里面说要想一想自己希望在简历上写哪些东西，然后做出一份期望的简历，再根据这份简历上的知识点去学习相关的东西。大三找实习的时候对这种说法很是不以为意，但是现在却觉得很是受用。可能主要是因为大三时眼界很局限，只有传统的SSM框架的知识，分布式，云计算和一些业界有名的解决方案都不是很了解，简历上可写的东西屈指可数，现在恰恰相反，前一段时间开始学MIT6.824的东西，又开始在Github上了解一些新潮的项目，逐渐地将眼界拓宽，明白需要学的东西还很多。我将这份期望简历分为两部分，一是个人的项目经历，二是掌握的技能，其中个人经历准备写四个部分，给开源项目做贡献，GSoC的项目，第一段实习和第二段实习，掌握的技能则包括Java Web和分布式存储两块来写。&lt;/p&gt;

&lt;p&gt;2020年的整个七月份，我准备按计划完成6.824的全部课程，包括阅读相关论文并完成所有的实验，还要看完SICP的课程视频以及阅读《Effective Java》，充实地读过这个月。八月份我决定用来阅读Spring，MyBatis和Dubbo的源码，并着手完成第一个项目经历，即提交具有相当质量的Pull Request给一个或几个开源项目。从目前看起来，用Java实现的开源项目并不多，大部分是用于后端的解决方案，给这些项目做贡献有一定的难度，然而我认为与其给与方向无关的项目提交代码，在简历给自己挖坑，不如花多一些时间给有一定体量的项目作贡献，到面试时吹起来也方便一些。&lt;/p&gt;

&lt;p&gt;科软的研究生学制是2.5年，第一年主要是上课，第二年则是一整年的实习，最后半年准备毕业论文。虽然看起来有一年时间准备实习面试，但第二年的实习实际上以暑期实习作为开始，而暑期实习的面试在春招进行，也就是当年的三四月份。这就意味着，研一上的寒假结束就要参加春招，哪怕从现在开始计算也就只有八个月的时间了。由于上述的暑假安排，我准备将实习面试的准备推到开学以后展开，其中主要有两项工作，一是刷LeetCode，二是准备相关的基础知识。由于字节跳动带节奏，所有大厂都喜欢在面试里加一道算法题，今年面腾讯的时候来了四道算法题，华为前两面都有算法题，又听说阿里要在面试里算法题，而如果想要去微软谷歌，也只有做题一条路。不过，如果算法题做得出彩，面试确实能加分不少。所以刷一刷LeetCode恐怕是必不可少的工作。而相关的基础知识的积累就需要阅读常用框架的源码，琢磨开发人员为什么要这么写，然后把这些零散的知识整理成体系，这样无论是面试吹水还是记忆知识都要更舒服一些。&lt;/p&gt;

&lt;p&gt;研一的寒假除了上述的两件事外还要准备一下GSoC的申请，前两次都没成功，就再试一次。有了前两次的经验，这次的proposal写起来可能会更快一些，另外，在写完毕业论文后，在行文的布局上可能会有一些帮助，但是关键仍在于能否解决项目的问题，并且把解决方案阐述清楚，这不仅需要技术水平，阅读代码的能力还需要一些英语的书面表达能力。当然，我个人现在对GSoC不再有执念了，能申请上固然很好，如果不行，也就算了。&lt;/p&gt;

&lt;p&gt;对于实习公司的选择，考虑外企和国内企业，实习地点在国外还是国内，其中外企&amp;gt;私企，国外&amp;gt;国内。之前在知乎上看了关于谷歌日本的介绍，很是向往，毕竟日本没有996也没有恐怖的房价，但是出国的难度实在不小，而谷歌难度就更大了，于是放弃了这个念头，转向国内的公司。国内外企仍是好于土著公司的，微软、谷歌虽然给的工资不多，但胜在不996也没有35岁强制辞退的制度，可以很安稳地过日子，如果有需要甚至能走内部通道转到其他国家的分部。而国内公司给的工资相当地多，阿里，腾讯甚至华为都能在三年后给出30w-50w不等的待遇，能拿到这种待遇其实房子的问题已经解决了一半，但是996和35岁仍是悬在程序员头上的达利克摩斯之剑。因为有一整年的实习时间，而其中一段实习经历必须要呆满七个月，所以初步的计划是能去微软之类的外企做一段时间，毕竟微软苏州离科软宿舍相当近，接下来希望能够去国内的头部公司，阿里，字节之类的实习一段时间，充分体验过两种的工作节奏，再做打算。不过，无论是上述的哪一家公司，想要进去都十分不容易。&lt;/p&gt;

&lt;p&gt;以上的计划和经历已经能够很好地写满简历的个人经历部分，这样的简历不说顶级也是一流的。相交之下，个人技能的介绍反而显得无关紧要，无非是一些技术的罗列，如果能充分阅读相关工具的源码和论文，讲起东西来也不会有多大问题。&lt;/p&gt;

&lt;p&gt;在上一篇文章中，我对能否考上研究生这个问题持否定的态度，其实是由于初试分不怎么高，但谁又能想到我复试竟然排第六呢？！上一篇文章吐槽了创新实践这门课，但恰恰是这门课看过的论文让我有机会在复试中吹出学术感来，加上四级600分，意外地拿了高分。以前觉得985不过更加厉害一些，但是真正得到这个名头之后才知道别人常说的“更大的平台”是什么意思。985+硕士意味着考公的门槛基本不存在，意味着移民条款中瞬间符合许多要求，意味着简历不可能再被卡住等等，这个平台确实相当地大。&lt;/p&gt;

&lt;p&gt;《新华字典》里有句话：“张华考上了北京大学，在化学系学习；李萍进了中等技术学校，读机械制造专业；我在百货公司当售货员：我们都有光明的前途。”&lt;/p&gt;

&lt;p&gt;希望我也能有光明的前途。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="essay" />
      

      
        <summary type="html">本来是准备把GitPage作为技术博客来使用的，所以第二篇本该是技术相关的文章。但是想想Spring、MyBatis之类的Web框架也没什么东西好写，Paxos和Raft还没吃透，写不出好东西。又正值大学毕业和研究生生活开始之前的暑假，不如就来写写对未来的一些展望和规划。</summary>
      

      
      
    </entry>
  
</feed>
