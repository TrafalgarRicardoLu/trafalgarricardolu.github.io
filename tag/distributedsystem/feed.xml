<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="http://localhost:4000/tag/distributedsystem/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2020-07-26T21:45:37+08:00</updated>
  <id>http://localhost:4000/tag/distributedsystem/feed.xml</id>

  
  
  

  
    <title type="html">Ghost | </title>
  

  
    <subtitle>The professional publishing platform</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">浅谈Primary-Back Replication和Chain Replication</title>
      <link href="http://localhost:4000/%E6%B5%85%E8%B0%88Primary-Back-Replication%E5%92%8CChain-Replication" rel="alternate" type="text/html" title="浅谈Primary-Back Replication和Chain Replication" />
      <published>2020-07-26T18:00:00+08:00</published>
      <updated>2020-07-26T18:00:00+08:00</updated>
      <id>http://localhost:4000/%E6%B5%85%E8%B0%88Primary-Back%20Replication%E5%92%8CChain%20Replication</id>
      <content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Primary-Back-Replication%E5%92%8CChain-Replication">&lt;p&gt;上图为Google提供的贪吃蛇游戏&lt;/p&gt;

&lt;p&gt;前面两篇文章，我们讨论了分布式系统中为了维护一致性所使用的共识性算法Paxos和Raft。这些算法保证了各机器之间数据的一致性，但是Paxos算法针对的是平等的Replication策略，而Raft算法针对的是Primary-Back Replication策略。Replication策略和共识性算法的目的不同，它的设计是为了实现容错（Fault-Tolerance），即在一部分机器不可用后，仍能保证正常提供服务。下面，我将简要描述一下6.824中讨论的两种复制模型Primary-Back Replication和Chain Replication。&lt;/p&gt;

&lt;h2 id=&quot;机器同步&quot;&gt;机器同步&lt;/h2&gt;

&lt;p&gt;为了能够实现集群能够及时接管服务，并提供相同的服务，集群需要保证不同机器之间的数据一致性。我们上面提到，要实现共识需要使用Paxos或者Raft算法，而我们需要对什么东西达成共识呢？这就要提到实现机器同步的两种方法，分别是State Transfer和Replicated state machine，其中差别如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;State Transfer：主机器将本身的状态变化全部传送给其他机器。&lt;/p&gt;

  &lt;p&gt;Replicated State Machine：由于业务的很多情况符合状态机的定义，所以只要保证所有机器初试状态相同，那么以相同的顺序执行相同指令后，它们的状态也是相同的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;举个例子，客户端发送{X=1，Y=X}的指令给集群。在State Transfer中，主机器执行这个操作后，将它发生变化的数据和值发送给其他机器，也就是{X=1，Y=1}，然后其他机器将自身的数据修改为对应的值，以此实现各机器的数据一致性。而在Replication State Machine中，主机器发送{X=1，Y=X}的指令给其他机器，要求其他机器也执行这一操作，按照状态机的状态变化，也能实现数据一致性。&lt;/p&gt;

&lt;p&gt;State Transfer的优点是不会消耗其他机器的计算能力，但是在实际情况下，发生变化的数据量比较大，它对集群的带宽和延迟要求都很搞。Replicated State Machine由于只发送指令，所以传输的数据量比较小，但是它要求各机器在本地执行指令，所以对机器的计算能力有一定要求。根据实际情况，一般集群中各机器的性能相当，所以计算能力不会成为瓶颈，而为了保证快速响应客户要求，必须尽量减少传输的延迟，State Transfer由于一次性传输的数据量大，所以在网络的传输延迟上很难实现要求。因此Replicated State Machine成为实际上的解决方案。&lt;/p&gt;

&lt;h2 id=&quot;vmft&quot;&gt;VM—FT&lt;/h2&gt;

&lt;p&gt;VMware在&lt;a href=&quot;http://nil.csail.mit.edu/6.824/2017/papers/vm-ft.pdf&quot;&gt;《The Design of a Practical System for Fault-Tolerant Virtual Machines》&lt;/a&gt;中介绍了基于虚拟机实现的Primary-Backup Replication的容错解决方案。其本质就是使用Replicated State Machine实现主从的数据一致性。&lt;/p&gt;

&lt;h3 id=&quot;deterministic-replay&quot;&gt;Deterministic Replay&lt;/h3&gt;

&lt;p&gt;在VM-FT中，它们将指令在副机器的执行称为Deterministic Replay。由于和数据库软件不同，有一些系统命令和时间息息相关，比如中断和时间戳，考虑到传输延迟，这就会使命令执行的结果不同，从而导致数据的不一致。所以这就给系统设计带来一些问题，论文将此分为三个目标&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;正确收集所有的非确定（Non-Deterministic）事件和操作，保证有在执行时有足够的信息在Replay时来修正操作&lt;/li&gt;
    &lt;li&gt;正确地在副机器上Replay非确定的事件和操作&lt;/li&gt;
    &lt;li&gt;保证以上两点不会降低机器性能&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;由于这个系统运行在VMware上，所以VMware可以收集所有它想要的信息并将其存储在文件中，能够正确地实现第一个目标。然后，主机器通过Logging Channel将需要执行的操作和相关的补充信息传递给副机器，因此可以保证第二个目标。在实际的测试中，VMware的实现也能够保证机器的性能，满足了第三点。&lt;/p&gt;

&lt;h3 id=&quot;ft-protocol&quot;&gt;FT Protocol&lt;/h3&gt;

&lt;p&gt;由于主从机器通过Logging Channel进行通信，所以其中会产生一定的延迟而导致问题。假设主机器在收到请求后，在本地执行了该请求并将其传送给副机器，同时将结果反馈给客户端，但是副机器由于一些问题没有成功执行该指令，此时主从机器的数据是不一致的。如果主机器在此时崩溃，集群交由副机器负责，那么客户端再次查询数据就会得到不一样的结果。&lt;/p&gt;

&lt;p&gt;因此，VMware提出了如下的要求：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;输出规则&lt;/strong&gt;：主机器只有在副机器已经接收并返回了该指令的ACK 后，才能回复客户端。其中，副机器必须成功执行了该指令后才能回复ACK。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其实就是要求主机器在副机器也执行完指令后，实现了整个系统的一致性，再回复客户端。其具体流程如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/FT Protocol.png&quot; alt=&quot;FT Protocol&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;错误检测和恢复&quot;&gt;错误检测和恢复&lt;/h3&gt;

&lt;p&gt;为了检测错误，VM-FT使用了以下两种手段&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;使用定期发送UDP来实现各机器间的Heartbeating&lt;/li&gt;
    &lt;li&gt;由于整个系统基于VMware，所以可以通过检测Logging Channel的流量来查看是否正常工作&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;第一种方法和Raft中使用的类似，都是通过周期性地Heartbeats来检测是否活跃。第二种方法基于系统不断接受客户端请求的事实，只要客户发送了请求，为了实现主从同步，两者必须通过Logging Channel发送指令或者返回ACK，而如果相当长的时间内没有流量的话，就说明至少主机器可能已经崩溃。&lt;/p&gt;

&lt;p&gt;一旦检测到主机器崩溃，那么副机器就会将Logging Channel中所有的指令执行完毕，以此到达主机器崩溃前的状态，然后在这个状态下回复客户端。而如果副机器崩溃了，主机器仅仅需要不再发送同步消息即可。&lt;/p&gt;

&lt;h3 id=&quot;logging-channel优化&quot;&gt;Logging Channel优化&lt;/h3&gt;

&lt;p&gt;为了保证主从的一致性，VM-FT还对Logging Channel进行了优化。如果副机器执行很慢，那么Logging Channel中的留存的信息过多，甚至填满了预留的空间，那么收到客户端请求时，主机器必须等待有空余空间才能发出请求，这使得客户端的体验很差。另一方面，如果主机器崩溃，副机器为了赶上主机器进度而需要花费的时间过多也会影响系统。所以VM-FT在通信中添加额外的信息，以此检测主从机器在执行同一指令上产生的延迟。如果这个延迟过大，VM-FT会减少分配给主机器的CPU算力，增加副机器的CPU算力，以此保证主从机器间的相对同步。&lt;/p&gt;

&lt;h3 id=&quot;brain-split&quot;&gt;Brain-Split&lt;/h3&gt;

&lt;p&gt;假设两台机器都能正常工作，但是由于通信原因，他们不能通过Heartbeat来确认对方正常工作，那么主机器仍然保持主机器的角色，而副机器则会将自己晋升为主机器，此时系统中有两个主机器，产生了脑裂（Brain-Split）现象。很不幸，这种情况不能通过系统自身解决，因为他们不能正常沟通，而在VM-FT中也没有使用基于Quorum的选举机制，所以只能通过外部解决。&lt;/p&gt;

&lt;p&gt;在VM-FT中，由于主从共享磁盘，所以可以通过磁盘这个中介来解决。可以在磁盘中记录当前是否有主机器，如果有机器想要成为主机器，那么它必须访问磁盘，读取这个字段，确认当前没有主机器，才能成为主机器。在论文中，这个过程被称为test-and-set 。而在分布式的系统中，则必须通过第三方服务器才能实现这一功能。&lt;/p&gt;

&lt;h3 id=&quot;vm-ft总结&quot;&gt;VM-FT总结&lt;/h3&gt;

&lt;p&gt;这篇论文发表于2010年，虽然距离现在有一段时间了，但是仍能看到后续设计的一些影子。比如Logging Channel中设计了Buffer来保存指令，这个设计其实就相当于WAL，它对Brain-Split的解决方案在今天仍然实用。不过，由于整体的实现是基于VMware，所以和实际的分布式系统仍然有一点差别。总而言之，我们对这篇论文最大的学习是对主从设计系统的简单了解。&lt;/p&gt;

&lt;p&gt;遗憾的是，这篇文章没有讨论多个副机器的情况，也就是说，没有使用Paxos算法实现多个副机器的一致性。&lt;/p&gt;

&lt;h2 id=&quot;craq&quot;&gt;CRAQ&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://pdos.csail.mit.edu/6.824/papers/craq.pdf&quot;&gt;《Object Storage on CRAQ High-throughput chain replication for read-mostly workloads》&lt;/a&gt;使用了一种类似于链表的复制模型Chain Replication。和Raft以及上面提到的主从模型不同，这个模型用很简单的方法就保证了数据的一致性。&lt;/p&gt;

&lt;h3 id=&quot;chain-replication&quot;&gt;Chain Replication&lt;/h3&gt;

&lt;p&gt;Chain Replication将每个结点连接成链表。其中写请求只能由头结点处理，读请求只能由尾节点负责。具体流程如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cr.png&quot; alt=&quot;cr&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;写请求：&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;头节点接受写请求，执行完毕后将其转发给后一个节点&lt;/li&gt;
    &lt;li&gt;所有节点都执行相关指令，同时将请求转发到下一个节点&lt;/li&gt;
    &lt;li&gt;等到尾节点也执行完相关指令后，会给上一个节点发送ACK&lt;/li&gt;
    &lt;li&gt;所有节点反向发送ACK，直到头节点&lt;/li&gt;
    &lt;li&gt;头节点收到ACK后，将结果返回给客户端&lt;/li&gt;
  &lt;/ol&gt;

  &lt;p&gt;读请求：&lt;/p&gt;

  &lt;p&gt;​	读请求只能由尾节点处理，尾节点返回当前数据&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们可以看到，只有当所有节点都执行完相关操作后，写请求才能得到结果，能保证写的一致性。由于尾节点是最后更新数据的节点，所以尾节点中的数据就是当前系统最新的一致性数据，读请求总能得到已经达成一致的数据。这个模型很简单地实现了读写的数据一致性，但是问题是，所有的读操作都由尾节点来处理，相当于整个系统的读取压力都来到了尾节点，尾节点很可能崩溃。所以这篇论文提出了一些改进。&lt;/p&gt;

&lt;h3 id=&quot;chain-replication-with-apportioned-queriescraq&quot;&gt;Chain Replication with Apportioned Queries（CRAQ）&lt;/h3&gt;

&lt;p&gt;这篇论文为了减少尾节点的压力，允许所有结点处理读请求，也就是名字中Apportioned Queries的含义。但是，由于读请求和写请求时并发的，有可能出现结点数据不一致时的读请求。所以这里还需要额外的处理。&lt;/p&gt;

&lt;p&gt;首先考虑最简单的情况，即整个系统数据保持一致的情况。这时，无论从哪个节点读取，数据都是正常的。示意图如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/craq clean read.png&quot; alt=&quot;craq&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其次，我们来考虑在处理写请求的同时，处理读请求的情况。其示意图如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/craq dirty read.png&quot; alt=&quot;craq read&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图中，头节点接受了写请求，并将其传送给第二个节点，第二个节点已经处理完毕。但此时，第三个节点和尾节点都还未进行相应的处理，导致前两个节点和后两个节点的数据不一致。CRAQ通过为数据添加版本并存储多个版本的数据来解决这一问题。&lt;/p&gt;

&lt;p&gt;一次完整的写请求完成后，整个系统仍然能保证数据一致性，唯一不能保证的就是上图中写到一半的情况。其中，已经处理写请求的节点和尾节点数据不一致，而未处理写请求的节点则保持一致。所以，CRAQ要求处理完写请求的节点把自己标记为Dirty，而未处理的则无需标记。另一方面，尾节点始终存储能被整个系统承认的数据，即最新的保持了一致性的数据。所以，为了确定当前应该返回的数据，Dirty节点应该询问尾节点应当返回数据的版本。&lt;/p&gt;

&lt;p&gt;这张图中的绿色圆柱表示数据库，我们可以看到，已经处理完写请求的两个节点的数据库中存有两个版本V1和V2的数据K，而后两个节点则只有V1版本的数据K。此时，第二个节点收到读请求，由于其数据库内有两个版本的数据K，而且本身标记为Dirty，所以它需要向尾节点发送询问请求。尾节点回复其应返回版本V1的数据K，此节点就按照尾节点的指示回复客户端V1版本的数据K。&lt;/p&gt;

&lt;p&gt;综上所述，由于无论在CR还是CRAQ中，尾节点所存储的数据都是最新的具有一致性的数据，那么在可能出现的并发读写情况下，只需要向尾节点询问相关数据的版本，就可以确定应该回复的数据。&lt;/p&gt;

&lt;h3 id=&quot;membership-change-和-failure-recovery&quot;&gt;Membership Change 和 Failure Recovery&lt;/h3&gt;

&lt;p&gt;由于CRAQ中节点的结构类似于双向链表，所以Failure Recovery的策略其实就是Membership Change中的删除节点类似，也就是双向链表中节点的删除。设要删除的节点为D，某节点为N，其具体情况如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果D是N的后继：N需要把其所有数据传送给其新的后继。因为D有可能是上一节图中第二个节点的情况，那么写请求就不能传送给后继节点。如果D是尾节点，那么N需要D把所有ACK都反向传输完毕后再删除节点。&lt;/p&gt;

  &lt;p&gt;如果D是N的前驱：N需要把数据传送给其新的前驱。同时，如果D是头节点，N需要成为新的头结点。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;节点的添加大同小异，只需要在新节点A是头结点和尾节点时进行特殊处理即可。&lt;/p&gt;

&lt;h3 id=&quot;craq总结&quot;&gt;CRAQ总结&lt;/h3&gt;

&lt;p&gt;CRAQ对原有的Chain Replication进行了改进，平摊了读请求的压力，还用简单的模型保证了数据的强一致性。但是另一方面，从写请求的流程来看，需要线性地流经每一个节点势必增加系统的延迟。而Raft等算法则取决于单个机器的最长时间，如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/compare.png&quot; alt=&quot;compare&quot; /&gt;&lt;/p&gt;

&lt;p&gt;假设使用RAFT和CRAQ的系统中都有三个节点，黄色表示向非主节点写入指令需要的时间，蓝色表示该节点返回ACK花费的时间。可以看到，由于RAFT算法是并行的，它的实际时间取决于单个节点花费的最长时间。而CRAQ是串行的，它的时间是所有结点花费的时间总和。&lt;/p&gt;

&lt;p&gt;如果想要使用CRAQ的模型进行备份，那么节点数量一定不能太多，最好节点不要跨机房，否则在网络传输上花费的时间就十分庞大了。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">前面两篇文章，我们讨论了分布式系统中为了维护一致性所使用的共识性算法Paxos和Raft。这些算法保证了各机器之间数据的一致性，但是Paxos算法针对的是平等的Replication策略，而Raft算法针对的是Primary-Back Replication策略。Replication策略和共识性算法的目的不同，它的设计是为了实现容错（Fault-Tolerance），即在一部分机器不可用后，仍能保证正常提供服务。下面，我将简要描述一下6.824中讨论的两种复制模型Primary-Back Replication和Chain Replication。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">浅谈Raft</title>
      <link href="http://localhost:4000/%E8%B0%88%E8%B0%88Raft" rel="alternate" type="text/html" title="浅谈Raft" />
      <published>2020-07-19T18:00:00+08:00</published>
      <updated>2020-07-19T18:00:00+08:00</updated>
      <id>http://localhost:4000/%E8%B0%88%E8%B0%88Raft</id>
      <content type="html" xml:base="http://localhost:4000/%E8%B0%88%E8%B0%88Raft">&lt;p&gt;上图为Steam同名游戏———RAFT&lt;/p&gt;

&lt;p&gt;Raft算法是由Diego Ongaro和John Ousterhout于2014年提出的共识性算法。在斯坦福当助教时，他发现学生很难理解Paxos算法，所以他希望能用一种更简单易懂的算法来代替Paxos，以此为契机，他把便于理解作为目的，提出了Raft算法。&lt;/p&gt;

&lt;h2 id=&quot;paxos算法的不足&quot;&gt;Paxos算法的不足&lt;/h2&gt;

&lt;p&gt;了解过Paxos算法的朋友都知道，Paxos算法最后提出了2PC的机制来保证共识。但是，我们也提到了，两段式的请求消耗过大，而且多Proposer的前提也很难实现，Proposal的数据结构也未提及。总之，Basic Paxos算法在工程上有非常多的实现困难。另一方面，也就是上文提到的，非常难于理解。而Raft正是为了解决这两大问题，它提出并总结了系统中节点的角色，各角色的功能和职责，需要使用的RPC及其参数和逻辑，甚至连数据结构都清楚地给出了定义。这简直就像手把手教开发人员实现算法。而也正是由于清晰详细的介绍，Raft算法理解起来也相当地容易，顺带一提，可能是为了“便于理解”，论文本身的行文和用词也相当地简单。&lt;/p&gt;

&lt;h2 id=&quot;raft&quot;&gt;Raft&lt;/h2&gt;

&lt;p&gt;Raft算法使用Leader代替Paxos中的多个Proposer，使用Log Entry代替Proposal，以此简化问题。也因此，Raft算法将整个共识性问题划分为Leader Election，Log Replication两大块，同时，为了解决在崩溃时可能出现的问题，还提出了Safety的问题。此外，Raft还提供了Snapshot和Membership Change的解决方案。&lt;/p&gt;

&lt;p&gt;Raft将系统中的角色划分为三种，Leader，Candidate和Follower。整个系统只有三种种通信方式，AppendEntryies PRC，RequestVote RPC和InstallSnapshot RPC。其中，AppendEntries RPC只能由Leader发出，用于向Follower追加Log Entry或者广播Heartbeats，RequestVote只能由Candidate发出，用于发起Leader Election，InstallSnapshot RPC只能由Leader发出，用于向Follower发送快照。&lt;/p&gt;

&lt;h3 id=&quot;leader-election&quot;&gt;Leader Election&lt;/h3&gt;

&lt;p&gt;Raft将系统中的角色划分为三种，Leader，Candidate和Follower。所有机器初试为Follower，他们三者的转化关系如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/state transfer.png&quot; alt=&quot;state transfer&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Follower-&amp;gt;Candidate：当自身的计时器超时后，将自身转变为Candidate&lt;/p&gt;

  &lt;p&gt;Candidate-&amp;gt;Leader：当Candidate获得Quorum的选票时，成为Leader&lt;/p&gt;

  &lt;p&gt;Candidate-&amp;gt;Follower：当Candidate在RequestVote RPC中收到的Term大于该自己的Term，或者收到Term大于等于该自己Term的AppendEntries PRC时，就变为Follower&lt;/p&gt;

  &lt;p&gt;Leader-&amp;gt;Follower：当在RPC中收到的Term大于该机器的Term时，自动变为Follower&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;要特别说明的是，只有Candidate才能成为Leader，所以只有Candidate才能发起Leader Election。下面，我们逐条来分析这么做的理由。&lt;/p&gt;

&lt;p&gt;Follower-&amp;gt;Candidate：首先，我们要声明的是，Raft规定了一个时限，如果在这个时限内没有收到Heartbeat（即不携带信息的AppendEnyties RPC），那么就认为超时了。假设在初始系统中，所有的机器角色都是Follower，但是只有Candidate才能发起选举，那么要怎么才能得到Leader呢？于是，这一条的作用就出现了。由于系统中没有Leader，那么就不会有Heartbeats，也就是说必定有一个Follower会超时，按着这一条的转换关系，此Follower会转变成Candidate，然后发出Leader Election请求，直到系统中出现Leader。还有一种情况，本来正常工作的系统的Leader崩溃了，同样地，由于收不到Heartbeats，其中的Follower会转变成Candidate发起Leader Election，保证系统正常运行。&lt;/p&gt;

&lt;p&gt;Candidate-&amp;gt;Leader：这一条其实无需多言，按照Paxos中提出的Quorum机制，获得多数承认的机器可以成为Leader。&lt;/p&gt;

&lt;p&gt;Candidate-&amp;gt;Follower：不同的Term代表不同的Leader周期，而且Term是自增的。Candidate会在发起Leader Election之前将自己的Term加一，代表自己比之前的Leader更新。而如果收到了一个大于自己的Term的RequestVote RPC，这就意味有机器在时间上比自己更新，所以就应该主动退出竞争。如果仅仅收到和自己Term相同的RequestVote RPC，这意味着在这一轮选举中有竞争者，谁能成为Leader就要看网络的连通情况，这就无需变为Follower了。而如果它收到了一个AppendEntries RPC且Term大于等于自己，而此RPC仅可能由Leader发出，这就意味本轮已经选出Leader或者有更新的Leader了，此时就应该退出竞争。&lt;/p&gt;

&lt;p&gt;Leader-&amp;gt;Follower：我们知道Term的大小代表Leader的新旧，也就是说，当在RPC中接收到的Term大于自己时，有一个新的Leader被选出了。这种情况有可能由于暂时的网络故障，使得此Leader不能与其他机器连通，而当剩余机器选出Leader后网络恢复了，那么此机器中存储的数据很可能不够完整，所以应该让位给新的Leader。&lt;/p&gt;

&lt;p&gt;有了以上的要求，整个系统的角色转换已经可以顺利流转了。还需要关注的，是RequestVote RPC中的投票逻辑。在Raft算法中，使用“先来先投票”的原则，这个原则暗示了在多个Candidate竞争的过程中，与其他节点通信良好的会更有可能赢得选举。也就是说，选举出来的Leader与其他节点的通信延迟低，从而提升了系统的性能。但是，当本机器的Term大于RequestVote RPC中的Term时，说明本机器比发送请求的机器更新，此时应当拒绝请求。&lt;/p&gt;

&lt;p&gt;不过，假如有一种极端情况：在某种条件下，所有的节点同时成为Candidate。而按照上述策略，它们首先会投自己一票，从而产生人均一票的情况，然后重新开始选举，如此往复。为了避免这种情况，Raft使用等待随机时间后再发起选举的解决方案。&lt;/p&gt;

&lt;h3 id=&quot;log-replication&quot;&gt;Log Replication&lt;/h3&gt;

&lt;p&gt;Raft中的Log Entry对应于Paxos中的Proposal，但是它远比Propsal具体实际，也比Propsal强大。它是Write-ahead Log（WAL），也就是说，它在执行之前就被存储在可靠的磁盘上了，这种方案被广泛地用在实际的分布式系统中，以此提供一定的容错能力。&lt;/p&gt;

&lt;p&gt;在Raft中，Log Entry使用Term和Index来标记自己，以此区别不同Log Entry并比较其新旧程度。其中，Term代表不同的Leader周期，而Index代表在同一个Leader周期中的不同Log Entry。Log Entry的整个生命周期如下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Client向Leader发送请求{x-&amp;gt;1}，Leader将其包装成Log Entry，追加到自己的Log Entries尾部，此时该log Entry的状态为Append&lt;/li&gt;
    &lt;li&gt;Leader使用AppendEntries RPC将这条Log Entry广播给所有的Follower，要求Follower也将其追加到自己的Log Entries的尾部&lt;/li&gt;
    &lt;li&gt;当Follower完成后，会送回AppendEntries RPC的回答，这个回答可能是成功，也可能是失败。&lt;/li&gt;
    &lt;li&gt;如果Leader收到Quorum个的成功的回应，他会将其状态设为Commited，表示已经在Quorum个Follower中写入该请求。如果没有，则重新发送。&lt;/li&gt;
    &lt;li&gt;在将该Log Entry设为Commited后，Leader就可以在机器中执行{x-&amp;gt;1}，同时将结果返回给Client。此时，该Log Entry的状态为Applied。各Follower则会在后台选择时间执行该命令。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;综上所述，Log Entry一共有三种状态，Append，Commited和Applied。&lt;/p&gt;

&lt;p&gt;Append状态表示仅仅被追加到了Leader的Log Entries中，此时既未被其他机器承认，也未被执行。唯一的好消息是，它被存储在Leader的磁盘中了，哪怕Leader暂时崩溃了，只要系统恢复后该机器仍然是Leader，也能读取其中内容，继续向下执行操作。&lt;/p&gt;

&lt;p&gt;Commited状态表示此Log Entry已经得到了系统中Quorum个机器的共识，在这个状态下，它能够承受2f+1个机器中f个机器的崩溃，一旦系统恢复，它可以被安全地执行。&lt;/p&gt;

&lt;p&gt;Applied状态表示它已经被执行过了，只要处于该状态，那么在形成快照时，它就可以被删除。不过，由于各机器都在后台执行命令，Raft仅保证Log Entry最后能被执行但不保证执行的一致性。&lt;/p&gt;

&lt;p&gt;在追加Log Entry和维护Log Entry的过程中，Raft必须维护一下两条准则&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;如果在两个不同机器上的Log Entry拥有同样的Term和Index，那么这个Log Entry包含的Command相同&lt;/li&gt;
    &lt;li&gt;如果在两个不同机器上的Log Entry拥有同样的Term和Index，那么在这个Log Entry以前所有Log Entry都相同&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;由于每个Term仅有一个Leader，而Leader给每个Log Entry标记的Index不同，这就意味着Log Entry的Term和Index构成了全局唯一性。这个特性保证了第一条准则。&lt;/p&gt;

&lt;p&gt;第二个准则由AppendEntries RPC的内部逻辑保证。AppendEntries PRC的请求中带有PrevLogIndex和PrevLogTerm两个参数用来验证Follower中最新的Log Entry是否和Leader一致，这两个参数是AppendEntries RPC中Log Entry的前一个Log Entry的Index和Term。一旦发现不一致，Follower被要求删掉此Log Entry并返回失败，在一下次AppendEntries RPC时，Leader会将这两个参数改为当前Log Entry的前两个Log Entry的Index和Term，以此类推。直到找到Follower和Leader匹配的位置后，Leader会把从匹配位置开始直到最后的Log Entry全部发送给Follower。Leader通过这种强制更换的方式实现了第二条准则。这里要特别指出的是，在正常情况下不会出现Follower和Leader不匹配的情况，只有在机器崩溃的时候才可能出现，而这种强制更换掉的Log Entry中有两种情况，被删除和位置变更。其中被删除的Log Entry只有可能处于Append状态，位置变更的Log Entry则处于Commited状态。&lt;/p&gt;

&lt;p&gt;除此之外，AppendEntries RPC还要喝RequestVote RPC一样检查当前Term是否小于请求中的Term，否则将拒绝请求。在Leader的CommitedIndex大于本机器的CommitedIndex时，还应将本机器的CommitedIndex设为LeaderCommitedIndex和本机最新Index中的小的值，从而能跟上Leader的处理节奏。&lt;/p&gt;

&lt;h3 id=&quot;safety&quot;&gt;Safety&lt;/h3&gt;

&lt;p&gt;由于系统中的机器有可能发生故障和崩溃，而此时会产生许多意外的情况，所以Raft给Leader Election和Log Replication增加了一些限制来解决这些情况。&lt;/p&gt;

&lt;h4 id=&quot;election-resitriction&quot;&gt;Election Resitriction&lt;/h4&gt;

&lt;p&gt;如果按照原来的Leader Election的要求，那么我们可能会出现下面的情况。有一台原来网络状况不是那么好的机器，它能赶得上当前的Term，但是不能收到最新的Log Entry，假设它在同Term中总比其他机器少一个Log Entry。此时由于某种原因，它的网络状况改善了并按照选举原则拿到多数票，赢得了选举，成为Leader。那么，按照AppendEntries RPC的要求，它会删除掉其他机器中比它快的那一个Log Entry，然而这个Log Entry已经被Commited了，这就出现了问题，这个系统虽然保证了一致性但是却损失了信息。所以Raft在RequestVote RPC中添加了一个逻辑：通过和AppendEntries RPC一样的方式进行比较，如果Candidate的Log Entry比自己的旧，就投拒绝票，只有Candidate和自己一样新或者比自己更新才投同意票。&lt;/p&gt;

&lt;p&gt;基于这个约束和AppendEntries RPC中的Quorum原则，如果2f+1个中有f个机器未收到最新的Log Entry，那么在选举中，他们最多只能拿到f票，从而无法成为Leader。我们可以得出结论，只有拥有最新的Log Entry的Candidate才能成为Leader。&lt;/p&gt;

&lt;h4 id=&quot;committing-entries-from-previous-terms&quot;&gt;Committing Entries From Previous Terms&lt;/h4&gt;

&lt;p&gt;假设有如图情况&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/overwrite.png&quot; alt=&quot;overwrite&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中，黑框代表Leader，Log Entry中的数字代表Term。由图可知，在Term（2）中，S1是Leader，它向S1和S2写入了Log Entry但是由于某些原因宕机了。此时，S5得到S3，S4和自己的投票成为Term（3）的Leader。但是，仅仅把Log Entry追加到本机上后，在向其他服务器发送Log Entry之前就崩溃了，注意，此时该Log Entry未被Commited。此时，S1又成为了Term（4）的Leader，在它把一个Log Entry复制给S3后，由于某些原因又崩溃了。按照上述的选举原则，S5仍然可以拿到S2，S3，S4的投票，当他成为Term（5）的Leader后，它还未收到任何Client的请求，于是它按照AppendEntries的的逻辑使用强制替换保证了一致性。但是，值得注意的是，它使用Term（3）的Log Entry覆盖了Term（4）的Log Entry。这种情况是不被允许的，旧的数据是不能覆盖新的数据的。于是，Raft提出了约束：只有和当前Term相同的Log Entry才能被Commited。&lt;/p&gt;

&lt;p&gt;通过这个约束，上述的蓝色Log Entry就不会在重启后重新发送给其他机器，而客户端也永远不会收到它想要的答复。此时，如果Client需要此命令被执行，那么它需要重新发送请求。&lt;/p&gt;

&lt;h3 id=&quot;membership-change&quot;&gt;Membership Change&lt;/h3&gt;

&lt;p&gt;上面的三个部分其实已经完成了Raft算法的主体介绍，这一部分讨论的是Raft算法如何解决成员变更的问题，成员的组成在Raft中被称为Configuration。成员变更意味着Quorum的总人数和成员的变化，所以无论是对Leader Election还是Log Replication都有很大的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/membership.png&quot; alt=&quot;membership&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示，在指定时刻，S1，S2在旧的Configuration中，S3，S4，S5在新的成员中，但此时整个的成员交接还未完成。所以，对于旧的Configuration来说，S1和S2组成了Quorum，对于新的Configuration来说S3，S4，S5组成了Quorum。那么此时就出现了两个Quorum，可以实现两种决策，这是Raft算法必须要避免的。&lt;/p&gt;

&lt;p&gt;Raft算法使用Log Entry实现成员变更。当需要进行成员变更时，Leader将包含新Configuration的Quorum以及旧Configuration的Quorum的Log Entry发送给Follower，等待Quorum的回复以此将其设为Commited。在此过程中，由于有部分成员已经收到此Configuration，所以所有的决策都需要在此条件下进行，而由于Leader包含这个Configuration，所以能确保这一点。由于中间态的Configuration既包含旧的Quorum又包含新的Quorum，所以所有的决策都需要两方的同意，这就保证的在这个阶段的决策能符合新旧Configuration的要求。然后，新的Configuration才被广播给所有Follower。同样地，由于至少有一个成员在新的Configuration下工作，所以一旦被广播给Follower，所有决策都要在新的Configuration下进行。这样，就完成了成员变更。&lt;/p&gt;

&lt;p&gt;总得来说，Raft引入了一个中间态，在这个中间态中，既包含旧Configuration的Quorum，又包含新Configuration的Quorum，以此满足在此期间决策的正确性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/membershipChange.png&quot; alt=&quot;membershipChange&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;snapchat&quot;&gt;Snapchat&lt;/h3&gt;

&lt;p&gt;在Log Replication中我们说，所有Commited的Log Entry都要被记录在磁盘上，但是磁盘的容量是有限的，如果达到上限了怎么办？很明显地，Raft使用Snapshot记录下机器中此时的状态，再将最新执行的的Log Entry以前的Log Entry全部删除就可以了。&lt;/p&gt;

&lt;p&gt;详细地说，Raft算法允许每台机器各自执行Snapshot，而Snapshot必须记录它最后包含的Log Entry的Term和Index作为LastIncludedEntry和LastIncludedIndex来标记自己记录的位置。然后将这个Log Entry及其之前的全部删除就能达到清理空间的效果。&lt;/p&gt;

&lt;p&gt;但是，有一个情况我们必须考虑。假如一台机器网络情况和运行状况都很差，它最新的Log Entry甚至没有其他机器的快照新，这时候使用AppendEntries RPC是不能帮助它的，因为对应的Log Entry已经被删掉了。这里就需要Leader使用新的RPC，InstallSnapshot RPC来帮助它。InstallSnapshot RPC包含Leader的Snapshot的数据，LastIncludedEntry和LastIncludedIndex等信息，当这个Follower使用Snapshot更新自己的状态后，它就需要使用LastIncludedEntry和LastIncludedIndex来更新自己记录的信息，然后Leader可以再使用AppendEntryies RPC去更新它的Log Entry。&lt;/p&gt;

&lt;h2 id=&quot;总结和思考&quot;&gt;总结和思考&lt;/h2&gt;

&lt;p&gt;Raft相对于Basic Paxos做了很大的改进。&lt;/p&gt;

&lt;p&gt;首先，它使用Leader代替了多Proposer，跳过了Prepare阶段，减少了网络请求的消耗。&lt;/p&gt;

&lt;p&gt;其次，它显式地明确了使用WSL来作为Propsal的载体，不仅降低了开发难度，还提供了相当的容错能力。&lt;/p&gt;

&lt;p&gt;最后，它对容错的场景进行了深入地考虑并且给出了相应的解决方案。&lt;/p&gt;

&lt;p&gt;最重要的，Raft在保证共识性的同时明确地实现了数据的一致性，这对于算法落地来说实在难能可贵。&lt;/p&gt;

&lt;p&gt;但是，Raft使用的成员变更看起来并不是一个很好的方案。我认为在实践中使用Zookeeper作为成员变更的中间工具可能更加合适易行。&lt;/p&gt;

&lt;p&gt;总得来说，Raft算法实至名归，对得起Practical和Understandable的主旨。在GitHub上，有相当部分的项目都使用Raft算法作为他们的核心算法。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">Raft算法是由Diego Ongaro和John Ousterhout于2014年提出的共识性算法。在斯坦福当助教时，他发现学生很难理解Paxos算法，所以他希望能用一种更简单易懂的算法来代替Paxos，以此为契机，他把便于理解作为目的，提出了Raft算法。</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">浅谈Paxos</title>
      <link href="http://localhost:4000/%E6%B5%85%E8%B0%88Paxos" rel="alternate" type="text/html" title="浅谈Paxos" />
      <published>2020-07-13T04:00:00+08:00</published>
      <updated>2020-07-13T04:00:00+08:00</updated>
      <id>http://localhost:4000/%E6%B5%85%E8%B0%88Paxos</id>
      <content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Paxos">&lt;p&gt;图为真·Paxos———位于希腊的岛屿&lt;/p&gt;

&lt;p&gt;如果想要在后端开发上更进一步而不是局限于SSM框架，那么分布式是一个不那么坏的发展方向。如果要研究分布式系统，Paxos算法是绝对绕不过，也不能绕过的知识点。&lt;/p&gt;

&lt;p&gt;Leslie Lamport在1998年在《The Part-Time Parliament》中提出了Paxos算法，单单从论文名字看也知道这是篇不怎么正经的论文，这一结论在看完论文后又得到了印证。作者在论文中虚构了一个叫做Paxos的希腊城邦，这个城邦以议会作为最高权利机构，每条法令都需要在此议会中通过后方可实施，作者又对其中的细节作了一些描述和规定，以此符合分布式系统的实际模型。但是，这篇寓言性质的论文对于母语中文的我来说实在过于难懂，哪怕参考了许多资料和解释以后，我仍然很难理解这种模型，就算是读了《Paxos Made Simple》，也很难将其与这个故事一一对应起来。所以，我决定跳过这篇论文，从《Paxos Made Simple》开始说起。&lt;/p&gt;

&lt;h2 id=&quot;为什么需要paxos&quot;&gt;为什么需要Paxos&lt;/h2&gt;

&lt;p&gt;在开始谈Paxos细节之前，我想有必要谈谈为什么我们需要Paxos。几乎所有的资料都在说Paxos是一种共识性（Consensus）算法那么什么是共识呢，共识性和一致性（Consistency）有什么差别，为什么我们需要共识呢？我希望通过下面的例子解决这个问题。&lt;/p&gt;

&lt;p&gt;假设我们开了家“肥宅”奶茶店，为了简化模型，店里只卖珍珠奶茶，我们的配方为奶茶之比为1:1。如果只有一家店，自然是我们说了算。但是有人看我们开得不错，提出要入伙一起干，也不管我们同不同意，总之我们现在有两家店了。作为商业机密，配方是不能透露的，这家店老板也没多想，配方就定了奶茶比为2:1。那么此时，分歧就出现了，也就是说，两家店没有在奶茶的配方上达成“共识”。在这种情况下，由于没有“共识”，消费者在两家“肥宅”奶茶店买到的奶茶竟然味道不同，这就产生了“一致性”的问题。&lt;/p&gt;

&lt;p&gt;将上述的奶茶店换成计算机，配方换成提议，奶茶换成数据，就变成了分布式系统的模型。在奶茶店模型中，消费者喝到味道不同的奶茶倒是小事，但是如果在银行系统中，一台机器上余额是一百万，一台是负一百万那问题就大了。而且，由于现在绝大多数业务都需要保证数据一致性，那么保证提议的共识性就显得格外重要了。&lt;/p&gt;

&lt;h2 id=&quot;basic-paxos&quot;&gt;Basic Paxos&lt;/h2&gt;

&lt;p&gt;鉴于有很多朋友也像我一样无法理解《The Part-Time Parliament》，作者在2001年又发了《Paxos Made Simple》重新解释Paxos算法。作者在这篇文章中，终于用能看得懂的英语解释了Paxos算法是怎么运作的。&lt;/p&gt;

&lt;p&gt;通读全文，我们可以知道Paxos有两个目标安全性（Safety）和活跃性（Liveness）。其期望分别如下（此处采用Raft作者的理解）&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;安全性&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;每次提议仅有一个值被选定&lt;/li&gt;
    &lt;li&gt;服务器在值被接受前不会知道该值已被选定&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;活跃性&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;在一些提议中，最后必定有值会被选定&lt;/li&gt;
    &lt;li&gt;如果值被选定了，那么服务器最后总会知道该值&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;为了实现以上目标Paxos提出了两条约束，并通过数学证明：任何遵循这两条约束的系统都能保证共识性，此处我们仅讨论在文中作者如何推导出两条约束，不讨论数学上如何证明。该算法中，提议（Proposal）具有两个属性，编号（Number）和值（Value），算法中的规定的角色如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;提议者（Proposer）:负责提出提议&lt;/p&gt;

  &lt;p&gt;接受者（Acceptor）:负责审阅提议并决定是否批准&lt;/p&gt;

  &lt;p&gt;学习者（Learner）  :不参与议案过程仅学习通过的提案&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;考虑最简单的情况，我们仅使用一台机器作为Acceptor且接受Number相同的第一个Proposal，那么所有的提议都会由它审阅，并且只会有一种结果，能够保证共识。但是一旦这台机器宕机，那么整个系统就无法继续运行。所以使用多个Acceptor是必要的。此时，仅当Proposal被大于一半的Acceptor接受，该Proposal才被视为通过。虽然在文中此条件没有被显式地列为约束，但我认为其重要程度与后两者相当，因此我将其列为P0&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P0：当且仅当Proposal被大多数Acceptor接受（Accepted），该Proposal才被视为选定（Chosen）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;为了能保证Paxos 在一个Proposer和一个Acceptor的情况下工作，即符合活跃性的第一条要求，我们提出下面的方案作为约束：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P1：Acceptor必须接受其收到的第一个Proposal&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然而此约束存在一个问题，假如几个Proposal同时提出，且被分别发送发不同的Acceptor，每个Acceptor都接受一个Proposal，那么就会出现人均一票的情况，无法形成符合P0的情况，Paxos并未提出如何解决这一问题，而Raft使用随机等待的办法解决此问题。&lt;/p&gt;

&lt;p&gt;由P0的约束可知，需要大多数Acceptor接受提案，而P1则要求Acceptor接受其收到的第一个Proposal，那么这就要求每个Acceptor需要接受多个Proposal。这里，我们使用Proposal的Number作为区分。而为了保证被选中的Proposal具有相同的Value，我们提出以下约束：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P2：如果Number为n，Value为v的Proposal被选中，那么所有被选中且Number &amp;gt; n的Proposal都具有Value v&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;由于Proposal被选中意味着Proposal被大多数Acceptor接受，所以可以进一步约束为&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P2a：如果Number为n，Value为v的Proposal被选中，那么所有被接受且Number &amp;gt; n的Proposal都具有Value v&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;现在，我们假设一个新的 Proposer 刚刚从崩溃中恢复或加入此系统,并且发送了一个带有不同 Value 且Number更高的Proposal。P1要求Acceptor接受这个 proposal，但是却违背了 P2a。为了处理这种情况，需要继续加强P2a，于是我们得到&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P2b：如果一个Value为v的Proposal 被选中，那么之后每个 Proposer 提出的具有更大Number的Proposal都有Value v&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;P2b通过强制要求新的Proposal的值中含有Value解决了上述的问题。而作者通过数学工具，证明以下约束能够满足P2b&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P2c：对于任意的Value v和Number n，如果Value为v且Number为n的Proposal被提出，那么一定有多数Acceptor集合S满足：（a）S中不存在Acceptor已经接受任何Number小于n的Proposal，或者（b）S中的Acceptor所接受的Proposal中Number最高的具备Value v。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;文中根据P2c将整个流程划分为两段：准备（Prepare）和接受（Accept），但是主要是针对Proposer的，为了适应这个流程，进一步将P1约束如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P1a：如果Acceptor没有在Prepare阶段回复过Number大于n的请求，那么在Accept阶段，它可以接受Number为n的Proposal&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在得到上述的两个约束后，我们就能够以此为依据，得到具体的算法流程。&lt;/p&gt;

&lt;h3 id=&quot;2-phase-commit&quot;&gt;2 Phase Commit&lt;/h3&gt;

&lt;p&gt;上文提到整个算法流程分为两个阶段，下面我们将介绍具体流程是怎么样的。&lt;/p&gt;

&lt;p&gt;阶段一为准备阶段，具体如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Proposer 选择一个Number n，然后向大多数Acceptor发送Number 为n的Prepare request。&lt;/li&gt;
    &lt;li&gt;如果一个Acceptor接收到Number为n的Prepare request，并且n大于任何它已经回复的Prepare request的Number，那么它将承诺不再接受任何Number 小于 n的proposal，并且回复已经接受的最大Number的 proposal。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;阶段二为接受阶段，具体如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;如果Proposer 接受了来自大多数Acceptor对它的Prepare request 的回 复，那么接下来它将给这些 Acceptor发送Number为n，Value为v的 Proposal作为Accept request。其中v是收到的回复中最大 Number 的Proposal的Value，或者如果回复中没有Proposal的话，就可以是它自己选的任意值。&lt;/li&gt;
    &lt;li&gt;如果 Acceptor 收到一个Number 为n的Accept request，如果它没有对Number 大于n的Prepare request进行过回复，那么就接受该Accept request。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Raft作者的这张图形象地展示了Paxos的整个流程&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/paxos.png&quot; alt=&quot;Paxos&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;对paxos的一些思考&quot;&gt;对Paxos的一些思考&lt;/h2&gt;

&lt;p&gt;上述内容本质是对《Paxos Made Simple》的翻译和复述，下面我想谈谈我对这个算法一些思考。&lt;/p&gt;

&lt;p&gt;我仍然用上文奶茶店的例子，共识指的是两者对同一奶茶的配方认可，一致是奶茶的味道相同。初始奶茶店希望分店和自己采用相同的配方，这个过程就是寻求共识，如果分店同意，那么他们就达成了共识，这一阶段对应于提出Proposal。得到配方后的分店按配方制作奶茶，就能得到和初始奶茶店一样的味道，就相当于保证了数据的一致性。如果采用复制状态机的方案，奶茶配方在计算机系统中就是指令，两台初始完全相同的机器，在以同样顺序执行相同的指令后，就会得到一致的数据。也就是说，只要保证各机器对执行指令和顺序的共识，那么我们就能保证数据的一致性。&lt;/p&gt;

&lt;h3 id=&quot;为什么paxos需要p0&quot;&gt;为什么Paxos需要P0？&lt;/h3&gt;

&lt;p&gt;这个约束有两个作用，一是在拥有2f+1台机器的系统中，它能够允许f台机器同时崩溃，而仍能正常处理请求。其次，我们来考虑有2f+1个机器，而我们仅需要f个机器接受请求，那么假如所有的请求和数据都由前f个机器处理，而后f+1个机器没有任何数据。此时，前f个机器崩溃了，那么，我们的系统中就没有任何的数据了。但是，假如是要求f+1个机器接受请求，那么任何两次请求都必然有至少一个机器接受了两次请求，也就是说，它存有全部的最新数据。使用数学归纳法可以得知，无论接受了多少次请求，f+1个机器中所有的存储的数据可以拼接成完整的全部数据，那么，即使f个机器崩溃了，也能保证数据的完整。&lt;/p&gt;

&lt;p&gt;总得来说，Quorum机制能保证在2f+1台机器中f台机器崩溃的情况下保证Paxos协议的正常运行和数据的完整性。&lt;/p&gt;

&lt;h3 id=&quot;为什么basic-paxos只能处理single-decree&quot;&gt;为什么Basic Paxos只能处理“Single Decree”？&lt;/h3&gt;

&lt;p&gt;我们可以看到，如果Proposal希望能被接受，那么他必须包含之前所有被接受过的Proposal的Value，这在实践中是不可能实现的。另外，由于Basic Paxos允许多个Proposer，那么每个Proposal的Number大概率是不一致的，在跨事件的情况下，不能根据自增的Number来判断是否多个Proposal是对一个事件的共识。最重要的是，恐怕作者这篇论文的目的也仅仅是为了解决单个事件的共识。&lt;/p&gt;

&lt;p&gt;以前看这篇的论文的时候，由于之前了解的都是有Leader的系统，Proposal的不同Number的代表对不同Value的共识，然后在读Paxos时也代入了这种想法，就不能很好理解Paxos算法，现在从这个角度来看就容易理解得多。同时也可以解释为什么P2a和P2b中要求大Number应包含小Number有的Value，因为此时并不是两个事件，而是对同一事件的修改。比如两次的Value分别是{x=1}和{x=1，y=2}，那么实际上后者只是对前者的补充而已。&lt;/p&gt;

&lt;h3 id=&quot;为什么需要prepare阶段&quot;&gt;为什么需要Prepare阶段?&lt;/h3&gt;

&lt;p&gt;Prepare阶段主要是为了实现Proposer的共识，这也是和有Leader系统非常不同的一点。&lt;/p&gt;

&lt;p&gt;假设三个Proposer有三个版本的Proposal，具体如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Proposer1：{Number：1，Value：[X=1] }&lt;/p&gt;

  &lt;p&gt;Proposer2：{Number：2，Value：[X=1，y=2] }&lt;/p&gt;

  &lt;p&gt;Proposer3：{Number：3，Value：[X=1]，y=2，z=3] }&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果Proposer3最先到达Acceptor，那么根据Prepare阶段的要求，由于Proposer1和Proposer2的Number小于Proposer3，他们会被一直拒绝直到其内容和Proposer3相同。如果Proposer3最迟到达Acceptor，那么Proposer1和Proposer会在Acceptor阶段被拒绝。这里我们假设了Proposer3的Proposal是被选定的情况，如果它不是最终版本，那么，很有可能会出现Value中继续添加值的情况，然后Proposer3也会在第Accept阶段被拒绝，要求它重新提案。&lt;/p&gt;

&lt;p&gt;总之，由于Basic Paxos允许多个Proposer存在，所以需要Prepare阶段保证提案的一致性。&lt;/p&gt;

&lt;h3 id=&quot;为什么我们不使用basic-paxos&quot;&gt;为什么我们不使用Basic Paxos？&lt;/h3&gt;

&lt;p&gt;由于Basic Paxos需要Prepare阶段保证提案的一致性，而且一次算法的运行只能允许完成单次操作，所以如果直接使用Basic Paxos在性能上很可能会达不到我们的要求。因此，学界提出了更加符合实际的Multi-Paxos，使用Basic Paxos选举Leader，让Leader直接提案代替Prepare阶段，从而大大缩短了一次算法运行需要的时间。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Trafalgar Ricardo Lu</name>
        
        
      </author>

      

      
        <category term="distributedsystem" />
      

      
        <summary type="html">如果想要在后端开发上更进一步而不是局限于SSM框架，那么分布式是一个不那么坏的发展方向。如果要研究分布式系统，Paxos算法是绝对绕不过，也不能绕过的知识点。</summary>
      

      
      
    </entry>
  
</feed>
