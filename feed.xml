<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-02-13T22:30:21+08:00</updated><id>http://localhost:4000/</id><title type="html">Ghost</title><subtitle>The professional publishing platform</subtitle><entry><title type="html">我的2020</title><link href="http://localhost:4000/%E6%88%91%E7%9A%842020" rel="alternate" type="text/html" title="我的2020" /><published>2021-02-13T18:00:00+08:00</published><updated>2021-02-13T18:00:00+08:00</updated><id>http://localhost:4000/%E6%88%91%E7%9A%842020</id><content type="html" xml:base="http://localhost:4000/%E6%88%91%E7%9A%842020">&lt;h1 id=&quot;我的2020&quot;&gt;我的2020&lt;/h1&gt;

&lt;p&gt;转眼又是农历新年，家家户户辞旧迎新。在年交之时，总结过去，展望未来，这是去年我给自己定下的目标。不过，去年有个漫长的暑假，于是又给自己的研究生生涯做了计划。这样一来，年更的总结反倒像是半年更的了。&lt;/p&gt;

&lt;p&gt;虽然说是2020年的年度总结，但说实话，我对毕业之前在学校待过的匆匆一月也没有太多印象。记得的事也不过是提交实习项目报告和毕业设计两件事。实习报告由于疫情直接变成了自选项目报告，拿了个以前写的Todo List当模板，写了下实现的思路就交上去了。毕业设计则相当地水，然后我在演示的时候竟然出现了Bug，幸好老师不追究，也就匆匆水过。但是那个Bug其实也就是一行代码的关系，事后稍作修改，也就过了。&lt;/p&gt;

&lt;p&gt;毕业之后是漫长的暑假。暑假的前一个月，我隔天交叉学习MIT6.824和SICP。6.824实在让我受益匪浅，作为分布式的课程实在是再合适不过。而SICP则似乎名不符实，知乎上盛赞其是程序员的内功，毫不吝啬溢美之词。然而，学完以后我只觉得这门课的价值并没有那么高。这门课中提及的模块化的抽象，黑盒以及流式计算等概念实在太过经典，以至于每一个计算机专业的学生都不可能没有听过，因此再花费一些时间去重新学习用Lisp怎么表达和处理这些概念实在有些浪费。正如在&lt;a href=&quot;https://segmentfault.com/a/1190000005064958&quot;&gt;《MIT为何停掉SICP》&lt;/a&gt;中提到的“Sussman 指出，现代的软件工程师们的主要工作是为那些他们并不完全理解的复杂的硬件（由于商业机密的缘故，也不可能理解）而写代码。软件层面也存在同样的情况，编程环境是由无所不能的巨大的程序库构成”，SICP的所教授的知识更像是用来独立编写一个模块或者是开发整个软件所需要的知识，而现代分工过于细化和追求商业利益优先而进行的敏捷开发则与其背道而驰。不过，我依旧不能否认当我发现Lisp可以用两个操作写出自己解释器时的赞叹之情。&lt;/p&gt;

&lt;p&gt;九月份正式报到，算是开启了研究生生涯（虽然有点水）。流水账地回顾一下这半年的生活。九月份已经大致明确了将来要做的方向，要么分布式数据库，要么后台开发。因为6.824的余毒未清，所以九月份都在看一些额外的论文，Dynamo之类的。所以十月份主要在看CMU15-445的视频，给数据库打个底子并且写相关的课程作业。十一月份则是读《数据库系统概念》，再把MIT6.824的作业认认真真地做完。十二月读了交大出的《现代操作系统》，也读了一些RocksDB的源码。一月份的前一个星期和后一个星期在做TinyKV，而中间两个星期由于考试的缘故，在疯狂划水。&lt;/p&gt;

&lt;p&gt;这几个月的学习，也基本是让我决定了走上分布式数据库的道路。&lt;/p&gt;

&lt;p&gt;回忆起来，其实每个月的学习还算Solid，这些东西写简历上去找一个不那么好的实习也够用了。但是半年的时间实在是太紧了，分布式的基础理论刚刚入门，数据库的知识也仅仅是了解了个大纲，与这两者相关的计算机网络和操作系统都没有针对性地深入地了解。这些基础知识都只局限于408的范围，如果想要真正在这个方向走下去，必须要有能力修改底层的Linux源码或者自定义通信协议。&lt;/p&gt;

&lt;p&gt;不过，值得庆幸的是，我对于Raft算法已经有了一定的理解，至少所有关于Raft的问题，我都有信心答上来。另外，数据库的事务的隔离级别和实现，我也有一套可以自圆其说的理解。但是对于SQL Plan的生成，SQL的执行都还有从理论到实践的距离，毕竟14-445的实验实在算不上有工业级的质量。&lt;/p&gt;

&lt;p&gt;这里不妨结合我对分布式数据库的理解，做一个简单的知识梳理。分布式数据库有两大块的知识，分布式和数据库，其中分布式的通信涉及计算机网络，数据库的优化涉及操作系统，正如我上面说的，这两块知识是我必须补齐的。分布式数据库和单机的数据库区别在于两点，一：单机的数据库存储数据具有上限，所以必须将一张表的数据Split到两台机器，这一块负责的是拓展性，可以用Shard做到，二：由于大多数服务需要很强的可用性，所以分布式的数据库还需要提供备份的功能，这一块负责的是可用性，可以用Replica做到。在Shard时，我们要考虑单机数据库事务的ACID的要求，其中，Isolation和Atomic有别于单机数据库。其他的两个概念，Consistency本来就是不准确的概念，需要通过其他三个来保证，Duration和单机一样，都是用WAL来实现的。Atomic其实就是要求一个事务要么在所有机器上都执行，要么都不执行，一般使用2PC来解决这个问题。Isolation在单机中三种实现方案，2PL，TO和OCC，貌似被使用的只有2PL和OCC。这里也对应两种方案，Percolator使用2PL加2PC来解决A和I，Omid使用OCC加2PC来实现A和I。而Replica则比较简单，只需要在一个Group里执行Raft或者Paxos算法就可以了。另外，在读取完需要的数据以后还要执行的Join或者Count等操作就要设计分布式计算的内容，这一块实在不太了解。其他的数据库内容，比如底层使用以LSM Tree为结构的KV存储还是参照传统数据库使用Page的存储都可以视为单机的问题。&lt;/p&gt;

&lt;p&gt;最后一部分是对未来的展望和计划。&lt;/p&gt;

&lt;p&gt;首当其冲的是表达能力，这么多次面试下来，最大的感觉就是不能将自己学的东西表达出来，并不是不会，而是开始面试以后说话不会深思熟虑，只想更快地回答问题。自我反思以后感觉有两个问题，第一个是心态问题：其实说句自大的话，我自己应该至少是华为保底的，那么大可不必那么害怕失败，面试本质是一个技术交流的场景，在这个场景里不会的就说不会，会的就尽可能地吹起来，不应该有太多的心理负担；第二个是对知识的梳理不够清晰，简历的东西其实都在我自己的脑子里，无非是记得清楚不清楚的问题，但是只有把所有的东西都清楚的梳理一遍，才能在说出来的时候有条理，有自信。我想对于这两点的解决方案是预设一些问题，然后反复地练习怎么回答，最后才能达到想要的状态。&lt;/p&gt;

&lt;p&gt;先讲一讲离得近的计划——实习。目前直投了阿里的提前批，后面至少要投华为，微软，阿里，字节这几家，所以一个合理的顺序是必要的。我准备是按照华为，阿里，字节的顺序投，不写微软是因为微软的面试内容只有算法，和其他的没什么关系，不构成攒经验的说法，然后在华为和阿里中间应该还要加几家一般的公司。期望的目标是，希望能在微软先做几个月，然后到明年的三月份再去字节或者阿里实习。这么考虑是因为微软的实习比较轻松也比较近，所以能够有比较长的时间带薪自习，去了解数据库相关内容的实现，Linux源码和计算机网络，再学习一下C++的知识，最好能够看到Azure内部的文档，学习一下他们分布式数据库的实现。最后，准备充分以后再去面试大厂的核心数据库部门，开启第二段实习，实习到九月份转正，十二月毕业就能直接入职。&lt;/p&gt;

&lt;p&gt;最后再聊一聊开源项目和GSoC。去年也说了，希望今年能够申请一个GSoC的项目，在给TinyKV提交过两个Pull Request以后，大概能了解到开源项目需要怎么样的贡献，另外，在学习分布式数据库的过程中也了解到很多的新技术，希望能把Proposal写得漂亮，详实一点，最好能申请到和数据库相关的工作。如果真的能够申请下来，那么这无疑是简历上的巨大两点。在开源项目方面，我非常看好PingCAP的前景。随着信息化程度加深，需要存储的数据指数级别增加，开源数据库必然是大势所趋，大厂有能力自己研发，小厂只能寻求开源的解决方案，MySQL的方案实在不够优雅，PingCAP的HTAP相当诱人。另外，我认为他们做Share-Nothing的结构也是出于小厂没有能力搭建分布式文件系统的考虑。如果能去PingCAP实习，也不失为一个好选择。&lt;/p&gt;

&lt;p&gt;回顾去年，最大的收获是发现了自己过去四年都在白混，如果能在本科四年确定要学的东西并且认真学下去，那我应该能找到一份不错的工作。但是现在也算是因祸得福，虽说也不知道算不算福气，走上了分布式数据库的道路，如果能在这条路上走下去，应该能成为核心团队的成员，35岁危机也小一些。现在，我总算是知道了要学什么——分布式数据库，怎么学——看国外公开课，做作业，参与开源，希望我能在这条路上好好走下去。&lt;/p&gt;

&lt;p&gt;最后的最后，祝我自己找到一份不错的实习，拿到GSoC的项目，学习未知的知识时能够融会贯通，走在历史的进程上。&lt;/p&gt;</content><author><name>Trafalgar Ricardo Lu</name></author><category term="essay" /><summary type="html">转眼又是农历新年，家家户户辞旧迎新。在年交之时，总结过去，展望未来，这是去年我给自己定下的目标。</summary></entry><entry><title type="html">共识性算法的发展</title><link href="http://localhost:4000/Paxos%E7%AE%97%E6%B3%95%E7%9A%84%E5%8F%91%E5%B1%95" rel="alternate" type="text/html" title="共识性算法的发展" /><published>2020-12-05T18:00:00+08:00</published><updated>2020-12-05T18:00:00+08:00</updated><id>http://localhost:4000/Paxos%E7%AE%97%E6%B3%95%E7%9A%84%E5%8F%91%E5%B1%95</id><content type="html" xml:base="http://localhost:4000/Paxos%E7%AE%97%E6%B3%95%E7%9A%84%E5%8F%91%E5%B1%95">&lt;p&gt;本文是工程实践的一篇总结，上图为达成共识。&lt;/p&gt;

&lt;p&gt;在分布式系统中，由于机器可能宕机，网络可能短连等许多问题，所以如何保证多个副本之间的一致性成为了最重要的问题之一，而Paxos算法就是解决这个问题的算法。&lt;/p&gt;

&lt;h2 id=&quot;basic-paxos&quot;&gt;Basic-Paxos&lt;/h2&gt;

&lt;p&gt;Basic Paxos就是Lamport在Paxos Made Simple中提出的最初的Paxos算法。&lt;/p&gt;

&lt;p&gt;Basic Paxos允许多个Proposer和多个Acceptor，同时允许一个节点兼任两个角色。&lt;/p&gt;

&lt;p&gt;这个算法包含两个部分：一个是Prepare阶段，二是Acceptor阶段。&lt;/p&gt;

&lt;p&gt;在Prepare阶段，多个Proposer可能提出多个提案，算法会选择具有最大id的提案，并且将该id记录在本地。这里，我们可以认为Prepare阶段的作用是选出“最新”的提案，并且在Acceptor阶段尝试提交这个提案。&lt;/p&gt;

&lt;p&gt;当提案的Proposer收到Quorum个确认之后，Proposer就会进入Accept阶段。&lt;/p&gt;

&lt;p&gt;在Accept阶段，Acceptor会根据本地记录的提案的最大id来判断是否接受该提案，这一步会拒绝非“最新”的提案。当Proposer在Acceptor阶段收到Quorum个确认之后，就认为共识已经达成了。&lt;/p&gt;

&lt;p&gt;Basic Paxos最明显的优点是解决了分布式系统中的共识问题。&lt;/p&gt;

&lt;p&gt;Basic Paxos的缺点则是每次Paxos算法的运行需要经历两个阶段，这中间需要多次网络IO，而且每次算法只能确定一个值。这两个问题使得Basic Paxos算法的效率很低。所以就催生了Multi-Paxos。&lt;/p&gt;

&lt;h2 id=&quot;multi-paxos&quot;&gt;Multi-Paxos&lt;/h2&gt;

&lt;p&gt;Multi-Paxos的基本想法是：Prepare阶段的作用是选出“最新”的提案，而在实际的环境中，针对一个值或者一条命令，并不会在同一时间出现两种提案，也就是说不会在Prepare阶段中相互争抢。那么就可以使用一台机器作为Leader去接受客户端的请求，同时将该请求作为提案，直接进入Accept阶段。&lt;/p&gt;

&lt;p&gt;然而，Leader的引入也造成了一个问题：在Basic Paxos中，由于有2f+1个Proposer，所以能容忍f个机器宕机。但是在Multi-Paxos中，由于只有一个Leader，那么必须要考虑怎么处理Leader宕机的情况。&lt;/p&gt;

&lt;p&gt;这里就需要Leader Election的机制，当检测到Leader宕机以后，就会发起这个流程，Leader Election其实也是一个共识问题，所以使用的算法仍然是Basic Paxos。当Leader Election结束以后，会有新的Leader代替旧的Leader继续运行。&lt;/p&gt;

&lt;p&gt;几乎所有的工业级软件使用的共识性算法都是Multi-Paxos，但是另一方面，因为Multi-Paxos只是一个模糊的概念，所以没有一个统一的规范，所以，我认为只要是实现了Paxos算法并且使用了Leader机制的实现都可以称为Multi-Paxos。&lt;/p&gt;

&lt;p&gt;Multi-Paxos使用Leader机制取代了Prepare阶段，将网络IO减少了一半，同时实现了能够连续地确定值，大大提高的Basic Paxos的效率。&lt;/p&gt;

&lt;p&gt;但是，就像我们上面提到的，Multi-Paxos只是模糊的概念，各有各的实现，而彼此之间的实现细节差异也很大。&lt;/p&gt;

&lt;h2 id=&quot;raft&quot;&gt;Raft&lt;/h2&gt;

&lt;p&gt;Raft是由Diego Ongaro提出的共识性算法，虽然有很多人认为它和Paxos是两种不同的算法，但我更倾向于认为它是Paxos的一种实现，更准确地说，Raft就是一种Multi-Paxos。但是Raft更加地清晰易懂，只要按照论文中的Figure 2要求，完全能够实现达到基本要求的Raft算法。&lt;/p&gt;

&lt;p&gt;Raft中还引入了一些概念：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;复制状态机：如果两台机器以同样的状态开始，并且按照同样的顺序执行相同的命令，那么在执行完同样数量的命令以后，两台机器的状态当前的状态也是相同的。&lt;/li&gt;
  &lt;li&gt;Ack：Follower将命令写入本地后回复的确认&lt;/li&gt;
  &lt;li&gt;Commit：当某个Log收到Quorum个Ack后，它的状态就是Commit的&lt;/li&gt;
  &lt;li&gt;Apply：当某个Log状态为Commit，那么就可以被Apply到状态机中&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Raft要求在正常运行的状态下，所有Follower（即之前的Acceptor）的Log都和Leader保持一致。为了实现这个目标，所有的Log在Ack时要保证之前的Log已经Ack了，所有的Log在Commit时要保证之前的Log已经Commit了，所有的Log在Apply时要保证之前的Log已经Apply了。&lt;/p&gt;

&lt;p&gt;简单地说，就是要按序Ack，按序Commit，按序Apply。&lt;/p&gt;

&lt;p&gt;另外，如果有Follower中的Log和Leader的Log不一致，就要删除该Log和其以后的所有Log，并把Leader的对应Log复制到Follower的对应位置上。&lt;/p&gt;

&lt;p&gt;Raft还明确提出了检测Leader宕机的机制。Leader必须定期向Follower发送心跳信息，一旦在规定的时间内Follower没有收到信息，就认为和Leader断开连接（有可能是网络故障，也可能是Leader宕机），这时候Follower就会发起Leader Election，尝试选出新的Leader。&lt;/p&gt;

&lt;p&gt;Raft的优点是给出了一套完全可行的实现共识的算法，同时考虑了使用状态机实现副本的数据一致。&lt;/p&gt;

&lt;p&gt;Raft最为人诟病的是它按序Ack，Commit，Apply的要求。这样一来，虽然Raft使用Leader减少了一半的网络IO，但是Multi-Paxos是可以乱序Ack和Commit的，也就是说Raft不能是并行的。而这在广泛使用并行提高效率的网络中几乎是不可接受的。&lt;/p&gt;

&lt;h2 id=&quot;parallelraft---polarfs&quot;&gt;ParallelRaft - PolarFS&lt;/h2&gt;

&lt;p&gt;在PolarFs的论文中，他们提出了ParallelRaft，但是要指出的是这种算法只适用于PolarFS。&lt;/p&gt;

&lt;p&gt;ParallelRaft允许Log乱序Ack，Commit，Apply，但是，允许乱序Ack和Commit的后果就是Log的记录中会出现“洞”，那么在Apply时就会出现问题。这里，他们给出的方案是使用Look Aside Buffer作为”桥“跨越”洞“，“桥”中记录了之前N个文件块的修改情况，而且这个“桥”是全局的。&lt;/p&gt;

&lt;p&gt;在论文中，他们指出桥的长度为2，简单来说，当执行到第i个Log时，如果之前i-1和i-2个Log缺失了，但是“桥”中记录表明，这两个Log涉及的文件块和第i个Log无关，那么可以直接执行第i个Log，中间的两个Log可以等到补全时再执行。&lt;/p&gt;

&lt;p&gt;但是，这种方案过于激进，而且只适用于PolarFS。因为PolarFS有特别的组件PolarCtrl来监测这些信息，恰恰是因为这是一个文件系统，记录的信息没有那么多，而且写文件块也不算频繁才能使用这一方案。&lt;/p&gt;

&lt;p&gt;如果想要用在数据库中，仅仅考虑对某一行的插入和更新两个操作就很难使用这个算法实现。&lt;/p&gt;

&lt;h2 id=&quot;tikv&quot;&gt;TiKV&lt;/h2&gt;

&lt;p&gt;在TiKV中，PingCAP给出了一种相对保守合理的优化。&lt;/p&gt;

&lt;p&gt;在原始的Raft中，必须要完整地Ack，Commit，Apply完一个Log才能继续处理下一个Log。而TiKV则并行地发送AppendEntries请求，并不等到上一个Log被Apply结束，其他的要求和原始的Raft相同。虽然这是一个简单的优化，单个请求花费的时间也没有减少，但是由于是并行地处理请求，整个流程是流水线的，系统的吞吐量大大提高了。&lt;/p&gt;

&lt;p&gt;另外，TiKV还提出了三种读数据的方案。&lt;/p&gt;

&lt;p&gt;第一种方案类似于原生的Raft读。传统的读需要像写一样经历整个Raft流程，然后从Leader处读取数据。这里TiKV只要求发送Heartbeats确认自己是Leader就可以返回数据。&lt;/p&gt;

&lt;p&gt;第二种方案使用了Lease机制。Leader和Follower达成一个共识：在Lease规定的时间内，不会发起Leader Election。由于Raft所有的写都经过Leader，所以只要不重新选举Leader，就可以保证Leader的数据是系统中最新的，那么在这段时间内，Leader可以直接响应写请求。&lt;/p&gt;

&lt;p&gt;第三种方案类似于Zookeeper的sync()请求。Follower也能接受读请求，但是在响应前必须发送请求给Leader确认自己是否拥有最新的数据。如果是，那么就可以直接回应请求，如果没有，就必须等到Apply到和Leader一样的位置。&lt;/p&gt;

&lt;p&gt;TIKV给出的写方案是我认为相对合理优秀的。另外，他的读的策略也有值得借鉴的地方。&lt;/p&gt;

&lt;h1 id=&quot;共识算法优化的思考&quot;&gt;共识算法优化的思考&lt;/h1&gt;

&lt;p&gt;杨新泰同学转达了Terrillma老师对目前算法的局限性的观察。这里，我阐述一下我的对这两个问题的思考。&lt;/p&gt;

&lt;h2 id=&quot;对于lease的思考&quot;&gt;对于Lease的思考&lt;/h2&gt;

&lt;p&gt;Lease机制的工作流程是这样的：Master将Lease颁发给某一台机器，这台机器会持有一段时间Lease，在这个时间段内，这台机器就是Leader。为了减少Leader的变更，在这段时间里，Leader每执行完一次命令就会更新Lease的时间。&lt;/p&gt;

&lt;p&gt;简单地说，Lease机制主要有两个作用：一是代替Leader Election，由Master挑选一个Leader；二是实现Failure Detection。当一台机器不再持有Lease时，有两种可能，一是在他的任期内没有执行命令，二是这台机器宕机了。其中，前者的可能相当地小，所以可以认为如果Lease过期，大概率是因为宕机。&lt;/p&gt;

&lt;p&gt;很直观地，Failure Detection的最大延迟就是Lease的持有时间。假设当一台机器在收到Lease后立刻宕机了，那么Master必须等到Lease时间结束才能检测到这个情况。&lt;/p&gt;

&lt;p&gt;同样的，在使用Heartbeats检测宕机的解决方案中，最大的延迟是设定的超时时间。但是，由于这个时间是在Follower中的，所以和Leader的任期时间无关，可以设置得相对短一些。&lt;/p&gt;

&lt;p&gt;所以，如果想要加快Leader的替换时间就必须减少宕机检测的时间。由于Lease设置的时间是针对Leader的，而Heartbeats是针对Follower的，所以可以使用Heartbeats来替换Lease。&lt;/p&gt;

&lt;p&gt;但是，如果单纯使用Raft中的Heartbeats，就需要花费额外的时间进行Leader Election。&lt;/p&gt;

&lt;p&gt;所以，我的初步的想法是：使用Master用较短的时间和Leader进行Heartbeats，但是Master又额外维护每个副本的最大Log Index。这里的信息由每个副本在写入WAL后，发送给Master。那么当Master发现Leader不可达时，就将拥有最大Index的设置为Leader，继续服务。&lt;/p&gt;

&lt;p&gt;这里存在相当多的问题，比如一台机器在写入第i个Log后宕机了，没有给Master发送信息，而另一台机器只有i-1条Log，但是却被选为Leader，那么Leader的数据就不是最新的。诸如此类，我还没有深入地思考过。这仅仅是一个相当初步的想法。&lt;/p&gt;

&lt;h2 id=&quot;对于并发写的思考&quot;&gt;对于并发写的思考&lt;/h2&gt;

&lt;p&gt;在重新读了Megastore以后，我认为它的写逻辑和原生的Raft类似。两者都是通过类似NextIndex确定下一个写入位置，从而保证了强一致性。但是因为必须等到nextIndex更新才能执行下一条命令，所以是非并发的。&lt;/p&gt;

&lt;p&gt;这里，我想可以使用TiKV的优化方案，实现流水线式的并发写。&lt;/p&gt;

&lt;p&gt;我们从复制状态机的角度重新考虑这个问题。状态机的状态取决于Apply的顺序和Apply了多少命令，而和Ack和Commit是无关的。所以从状态机的角度来说，它并不关心Log是否按序Ack和Commit。也就是说，两台机器的数据一致性只取决于Apply而不是Ack和Commit。&lt;/p&gt;

&lt;p&gt;另外，由于Leader的完备性等性质，可以确定当前Term的Leader的数据是最正确最完整的。AppendEntries的失败有多重可能，总结起来就是两种，发出AppendEntries请求的Leader是过时的，或者，Follower的Log出现错误。也就是说只要是当前的Leader发出的AppendEntries，在大多数情况下是会最终成功的（除非Leader宕机了）。&lt;/p&gt;

&lt;p&gt;所以，我们完全可以实现Log的乱序Ack和乱选Commit，而只要求Apply是按序的。这样，能够实现并发地处理网络请求，写WAL。理论上能大幅提高系统的吞吐量。&lt;/p&gt;

&lt;h1 id=&quot;总结&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;以上的两个方案是相当粗糙的，没有深入思考过的方案。希望大家能够多多讨论可能出现的问题。我个人对Paxos的理解可能也有偏颇之处，希望大家能够多多指正。&lt;/p&gt;</content><author><name>Trafalgar Ricardo Lu</name></author><category term="distributedsystem" /><summary type="html">在分布式系统中，由于机器可能宕机，网络可能短连等许多问题，所以如何保证多个副本之间的一致性成为了最重要的问题之一，而Paxos算法就是解决这个问题的算法。</summary></entry><entry><title type="html">回顾MIT 6.824</title><link href="http://localhost:4000/%E5%9B%9E%E9%A1%BEMIT6.824" rel="alternate" type="text/html" title="回顾MIT 6.824" /><published>2020-08-29T19:00:00+08:00</published><updated>2020-08-29T19:00:00+08:00</updated><id>http://localhost:4000/%E5%9B%9E%E9%A1%BEMIT6.824</id><content type="html" xml:base="http://localhost:4000/%E5%9B%9E%E9%A1%BEMIT6.824">&lt;p&gt;上图为MIT 6.824教授Robert Morris。&lt;/p&gt;

&lt;p&gt;大概是在六月底，经过了一段时间的划水，决定开始学一点研究生该学的东西。之前 一直学的后端开发入门的技术，比如怎么使用Spring，MyBatis之类的框架，在学这些技术的过程中虽然听说了一些分布式系统的组件和原理，但是在那个时候觉得这些都是进阶的知识，大可以工作以后再认真学，就匆匆掠过。研究生嘛，总要学得深入一点，正好在知乎上听闻这门课的风评很好，今年又录了高清的授课视频，就决心跟着这门课学下去。&lt;/p&gt;

&lt;p&gt;6.824差不多是一节课读一篇论文，所以要想更好理解上课的内容，还是在课前就读完对应的论文比较好。于是我安排周一周三读论文，周二周四看视频，周五做实验，周日写论文读后感。这里说是读后感，也不过就是中文的摘要罢了。按照这样的进度，经过了大约一个的时间，我在八月第一个星期完成了全部的学习，这期间又被知乎安利去看了《Design Data-Intensive Application》。通过这些学习，我大概算是对分布式系统有了一个大致的了解。&lt;/p&gt;

&lt;h2 id=&quot;分布式系统结构&quot;&gt;分布式系统结构&lt;/h2&gt;

&lt;p&gt;分布式系统中有三个子系统，分别是分布式计算系统，分布式存储系统和分布式管理系统。在6.824中，主要讨论分布式存储系统，但通过两篇论文聊了聊分布式计算系统，但是没有涉及分布式管理系统。虽然这里分得很清楚，但是实际上来说，分布式存储系统提供数据给分布式计算系统，分布式管理系统负责监测相关信息，保证正常运行。&lt;/p&gt;

&lt;p&gt;分布式系统我们得从Google的三驾马车讲起。谷歌将分布式系统分成三级，GFS负责分布式文件系统，在GFS之上是Bigtable，负责分布式数据库，再上面是分布式计算系统MapReduce。这种分层几乎沿用至今，最多只在分布式数据库之前再加一层缓存。&lt;/p&gt;

&lt;p&gt;从更高的维度来看，当我们解决一致性问题以后，我们甚至可以把分布式系统看做是单机的。磁盘存文件对应文件系统，分布式数据库对应单机数据库，而计算则对应我们的代码逻辑。或者说，这反映了分布式系统设计的目标，让客户端感受不到“分布式”的存在，用起来跟单机完全一样。&lt;/p&gt;

&lt;p&gt;那么，我们以自底向上的顺序聊一聊整个架构中可能用到的技术和解决方案。&lt;/p&gt;

&lt;h3 id=&quot;分布式文件系统&quot;&gt;分布式文件系统&lt;/h3&gt;

&lt;p&gt;分布式文件系统设计之初的目标是存储更多的文件，也就是可拓展性（Scalability），但是在使用的过程中发现容错（Fault-Tolerance或者Resilience）也是一个很重要的目标，那么整个的系统的设计就必须围绕这两个点。&lt;/p&gt;

&lt;p&gt;首先考虑怎么实现可拓展性，GFD采用的办法是使用一个服务器存储所有文件的位置，读写文件都要访问这个节点拿到想要访问或添加的文件。这个想法类似于Linux的文件系统，元文件块存储FCB的位置，要读取FCB就要访问元文件块。这个方法简单而且有效，但是很遗憾，我不能拿其他的方案作比较，因为我暂时还没有读过其他的分布式论文，之后会去看看Ceph和PolarFS是怎么实现的。&lt;/p&gt;

&lt;p&gt;实现容错的通用方案是使用备份（Replica），当前节点宕机以后就使用备份接替工作。根据6.824和《DDIA》的内容，目前主要的备份方案分为主从复制和链式复制，其中主从复制又根据主节点的个数分为单主节点复制，多主节点复制和无主节点复制。单主节点的方案使用比较多，Spanner和许多系统都使用这种方案。根据《DDIA》的说法，MySQL的Tungsten Replicator，用于PostgreSQL的BDR以及用于Oracle的GoldenGate都使用了多主复制，但是我对这些还没有了解。无主复制在相当长的时间里都不是很流行，但是当Amazon开发了无主复制的Dynamo以后，就有相当多的公司和组织尝试用无主复制的方案开发工具。相较之下，链式复制就要低调得多，但是Amazon Aurora中使用了这一方案。&lt;/p&gt;

&lt;p&gt;虽然我们已经使用备份解决了容错的问题，但是另一个问题随之而生，如何保证多个备份之间的数据一致性？在主从复制中，经常使用Paxos，Raft和Zab来解决这些问题，而在链式复制中，由于数据会流经每一个节点，所以必然能够保证数据的一致性，只不过需要花费更久的延迟。&lt;/p&gt;

&lt;p&gt;另外一方面，为了追求高吞吐量，分布式文件系统会使用分片技术将数据分发到多个节点上，这样对数据的请求压力也能够被多个节点一起承担。在这里，为了实现数据均匀分布，会使用哈希算法决定数据位置。同时还会使用一致性哈希算法减少节点崩溃时需要的数据传输量。&lt;/p&gt;

&lt;h3 id=&quot;分布式数据库&quot;&gt;分布式数据库&lt;/h3&gt;

&lt;p&gt;说完了分布式文件系统，我们继续聊分布式数据库。分布式数据库的目标包括数据一致性和事务管理两大目标。当然，肯定不仅限于这两点，上面提到的容错和可拓展性也是重要的目标，但是解决方案类似，也就不再讨论。&lt;/p&gt;

&lt;p&gt;在分布式数据库中，数据的一致性从脏读，幻读等隔离级别变成了如下的级别。遗憾的是，虽然6.824中用两三篇论文讨论过了这些问题，我对这部分的理解仍然相当有限，所以很难作进一步的解释。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Linearizable（线性）&amp;gt; Sequential（顺序）&amp;gt; Causal（因果）&amp;gt; RyW(read your write)。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;不过可以确定的是，分布式数据库通过对事务的管理尽可能实现强的数据一致性。最广为人知的事务管理算法是2PC，但是2PC在两段提交的过程中可能会由于某一方节点宕机而失败，并且其效率很低，所以被渐渐抛弃。之后有人提出了TCC的方式，但是我也不是很了解。在这之后，分布式系统中引入了消息队列，由于队列的性质能够保证FIFS，所以在一定程度上解决了事务的先后问题，但是消息队列仍然有消息丢失等问题。特别要说的是，Spanner使用TrueTime实现了全球一致的时间并且使用这个时间一定程度上进行事务管理，是我看到的最好的方案。目前分布式事务仍然有很多问题需要解决，不失为一个不错的研究方向。&lt;/p&gt;

&lt;p&gt;数据库的另一个重要性能指标是数据读写的速度。当然，我们不能通过软件提高硬件的性能，但是可以通过合理的数据模型减少检索数据所需要的时间。Bigtable就使用了SSTable将数据有序存储提高性能，Spanner和F1数据库则使用父子表的设计支持关系型数据存储并加快检索。&lt;/p&gt;

&lt;h3 id=&quot;分布式计算&quot;&gt;分布式计算&lt;/h3&gt;

&lt;p&gt;分布式计算在课程中占比很少，主要讨论了两个方案，MapReduce和Spark。&lt;/p&gt;

&lt;p&gt;前者的特点是模型简单，但是需要已经准备好的数据，有点类似于批处理，这就注定了MapReduce不能用于快速响应的情景。另外，由于基础模型过于简单，在希望实现复杂逻辑的时候，编程就比较困难。这些问题导致MapReduce没有在今天大范围使用。&lt;/p&gt;

&lt;p&gt;Spark使用了流式计算的方案，每当有数据输入就能生成对应的结果。另外，Spark也支持比较复杂的操作，像JOIN和GROUP，相当程度上弥补了MapReduce的缺点。这使得在其推出后大受欢迎，在今天也有很多人使用。&lt;/p&gt;

&lt;h3 id=&quot;中间件&quot;&gt;中间件&lt;/h3&gt;

&lt;p&gt;目前的分布式系统中，除了上述的层级还有一些组件用于在各层之间协调，6.824介绍的Memcache就是其中的一种。&lt;/p&gt;

&lt;p&gt;Memcache用于用户层和数据库之间，由于查询数据需要经过磁盘，读写效率很低。所以Memcache将一些数据放在内存中，只有当Memcache中没有用户需要的数据时才去查询数据库，这就大大提高了查询效率。&lt;/p&gt;

&lt;p&gt;在分布式系统中还经常使用Zookeeper作为协调组件。Zookeeper提供一个运行在内存中的小型文件系统，系统可以使用这个文件系统实现锁服务，成员管理等相当多的功能。&lt;/p&gt;

&lt;p&gt;上面我们还提到了消息队列，现在的系统中也广泛使用类似于Kafka，RabbitMQ等组件来实现消息解耦，同时减少服务器压力。不过消息队列似乎仅在可以异步处理的情况下比较有效。&lt;/p&gt;

&lt;h2 id=&quot;课程内容&quot;&gt;课程内容&lt;/h2&gt;

&lt;p&gt;可能是囿于课程时间的限制，6.824没有完整地讨论很多东西，比如Bigtable和Paxos就没有讨论。另外，我觉得课程的安排也有一些问题，比如它把MapReduce作为第一课，虽然MapReduce够简单，也很合适实验设置。但是我想一课可以安排Paxos或者Raft作为基础，在这之后可以完整详细地讨论谷歌的三驾马车，这之后再讨论分布式事务，最后讨论一下分布式计算。这门课程花了三节课的时间讨论区块链，这实在是不值得，我想最多只能花两节课时间再这上面。个人认为区块链的唯一用处是用Work Proof解决了拜占庭问题，但是Work Proof的花费太大以至于几乎不可能在实际中使用。&lt;/p&gt;

&lt;p&gt;另外要说的是这门课的实验。很难，真的很难，从实验二开始就相当地难。实验二是实现Raft算法，这就要求对Raft算法有很深入的了解，至少Figure2是要懂透的，这对于初学者来说过于痛苦了。很丢人地说，我只完成了0.5个实验，我连实验一的case都没有全部通过，实验二三四就都是读别人的代码，只能在感叹别人强悍的同时自我反省。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;哪怕我认为6.824有一些不足，但是对于初学分布式系统的人来说仍然是最好的课程，没有之一。课程中大量的论文能帮助初学者快速入门，足够系统的知识点保证在学习完课程之后能够对分布式系统的体系有大致的把握。我个人推荐把网上对论文的分析解读和实际的论文以及视频中的内容结合起来，这样能够更好地理解论文中一些的设计意图和巧妙之处。虽然实验很难，但是如果能够独立完成全部实验的话，那么我想对理解Raft算法和分片，分布式数据库的理解都会更进一步。在完成6.824的课程之后，可以去读一读《DDIA》，这本书通俗易懂，知识丰富，如果对哪个知识点感兴趣的话完全可以去读书中的参考文献。&lt;/p&gt;</content><author><name>Trafalgar Ricardo Lu</name></author><category term="distributedsystem" /><summary type="html">大概是在六月底，经过了一段时间的划水，决定开始学一点研究生该学的东西。之前 一直学的后端开发入门的技术，比如怎么使用Spring，MyBatis之类的框架，在学这些技术的过程中虽然听说了一些分布式系统的组件和原理，但是在那个时候觉得这些都是进阶的知识，大可以工作以后再认真学，就匆匆掠过。研究生嘛，总要学得深入一点，正好在知乎上听闻这门课的风评很好，今年又录了高清的授课视频，就决心跟着这门课学下去。</summary></entry><entry><title type="html">浅谈Memcache</title><link href="http://localhost:4000/%E6%B5%85%E8%B0%88Memcache" rel="alternate" type="text/html" title="浅谈Memcache" /><published>2020-08-24T19:00:00+08:00</published><updated>2020-08-24T19:00:00+08:00</updated><id>http://localhost:4000/%E6%B5%85%E8%B0%88Memcache</id><content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Memcache">&lt;p&gt;上图为Facebook登录页面图片&lt;/p&gt;

&lt;p&gt;Facebook拥有庞大的社交网络和因此产生的频繁的请求，所以为了提高响应速度，使用了缓存的策略处理这一问题。通常情况下，用户的读写请求都会落到数据库上，而数据库的数据存储在磁盘上，磁盘低下的读写效率对响应的速度影响很大。所以，Facebook使用运行在内存中的Memcached作为中间件，先在其中查询相关数据，只有当Memcached中没有请求的数据时才向数据库查询。他们充分利用了内存远高于磁盘的读写速度，实现了业务的优化。下面，我会根据其论文谈谈他们具体是怎么做的。要特别指出的是，Memcache指的的整个系统，而Memcached则是服务器中的组件，众多Memcached服务器组成了Memcache系统，两者是不同的。&lt;/p&gt;

&lt;h2 id=&quot;总体架构&quot;&gt;总体架构&lt;/h2&gt;

&lt;p&gt;Memcache的总体架构如下图所示&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Memcache Architecture.png&quot; alt=&quot;Memcache Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;类似于Aurora和Spanner，Facebook也有Region的概念。现在，我们可以基本确定，Region指的是物理上实现地理分割的不同数据中心，这样划分可以根据用户的地理位置选择最近的数据中心来减少延迟。不同的Region之间只同步存储的数据。每个Region中有多个Cluster（集群），每个Cluster中又有多个Web服务器和Memcache系统，但每个Region只有唯一的存储集群。在这里，我想每个Cluster负责的业务应该是不同的，而每个Cluster中的多个服务器则应该是被用于实现均衡负载。&lt;/p&gt;

&lt;p&gt;另外要讲的是Memcache的读写模型。Memcache使用的是Look-aside策略：服务器先尝试向Memcache读取数据，如果没有数据，则服务器再向数据库读取数据，读取完毕后将Memcache中的数据更新。示意图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Look-aside.png&quot; alt=&quot;Look-aside&quot; /&gt;&lt;/p&gt;

&lt;p&gt;与之对应的是Inline策略，服务器尝试向Cache读取数据失败后，由Cache而不是服务器去数据库读取数据，再从Cache返回数据给服务器。&lt;/p&gt;

&lt;p&gt;两者的区别在于处理数据缺失时的压力是由服务器还是缓存来承担。&lt;/p&gt;

&lt;p&gt;下面，Memcache根据Cluster和Region分别讨论起内部的要求和优化。特别要说的是，在Facebook的应用场景中，读请求远比写请求多。&lt;/p&gt;

&lt;h2 id=&quot;in-cluster&quot;&gt;In Cluster&lt;/h2&gt;

&lt;p&gt;在Cluster中，Memcache主要关注两点，一是延迟（Latency），二是负载（Load）。&lt;/p&gt;

&lt;h3 id=&quot;latency&quot;&gt;Latency&lt;/h3&gt;

&lt;p&gt;在Facebook的服务中，一次get请求不仅仅只读取单个K-V对，根据统计，一次页面的加载平均要从Memcache读取521个不同的数据，所以必须要采取一些措施来降低读取数据的延迟。&lt;/p&gt;

&lt;p&gt;第一，Memcache使用DAG来表示数据直接的依赖关系。如果有两组完全不相关的数据，那么这两组数据就可以并行地读取，而单个DAG中的数据可以一次性提交读请求而不是每个数据提交一次。这里使用的并行和批读取数据有效地提升了读取速度。&lt;/p&gt;

&lt;p&gt;第二，Memcache使用UDP来发送读请求，TPC处理Set和Delete请求。由于UDP只负责发送而不考虑可靠性，也不维护链接，所以UDP要比TCP快。在大多数为读请求的情况下，使用UDP而不是TCP能减少读取的时间。而Set和Delete请求则要求必须可达，所以使用TCP。下图展示了UDP和TCP的性能比较。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Memcache Udp.png&quot; alt=&quot;Memcache Udp&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第三，Memcache自己实现了一个类似TCP的拥塞窗口控制的机制以防止出现热点或阻塞。因为使用UDP传输读请求，所以自然也就没有TCP的拥塞窗口控制了。那么一旦出现大量的请求，UDP可能会使请求阻塞或者直接抛弃请求，抛弃请求则会使客户端重新发起请求，这两者都会影响整体的响应时间。所以Memcache在上层又实现了滑动窗口机制。&lt;/p&gt;

&lt;h3 id=&quot;load&quot;&gt;Load&lt;/h3&gt;

&lt;p&gt;在Look-aside模型中，当数据不再Memcache中时，会向数据库读取数据，在这种情况下，会出现很多情况，从而导致负载加重。Memcache使用下列方法减少负载。&lt;/p&gt;

&lt;p&gt;第一，Memcache使用Lease处理两个问题：Stale Set和Thundering Herd。Stale Set指的是服务器在Memcache中设置了过期的数据，这个数据不能反映最新的值。Thundering Herd指的是某个值在一段时间内被大量地读和写，如果Memcache中存的数据不是最新的，读和写就要去请求数据库，这导致数据库的负载大幅增加。&lt;/p&gt;

&lt;p&gt;为了解决Stale Set问题，在Cache 未命中的时候，Memcache会给客户端分配一个64位的Lease， 当拿到数据库数据之后，需要使用这个Lease来证明其身份。如果这个数据在返回之前被Delete或者更改了，也就是说，现在的数据是更新的，那么这个Lease就会被认为是无效的，从而拒绝旧的写入。&lt;/p&gt;

&lt;p&gt;为了解决Thunder Herd问题，Memcache会每10秒钟给这大量请求中的一个请求分配Lease，其他的客户端则被要求等待。这个持有Lease的请求会继续其流程，访问数据库，更新Memcache中的数据。而当Memcache中的数据被更新后，其他客户端会重新发起请求，这时大多数就能拿到其想要的数据了。那么，在整个流程中，重复的查询和写入被减少为一次。论文中表示，Lease的引入将数据库的查询次数从17K/S降低到了1.3K/S。&lt;/p&gt;

&lt;p&gt;第二，Memcache使用了Memcache Pool来分别处理不同场景的数据。论文中举了例子说，可以为经常访问但处理Cache未命中的开销不大的Key提供一个小的Pool 。可以为不经常访问的Key提供一个大型Pool ，因为对于这些Key来说，Cache未命中的代价很高。&lt;/p&gt;

&lt;p&gt;另外，论文还讨论了应该使用分片还是复制降低负载。考虑系统总共每秒1M个请求，一次客户端发起的请求包含100个key。如果是使用分片的话，假设对应的数据平均分配在在两台机器上，100个Key的请求，每台机器存储的恰好是50个Key，那么两台每秒仍然要处理1M个请求，只是每个请求拿的key少了，这是存储问题，但是不能解决 QPS 过高。使用复制的话，那么每台机器就只需要处理0.5M个请求了。所以，Memcache最后使用复制实习均衡负载。&lt;/p&gt;

&lt;h3 id=&quot;handle-failure&quot;&gt;Handle Failure&lt;/h3&gt;

&lt;p&gt;在缓存系统中，我们仍然要考虑容错的问题。因为Memcache是缓存系统，所以不影响数据库本身的正确性，那么我们就无需考虑一致性的问题，只需要考虑均衡负载，把本应分配给崩溃机器的请求分配给其他机器。&lt;/p&gt;

&lt;p&gt;Memcache将崩溃分为两种情况，很少一部分机器崩溃，很大一部分机器崩溃。&lt;/p&gt;

&lt;p&gt;很少一部分机器崩溃的解决依赖自动修复系统，Memcache准备了约占总机器数1%的机器来接管崩溃机器的服务，这些备用机器被称为Gutter。一旦客户端发现请求没有回应，它们就认为该机器崩溃，然后将请求转发到Gutter服务区上。&lt;/p&gt;

&lt;p&gt;很大一部分机器崩溃的话，Memcache会考虑将这个Cluster的请求转发到其他Cluster上，以保证服务能正常提供。&lt;/p&gt;

&lt;h2 id=&quot;in-region&quot;&gt;In Region&lt;/h2&gt;

&lt;p&gt;在这部分中，Memcache讨论了在同一Region中的一些问题。&lt;/p&gt;

&lt;h3 id=&quot;regional-invalidations&quot;&gt;Regional Invalidations&lt;/h3&gt;

&lt;p&gt;考虑一下，某个客户端的写入请求会使存储集群的数据发生变化，但是这里的变化没有经过其他Memcache或者其他客户端，所以Memcache中的缓存实际是过时的，但是其他客户端和其他Memcache都不能意识到这一点，所以必须由数据存储集群来处理这个问题。&lt;/p&gt;

&lt;p&gt;Memcache通过设置守护程序McSqueal来解决这个问题。当更新内容被Commit后，其会被送入到McSqueal中，McSqueal会判断这条语句是否会影响数据一致性。如果发现可能影响一致性的操作，它会将相关的信息送入Mcrouter中，再由Mcrouter发送给Memcache，这样Memcache就能根据相关的信息更新本地的数据，以此实现数据的一致性。在这里，McSqueal还使用批处理来降低系统的负载。其示意图如下。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Memcache Invalid SQL.png&quot; alt=&quot;Memcache Invalid SQL&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;region-pool&quot;&gt;Region Pool&lt;/h3&gt;

&lt;p&gt;由于每一个前端集群是独立处理发送过来的请求的，如果用户的请求被随机分配到每个集群，那么每个集群缓存的数据是差不多的。如果每个集群都保存一份相同的数据，那么有点得不偿失。所以Memcache设置一个Region Pool，即多个前端机器共享相同的Memcache集群。这样既能保证请求被响应，也能减少存储空间。&lt;/p&gt;

&lt;h3 id=&quot;warm-up&quot;&gt;Warm Up&lt;/h3&gt;

&lt;p&gt;在一个新的Memcache集群上线时，整个集群是没有数据的，那么所有转发过来的请求都会未命中。这时候如果这些请求全部去查询数据库，那么数据库的压力会很大。所以冷集群要热身，它会先去其它正常运行的集群去做查询用户需要的数据，如果命中了就不用去数据库查了。这大大减少了数据库的压力。&lt;/p&gt;

&lt;h2 id=&quot;across-region&quot;&gt;Across Region&lt;/h2&gt;

&lt;p&gt;由于Facebook有多个Region，而且每个Region的存储集群都是独立的，所以要保证数据的一致性还要额外的操作。由于读请求不会影响数据一致性，这里考虑两种情况，一是主Region写，二是从Region写。&lt;/p&gt;

&lt;p&gt;处理主Region写要简单一些，当主Region接受写请求后，它会将命令写入数据库，然后广播给其他Region。从Region接受到广播的信息后就利用我们之前提到的McSqueal来同步本地存储和Memcache之间的数据。&lt;/p&gt;

&lt;p&gt;处理从Region写则要麻烦一点，因为从Region必须保证和主Region一致而不能单独处理请求，一旦从Region在本地执行请求而主Region不处理，就会出现数据不一致的情况。这里，Memcache牺牲了一点效率，它会把从Region收到的写请求打上标记然后转发给主Region。只有当主Region处理完该请求并发回到从Region时，才会清除这个标记，那么此时，数据也就一致了。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;Memcache最关键的一点在于它使用内存作为服务器和数据库之间的缓存，大大提高了数据访问的速度。在现在的系统设计中，Redis或者Memcache都是必不可少的一部分。另外，它在优化延迟和负载时使用的解决方案也相当有趣，虽然看起来很简单，但从数据上来看相当使用。但是，从我个人角度来看，他在实现缓存一致性的问题上解决方案不是很高效。想要在Memcache中实现跨Region之间或者跨Cluster之间的缓存数据一致性，那么必须要经过数据库，还要使用McSqueal和Mcrouter调整，效率想必是很低下的。不知道这后来有没有更好的解决方案。&lt;/p&gt;</content><author><name>Trafalgar Ricardo Lu</name></author><category term="distributedsystem" /><summary type="html">Facebook拥有庞大的社交网络和因此产生的频繁的请求，所以为了提高响应速度，使用了缓存的策略处理这一问题。通常情况下，用户的读写请求都会落到数据库上，而数据库的数据存储在磁盘上，磁盘低下的读写效率对响应的速度影响很大。所以，Facebook使用运行在内存中的Memcached作为中间件，先在其中查询相关数据，只有当Memcached中没有请求的数据时才向数据库查询。他们充分利用了内存远高于磁盘的读写速度，实现了业务的优化。</summary></entry><entry><title type="html">浅谈Spanner</title><link href="http://localhost:4000/%E6%B5%85%E8%B0%88Spanner" rel="alternate" type="text/html" title="浅谈Spanner" /><published>2020-08-17T19:00:00+08:00</published><updated>2020-08-17T19:00:00+08:00</updated><id>http://localhost:4000/%E6%B5%85%E8%B0%88Spanner</id><content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Spanner">&lt;p&gt;上图为扳手。&lt;/p&gt;

&lt;p&gt;在2012年的OSDI上，谷歌发表了《Spanner: Google’s Globally-Distributed Database》，其中介绍了谷歌第二代的数据库，也就是Bigtable的继任者——Spanner。在使用Bigtable的过程中，谷歌的开发人员逐渐意识到Bigtable的一些不足之处，比如不能处理变化的数据格式，不能保证大范围内数据库的一致性以及对跨行事务的处理。谷歌为了解决这些问题，开发出了Spanner。&lt;/p&gt;

&lt;h2 id=&quot;总体架构&quot;&gt;总体架构&lt;/h2&gt;

&lt;p&gt;谷歌设计Spanner的一个重要目标是对全球范围内的数据库进行管理，为了更加清晰有效地划分和管理数据，Spanner划分了多个层级。其中，最高级的是Universe，在论文中谷歌表示目前只有三个Universe，包括一个用于测试或后台运行的Universe，一个用于部署或生产的Universe和一个仅用于生产的Universe。每个Universe下面包含多个Zone，每个Universe使用UniverseMaster和PlaceDriver检测和管理Zone，这里的Zone就相当于Bigtable的Server，对应实际情况中的一台或多台物理机器。而每个Zone中有一个ZoneMaster管理多个LocationProxy和数百至数千个SpannerServer，其中，LocationProxy负责将客户端的请求转发到对应的SpannerServer。论文中给出的示意图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Spanner structure.png&quot; alt=&quot;Spanner structure&quot; /&gt;&lt;/p&gt;

&lt;p&gt;论文中仅仅披露了SpannerServer的具体内容，所以下面我们也只讨论SpannerServer。由于Spanner是为了代替Bigtable而设计的，所以SpannerServer的内部架构其实和Bigtable有一点类似，但是Spanner又作出了很多优化。内部架构的示意图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Spanner Server.png&quot; alt=&quot;Spanner Server&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这张示意图以三副本的情况为例介绍了Spanner Server的内部架构。Spanner和Bigtable相似的地方在于，他们都基于分布式的文件系统GFS，这里的Colossus是第二代GFS，具体的实现细节仍未公布，而且他们都使用了Tablet作为单位管理存储数据，但是，这里的Tablet和Bigtable中Tablet是有些不同的。但是再上面一层就有些不同了，因为Bigtable中数据基于爬虫获得而且使用了SSTable存储数据，这保证了数据的唯一性，所以在Bigtable中没有使用具体的算法保证数据的一致性，但也因为这一点，Bigtable没有很好地支持事务。Spanner中使用了多副本的机制备份数据，同时在副本之间使用Single Paxos算法保证了数据一致性，在这里，谷歌可能是为了提高Paxos算法的性能并降低耦合，他们并不是把整个机器作为Paxos算法的基本角色，而是将机器中的数据分割为多个Paxos Group，每个机器中的相同数据被标识为同一Group，每次运行Paxos算法都仅由关联的Paxos Group参与。再往上一层就是各机器之间的关系了，Spanner使用主从结构管理副本，Leader节点要额外维护LockTable和Transaction Manager，其中LockTable的作用类似Bigtable中的Chubby，用于协调各副本之间的并发操作，Transaction Manager则用于管理分布式事务。Leader节点还要负责所有的写操作和和其他节点的沟通。&lt;/p&gt;

&lt;p&gt;Paxos Group仍然是很大的操作单位，想要更加灵活地进行数据迁移工作就需要更小的数据单位。于是Spanner将每个Paxos Group分割为多个Directory，而每个Directory包含若干个拥有连续前缀Key的数据。不得不说，这里的连续前缀Key有点像SSTable的设计。Spanner将Directory作为物理位置记录的单元，同时也是均衡负载和数据迁移的基础单元。这很好理解，均衡负载是把请求转发到拥有相同数据的机器，数据迁移是把数据从一台机器复制到另一台机器，这两者都需要目的机器的物理地址，所以最小只能把Directory作为单位。&lt;/p&gt;

&lt;p&gt;Spanner把在Paxos Group之间迁移Directory设计为后台任务，但是由于数据迁移可能造成读写阻塞，所以它不被设计成事务。操作的时候是先将实际数据移动到指定位置，然后再用一个原子的操作更新元数据，完成整个移动过程。&lt;/p&gt;

&lt;p&gt;这里要特别说明的是，Spanner中的Tablet和Bigtable中的Tablet有些不同。Bigtable中的Tablet可以简单地看做是若干连续的有序记录，而Spanner中的Tablet则被设计成一种容器，其不一定是连续的有序记录，而可能包括多个副本的数据。&lt;/p&gt;

&lt;h2 id=&quot;数据模型&quot;&gt;数据模型&lt;/h2&gt;

&lt;p&gt;Spanner和Bigtable的数据模型差别也很大，其数据模型如下。很容易地可以发现，其从Bigtable中类似于关系型的数据库变成了类似于K-V的数据库。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Spanner datamodel.png&quot; alt=&quot;Spanner datamodel&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这种变化重要的原因是Bigtable的数据模型仅适用于类似PageRank等数据格式长期稳定且不怎么变化的任务，如果任务需要快速版本迭代可能就不再使用。&lt;/p&gt;

&lt;p&gt;另一方面，在Google内部有一个Megastore数据库，尽管要忍受性能不够的折磨，但是在Google有300多个应用在用它，包括Gmail, Picasa, Calendar, Android Market和AppEngine。而这仅仅因为Megastore支持一个类似关系数据库的语法和同步复制。所以，Spanner决定支持数据库的语法，论文中说这种语法类似于SQL语句。其结果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Spanner Data Example.png&quot; alt=&quot;Spanner Data Example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，Spanner底层的存储结构仍然是K-V的形式，但是在创建Albums的表时，其指出了父类表为User，那么在Directory中组织成了上图中关联的User和Albums相邻的形式。那么在进行SQL查询的时候，就可以通过顺序读写得到数据，相比随机读写要快得多。&lt;/p&gt;

&lt;h2 id=&quot;truetime&quot;&gt;TrueTime&lt;/h2&gt;

&lt;p&gt;前面讨论了Spanner总体架构，内部架构以及数据模型，平常的论文可能到此就结束了，但是这篇论文到这里进入最重要的部分，因为Spanner引入了一个开创性的想法，使用TrueTime标记时间。而TrueTime指的是真实的时间戳，谷歌使用GPS和原子钟两个物理元件得到这个时间。正常情况下使用GPS获取该时间，如果GPS由于电波影响不能工作，那么原子钟就会接替任务直到GPS恢复工作。Spanner中提供了三个和其相关的API，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/TrueTime API.png&quot; alt=&quot;TrueTime API&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中，TT.now()返回的值称为TTinterval，它不是一个确切的时刻，而是一段时间，包括最早时间戳Earlist和最晚时间戳Latest。因为全球范围内的时间总是不可能完全同步，各机器的通信也有延迟，所以只要在一个时间段内，他们就认为是同步的。另外两个函数的伪代码定义如下&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TT.after(t) = TT.now().earliest &amp;gt; t.latest
TT.before(t) = TT.now().latest &amp;lt; t.earliest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Spanner给每个数据中心都安装了TrueTime系统，有了这三个API，Spanner就能判断两个时间戳的先后关系了，再以此为基础，就能实现很多功能。&lt;/p&gt;

&lt;h2 id=&quot;并发控制&quot;&gt;并发控制&lt;/h2&gt;

&lt;p&gt;TrueTime是相当强大的工具，Spanner中相当多的操作都依赖于它。下面的表列举了Spanner支持的操作类型：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Spanner Transaction.png&quot; alt=&quot;Spanner Transaction&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;leader-leases&quot;&gt;Leader Leases&lt;/h3&gt;

&lt;p&gt;在讨论事务之前，我们回到Paxos算法上。与Raft使用的解决方案不同，Spanner不使用Heartbeats来检测是否有失效节点，而是使用Lease来规定Leader的任期。如果Leader执行了写操作，那么它的Lease会自动延长。否则，Spanner默认每十秒Leader要发起续租Lease的请求，当收到Quorum的投票后会延长，反之就失去Lease，转变为Follower。&lt;/p&gt;

&lt;p&gt;在这里，Spanner要求单个Paxos Group中的Leader Lease中的TrueTime要不相交，因为一旦相交，就意味同一时刻有两个Leader，这是不被允许的。所以，当Leader退出系统或者降级为Follower时，要保证TT.after(t)对每个副本都成立。&lt;/p&gt;

&lt;h3 id=&quot;读写事务&quot;&gt;读写事务&lt;/h3&gt;

&lt;p&gt;Spanner保证了强外部一致性：如果一个事务T2开始（Start）在事务T1提交之后发生，那么T2的提交（Commit）时间肯定比T1的提交时间大。简单地说，早到早完成，迟到迟完成。&lt;/p&gt;

&lt;p&gt;Spanner使用2PC提交事务，为了保证外部一致性，Spanner对每个事务作出以下两点要求：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Start：事务T的协调者Leader分配了一个提交时间戳S，S不小于TT.now().latest并且S不早于Commit请求到达时间&lt;/p&gt;

  &lt;p&gt;Commit Wait：Spanner保证只有当当前时间晚于事务T的提交时间后，其他成员才能看见。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;论文中的符号表示如下
&lt;img src=&quot;/assets/images/Spanner formula1.png&quot; alt=&quot;Spanner Sympol1&quot; /&gt;
&lt;script type=&quot;math/tex&quot;&gt;e^{Start}_i:事务Start事件\\e^{Server}_i:事务Commit事件请求到达\\e^{Commit}_i:中事务Commit完成事件\\t_{abs}(e):该事务完成的真实时间\\S_{i}:事务Commit的时间戳\\&lt;/script&gt;
那么上述的要求就变为
&lt;img src=&quot;/assets/images/Spanner formula2.png&quot; alt=&quot;Spanner Sympol2&quot; /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
Start:t_{abs}(e^{Server}_i) &lt; S_{i}\\
Commit Wait:S_{i} &lt; t_{abs}(e^{Commit}_i)\\ %]]&gt;&lt;/script&gt;
有如下推断
&lt;img src=&quot;/assets/images/Spanner formula3.png&quot; alt=&quot;Spanner Sympol3&quot; /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
如果\\
t_{abs}(e^{Commit}_i) &lt; t_{abs}(e^{start}_{i+1})\\
那么\\
S_{i} &lt; t_{abs}(e^{Commit}_i)\\
t_{abs}(e^{Commit}_i) &lt; t_{abs}(e^{start}_{i+1})\\
t_{abs}(e^{start}_{i+1}) &lt; t_{abs}(e^{server}_{i+1})\\
t_{abs}(e^{Server}_{i+1}) &lt; S_{i+1}\\
所以\\
S_{i} &lt; S_{i+1} %]]&gt;&lt;/script&gt;
所以，只要满足上述两个要求，就能实现外部一致性。而Spanner中使用Coordinate Leader来实现这两个要求。&lt;/p&gt;

&lt;p&gt;另外，还需要保证任意时间读取到的数据都是可靠的安全的。Spanner通过规定安全时间保证任意时间的读。论文中安全时间的定义为
&lt;img src=&quot;/assets/images/Spanner formula4.png&quot; alt=&quot;Spanner Safe&quot; /&gt;
&lt;script type=&quot;math/tex&quot;&gt;t_{sate}=min(t^{Paxos}_{safe},t^{TM}_{safe})\\
t^{Paxos}_{safe}:Paxos算法能保证的安全时间\\
t^{TM}_{safe}:事务管理器能保证的安全时间&lt;/script&gt;
其中，Paxos算法能保证的安全时间是最近Paxos算法写入数据的时间。事务管理器能保证的安全时间在，没有事务处于Prepare阶段的情况下，是无限大的。因为它无法确定第二阶段的事务可能发生什么。如果有事务已经完成Prepare阶段，那么它的值就是完成Prepare阶段的事务中最早的时间-1。在这个时间前，事务管理器能保证所有已提交的数据都被看见。只要读取的数据在这个时间之前，那么这个数据就是安全的。&lt;/p&gt;

&lt;p&gt;以下是读写事务的具体流程&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;执行读操作时，Spanner先找到相关数据的副本，然后加上读锁并读取最新的数据。在事务开启的时候，客户端会发送Keeplive消息防止超时。&lt;/li&gt;
    &lt;li&gt;当客户端完成了所有的读操作并缓存了所有的写操作，就准备开始2PC。客户端选择一个Coordinator Group，并给每一个相关的Leader发送Coordinator的id和缓存的写数据。&lt;/li&gt;
    &lt;li&gt;每个和事务相关的Leader（除了Coordinator Leader）会尝试得到一个写锁，然后选取一个比现有事务晚的时间戳并通过Paxos发送给其他副本。（这里是为了保证上面的假设）&lt;/li&gt;
    &lt;li&gt;Coordinator Leader一开始也会上个写锁，但是会跳过Prepare阶段。当接受到其他Leader的时间戳之后，他会选择一个提交时间戳。这个提交的时间戳必须满足Start保证并且大于所有完成Prepare阶段的时间戳（安全读取保证）。然后将这个信息通过Paxos记录下来。&lt;/li&gt;
    &lt;li&gt;Coordinator必须等到TT.after(S)成立才能在副本提交事务。这是为了满足Commit Wait保证。这需要等两倍时间误差，大约是20ms。&lt;/li&gt;
    &lt;li&gt;然后Coordinator将提交时间戳发送给客户端还有其他的副本。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;只读事务&quot;&gt;只读事务&lt;/h3&gt;

&lt;p&gt;一个只读的事务会分2阶段来执行：先给该事务分配一个时间戳，然后执行事务的读操作。相当于通过快照读来读取时刻S的值。那么只需要保证如下要求，由于时间永远向前，所以能保证读取的是最新的数据。
&lt;img src=&quot;/assets/images/Spanner formula5.png&quot; alt=&quot;Spanner Sympol5&quot; /&gt;
&lt;script type=&quot;math/tex&quot;&gt;S_{read}=TT.now().least&lt;/script&gt;
以下是只读事务的具体流程&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;由于读操作涉及多个Paxos Group，所以要通过协商阶段决定这个值Scope。&lt;/li&gt;
    &lt;li&gt;如果Scope只被一个Paxos Group处理，那么客户端会把只读事务发给那个的Leader。我们把LastTS()定义为Paxos组里最后一个写操作提交的时间戳。如果没有prepare阶段的事务，那么让S = LastTS()就能满足要求。&lt;/li&gt;
    &lt;li&gt;如果Scope会被多个Paxos Group处理，Spanner当前的实现是一个简单的方案。客户端会让S = TT.now().latest，然后以S时间戳执行读操作。所有事务里的读操作都会被发送到数据足够新的副本。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;谷歌充分了利用了使用Bigtable的丰富经验，设计出了Spanner。Spanner的全球架构和内部架构虽然精巧，但和Amazon的Aurora比起来也没有过人的地方。但是，TrueTime的提出和其在事务中的使用实在令人惊叹。以前的工作，比如2PC的交互和Raft中的Index，又或者是现在流行的消息队列，几乎都是通过自增的主键或者是逻辑上的前置条件判断事务的先后。Spanner简单粗暴却优雅地使用了时间，这一永远自增不会重复的量解决了这个问题。同时使用时间还直接解决了全球范围内的数据同步这一问题，而这一问题直接使用逻辑上的先后是无法实现的。这应该是我在6.824这门课上读到的最强的也是最令人赞叹的系统了。&lt;/p&gt;</content><author><name>Trafalgar Ricardo Lu</name></author><category term="distributedsystem" /><summary type="html">在2012年的OSDI上，谷歌发表了《Spanner:Google’s Globally-Distributed Database》，其中介绍了谷歌第二代的数据库，也就是Bigtable的继任者——Spanner。在使用Bigtable的过程中，谷歌的开发人员逐渐意识到Bigtable的一些不足之处，比如不能处理变化的数据格式，不能保证大范围内数据库的一致性以及对跨行事务的处理。谷歌为了解决这些问题，开发出了Spanner。</summary></entry><entry><title type="html">总结Google的三驾马车</title><link href="http://localhost:4000/%E6%80%BB%E7%BB%93Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6" rel="alternate" type="text/html" title="总结Google的三驾马车" /><published>2020-08-13T19:00:00+08:00</published><updated>2020-08-13T19:00:00+08:00</updated><id>http://localhost:4000/%E6%80%BB%E7%BB%93Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6</id><content type="html" xml:base="http://localhost:4000/%E6%80%BB%E7%BB%93Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6">&lt;p&gt;前面的三篇文章，我们谈了Google的三驾马车，现在我们总体地对它们进行一个总结。&lt;/p&gt;

&lt;p&gt;要想对这三篇文章作总结，那么就必须要对这三篇文章的背景更加仔细地了解。文章开头的介绍是从总体的角度来看的，但是我最近看到一篇关于Jeff Dean的文章，从Jeff Dean的角度更加详细地介绍了当时的困局。我下面将引用一些该文章的中内容，并以此讨论三架马车的设计出发点。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;2000 年 3 月的一天，谷歌公司最顶尖的六位工程师齐聚某临时“作战指挥室”。当时的谷歌，正面临着前所未有的紧急状况。前一年 10 月，谷歌用于爬取 Web 以建立网络内容索引的核心系统宣告停止工作。虽然用户仍然可以通过 http://google.com 网站进行结果查询，但他们收到的结果实际上已经过期了五个月。由此引发的利益冲突远超工程师们的想象……就在当时，互联网的体量在一年之内增长了一倍。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;相当有趣的是，哪怕谷歌的爬虫已经崩溃了快半年，所有的查询结果都是六个月之前的，他们仍然敢和雅虎谈合作。也就是当时互联网内容更新不够快，要是放在今天，一个星期就够谷歌倒闭了。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;在作战室中度过的第五天，Jeff 与 Sanjay 开始怀疑其中的问题不是出在逻辑层面，而属于物理范畴。他们将混乱的索引文件转换为最原始的表示形式：二进制代码。更具体地讲，他们想了解机器到底看到了什么。&lt;/p&gt;

  &lt;p&gt;Sanjay 的显示器上出现了一系列粗体的 0 和 1，每一行代表一个索引词。Sanjay 发现，其中某个本应是 0 的数位却显示成了 1。接下来，Jeff 与 Sanjay 将所有错误排序的词语汇总了起来，并发现了其中的共通模式——每个词语都存在相同的问题。他们机器上的内存芯片发生了故障。&lt;/p&gt;

  &lt;p&gt;Sanjay 不由自主将目光投向 Jeff。几个月以来，谷歌一直在经历各种各样且持续增加的硬件故障。问题是，随着谷歌业务的快速发展，其计算基础设施也在不断扩大。计算机硬件发生故障的概率一般是很低的，因此问题堆积起来之后，一下子引发了破坏性的影响。电线磨损、磁盘损坏、主板过热。相当一部分设备根本无法一次性成功启动，也有一些莫名其妙地速度变慢了。&lt;/p&gt;

  &lt;p&gt;我们都会使用“搜索 Web”或者“搜索网络”的说法，但实际上这种表述并不准确。实际上，我们的搜索引擎遍历的是 Web 的索引——或者说地图。在 1996 年还在使用 BackRub 这个名号时，谷歌的索引地图还很小，足以安装在 Page 宿舍中的个人电脑里。但到 2000 年 3 月，其规模已经超出了任何超级计算机的处理能力。&lt;/p&gt;

  &lt;p&gt;要跟上如此迅猛的发展速度，谷歌公司唯一的方法就是购买消费级设备并将其组成一个集群。由于消费级设备当中有半数成本来自对谷歌公司毫无意义的“垃圾”——包括软盘驱动器与金属机箱，因此他们决定直接订购主板与磁盘并将其连接起来。谷歌在加利福尼亚州圣克拉拉市的一栋楼里将 1500 套这样的设备堆到了六英尺高； 但由于硬件故障，其中只有 1200 套设备能够正常工作。另外，随机发生的故障也在不断破坏着这套系统。为了维持业务运转，谷歌方面必须将这些计算机构建成一套无缝且具备弹性的整体。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从这里，我们一方面可以看出Jeff和Sanjay确实厉害，一般人最多查到逻辑层面，现在他们直接怀疑是硬件的问题。另一方面，这也说明了为什么GFS，Bigtable和MapReduce都要将容错（Fault-Tolerance）作为重要的考虑因素，并且都考虑最坏的情况，甚至在GFS中使用了多个备份的机器。&lt;/p&gt;

&lt;p&gt;这里必须要吹的是，GFS能充分发挥这么一堆“商业垃圾”的性能实在是强得可怕。哪怕是读过GFS的论文，对GFS有了初步的了解，我也很难想象它能在这么差劲的机器上撑起足够能用的存储体系，而且还能够保证数据的完整性。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;在 2003 年的四个月当中，Jeff 与 Sanjay 帮助谷歌完成了创立以来规模最大的一次升级。他们所使用的软件，就是后来赫赫有名的 MapReduce。这次升级的灵感，来自他们在对谷歌爬取工具与索引器进行第三次重写的过程。他们意识到，每一次他们解决一个重要问题，所面向的都是地理分布广阔且个别设备可能不太可靠的无数计算机上协同运行。因此，只有对解决方案进行全面推广，才能避免一次又一次重复面对同样的问题。更具体地讲，应该创建一款工具，确保谷歌公司的每一位程序员都能够利用其运行数据中心内的机器——换言之，将谷歌的所有基础设施视为一台硕大无朋的整体计算机。&lt;/p&gt;

  &lt;p&gt;MapReduce 的意义在于把可能令人费解的复杂流程整理得井然有序。在 MapReduce 出现之前，每一位程序员都必须弄清楚要如何对数据进行分割与分发，分配工作并自行负责硬件故障的处理。MapReduce 为编程人员提供了一种用于考量此类问题的结构化方法。正如厨师在食材下锅之前要先对其进行分类一样，MapReduce 也要求程序员将自己的任务分成两个阶段。首先，程序员需要告诉每台机器如何完成任务的“Map”阶段（比如计算某个词语在网页中出现的次数）； 接下来，程序员要编写指令以实现全部机器结果的“Reduce”（例如将上述结果相加）。MapReduce 可以处理分发工作的细节，从而帮助程序员摆脱这些复杂且枯燥的任务。&lt;/p&gt;

  &lt;p&gt;在接下来的一年当中，Jeff 与 Sanjay 以 MapReduce 任务的形式重写了谷歌的爬取与索引系统。很快，当其他工程师意识到 MapReduce 的强大力量后，他们也开始利用其处理视频并在谷歌地图上渲染图块。MapReduce 非常简单，能够轻松消化各类新型任务。谷歌的业务有着明确的所谓“昼夜使用曲线”——即白天的流量比晚间更多，而 MapReduce 任务则开始使用闲置部分的容量。正如生物会在梦中回顾白天的经历，现在谷歌也在利用同样的方式处理自己的数据。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从这里，我们就可以完全理解MapReduce的设计逻辑了。在这三篇文章发布的时候，谷歌几乎所有的重心都在搜索引擎上面，谷歌搜索引擎的核心就是PageRank算法，而URL之间的引用次数是PageRank重要的一部分。MapReduce要做的就是计算网页直接的引用次数，所以他们简单地将爬取的数据作为输入文件交给Map处理，当MapReduce都得到结果后，他们就能得到相关网页的引用次数，然后将其作为参数传入到PageRank中就能得到想要的答案。&lt;/p&gt;

&lt;p&gt;另外，由于夜间的流量较低，所以他们可以在晚上运行MapReduce程序而不影响用户体验。这也就解释了为什么谷歌能够容忍那么长的计算时间。&lt;/p&gt;

&lt;p&gt;简单地说，MapReduce之所以简单是因为它负责的计算就很简单，但是计算量很大，所以使用分布式的架构。MapReduce慢是因为谷歌根本不需要它快。所以MapReduce可能仅仅适用于谷歌和一些业务场景符合的公司。&lt;/p&gt;

&lt;p&gt;这篇文件中没有提到Bigtable，而且看起来MapReduce可以调用GFS而无需访问Bigtable，那么为什么Bigtable仍然成为人们津津乐道的技术，甚至在后来的Spanner中谷歌也使用了相当一部分的设计呢？那就是，数据库在业务逻辑中是无可替代的。&lt;/p&gt;

&lt;p&gt;数据库提供的不仅仅是数据的存储，更重要的是数据的读取。这里的存储和读取，不仅仅是完整的一致的，还要求是快速的。假如我们直接使用GFS，那么我们就没有完整的数据视图，需要像查文件一样查找数据，不仅效率低下，而且指定文件路径在编程上也是十分死板的做法。另一方面，GFS在数据写入方面也完全没有优化的处理。&lt;/p&gt;

&lt;p&gt;而在Bigtable中，由于Bigtable保证数据有序，那么读取和查找数据的效率会指数级别地提高。在写的方面，它使用WAL保证容错，使用Memtable保证高效，是GFS所不能比的。另一方面，它使用Column Family实现了类似于事务的功能，能够实现ACID的要求，这也是业务所强烈要求的。&lt;/p&gt;

&lt;p&gt;而且我相信，根据Bigtable的数据模型来看，爬虫爬下来的网页数据应该不是直接存入GFS，而是先存入Bigtable，然后通过Bigtable保存到GFS上的。因为Bigtable的数据模型怎么看都像是为了存储网页直接相互引用的信息设计的。而且，由于MapReduce产生的数据有可能在未来也被使用，这些数据被存入GFS。&lt;/p&gt;

&lt;p&gt;那么我们最后来总结一下三者的调用关系。谷歌使用爬虫爬取网页信息，并且把这些信息存入到Bigtable中，Bigtable会将这些数据排序并保存到GFS中。MapReduce为了得到PageRank的参数，会调用Bigtable中的数据，计算各网页的引用次数。计算完成后，MapReduce会将这些数据保存到GFS中以待下次使用。&lt;/p&gt;

&lt;p&gt;如果要对这三驾马车作出一个评价的话，那么应该这么说：在谷歌危急的时候，这三驾马车不仅力挽狂澜，而且为谷歌和其他互联网企业的发展指明了未来，是分布式系统的重要基石。&lt;/p&gt;</content><author><name>Trafalgar Ricardo Lu</name></author><category term="distributedsystem" /><summary type="html">前面的三篇文章，我们谈了Google的三驾马车，现在我们总体地对它们进行一个总结。</summary></entry><entry><title type="html">浅谈Google的三驾马车之MapReduce</title><link href="http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BMapReduce" rel="alternate" type="text/html" title="浅谈Google的三驾马车之MapReduce" /><published>2020-08-13T18:00:00+08:00</published><updated>2020-08-13T18:00:00+08:00</updated><id>http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BMapReduce</id><content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BMapReduce">&lt;p&gt;上图为世界地图（World Map），实在找不到Reduce了。&lt;/p&gt;

&lt;p&gt;谷歌在2003到2006年间发表了三篇论文，《MapReduce: Simplified Data Processing on Large Clusters》，《Bigtable: A Distributed Storage System for Structured Data》和《The Google File System》介绍了Google如何对大规模数据进行存储和分析。这三篇论文开启了工业界的大数据时代，被称为Google的三驾马车。本文介绍MapReduce的相关内容。&lt;/p&gt;

&lt;h2 id=&quot;背景介绍&quot;&gt;背景介绍&lt;/h2&gt;

&lt;p&gt;在21世纪初，互联网上的内容，大多数企业需要存储的数据量并不大。但是Google不同，Google的搜索引擎的数据基于爬虫，而由于网页的大量增加，爬虫得到的数据也随之急速膨胀，单机或简单的分布式方案已经不能满足业务的需求，所以Google必须设计新的数据存储系统，其产物就是Google File System（GFS）。不过，在Google的设计中，为了尽可能的解耦，GFS仅负责数据存储而不提供类似数据库的服务。也就是说，GFS只存数据，而对数据的具体内容一无所知，自然也就不能提供基于内容的检索功能。所以，更进一步，Google开发了Bigtable作为数据库，向上层服务提供基于内容的各种功能。此外，Google 的搜索结果依赖于PageRank算法的排序，而该算法又需要一些额外的数据，比如某网页的被引用次数，所以他们还开发了对于的数据处理工具MapReduce，在读取了Bigtable数据的技术上，根据业务需求，对数据内容进行运算。其总体架构如下，GFS能充分利用多个Linux服务器的磁盘，并向上掩盖分布式系统的细节。Bigtable在GFS的基础上对数据内容进行识别和存储，向上提供类似数据库的各种操作。MapReduce则使用Bigtable中的数据进行运算，再提供给具体的业务使用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Google Troika.png&quot; alt=&quot;Troika&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;mapreduce&quot;&gt;MapReduce&lt;/h2&gt;

&lt;p&gt;MapReduce本来是函数式编程中的两个函数，在尝试解决利用大数据进行计算时，Jeff Dean和Sanjay Ghemawat想到了使用这种思想简化计算模型。&lt;/p&gt;

&lt;h3 id=&quot;基本思想&quot;&gt;基本思想&lt;/h3&gt;

&lt;p&gt;MapReduce把所有的计算都拆分成两个基本的计算操作，即Map和Reduce。其中Map函数以一系列键值对作为输入，然后输出一个中间文件（Intermediate）。这个中间态是另一种形式的键值对。然后，Reduce函数将这个中间态作为输入，计算得出结果。其中，Map函数和Reduce函数的逻辑都是由开发人员自行定义的。一种经典的逻辑如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/MapReduce Model.png&quot; alt=&quot;MapReduce Model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以WordCount为例，准备要统计一本书中所有单词出现的次数。在Map函数中，我们每遇到一个单词W，就往中间文件中写入（W，1）。然后，在Reduce函数中，把所有（W，1）出现的次数相加，就能得到W的出现次数V。&lt;/p&gt;

&lt;h3 id=&quot;分布式mapreduce流程&quot;&gt;分布式MapReduce流程&lt;/h3&gt;

&lt;p&gt;上面提到的模型和思想都是单机的，想要在分布式系统中实现，还需要一些改动。在MapReduce中，他们选择将大任务拆分成小任务分配给多台机器，以此充分利用分布式系统的性能。下图是论文中展示的MapReduce的流程图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/MapReduce Overview.png&quot; alt=&quot;MapReduce Overview&quot; /&gt;&lt;/p&gt;

&lt;p&gt;具体的流程如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;MapReduce客户端会将输入的文件会分为M个片段，每个片段的大小通常在 16~64 MB 之间。然后在多个机器上开始运行MapReduce程序。&lt;/li&gt;
    &lt;li&gt;系统中会有一个机器被选为Master节点，整个 MapReduce 计算包含M个Map 任务和R个 Reduce 任务。Master节点会为空闲的 Worker节点分配Map任务和 Reduce 任务&lt;/li&gt;
    &lt;li&gt;执行Map任务的 Worker开始读入自己对应的片段并将读入的数据解析为输入键值对。然后调用由用户定义的 Map任务。最后，Worker会将Map任务输出的结果存在内存中。&lt;/li&gt;
    &lt;li&gt;在执行Map的同时，Map Worker根据Partition 函数将产生的中间结果分为R个部分，然后定期将内存中的中间文件存入到自己的本地磁盘中。任务完成时，Mapper 便会将中间文件在其本地磁盘上的存放位置报告给 Master。&lt;/li&gt;
    &lt;li&gt;Master会将中间文件存放位置通知给Reduce Work。Reduce Worker接收到这些信息后便会通过RPC读取中间文件。在读取完毕后，Reduce Worker会对读取到的数据进行排序，保证拥有相同键的键值对能够连续分布。&lt;/li&gt;
    &lt;li&gt;最后，Reduce Worker会为每个键收集与其关联的值的集合，并调用用户定义的Reduce 函数。Reduce 函数的结果会被放入到对应的结果文件。&lt;/li&gt;
    &lt;li&gt;当所有Map和Reduce都结束后，程序会换新客户端并返回结果。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;整个流程非常清晰。首先，将输入文件分割成M个个片段，然后每个Map Worker读取对应的片段并执行Map函数，将结果存入中间文件。Reduce Work则通过Master得知中间文件的位置，然后读取其对应中间文件的内容并运行Reduce函数，最后把结果输出到结果文件中。&lt;/p&gt;

&lt;p&gt;这里值得说明的是，无论是输入文件到Map Worker的映射还是中间文件到Reduce Worker的映射都可以通过自定义的哈希函数来确定，论文中默认使用&lt;strong&gt;Hash(key) mod R&lt;/strong&gt;来确定。另外，M和R的值都是由用户指定的，应当比实际的机器数量要多一些，以此实现均衡负载。&lt;/p&gt;

&lt;h3 id=&quot;fault-tolerance&quot;&gt;Fault-Tolerance&lt;/h3&gt;

&lt;p&gt;因为使用了分布式系统，所以不可避免地要考虑容错的问题，在MapReduce中，容错也考虑Master和Work两种情况。&lt;/p&gt;

&lt;p&gt;Master节点会定期地将当前运行状态存为快照，当Master节点崩溃，就从最近的快照恢复然后重新执行任务。&lt;/p&gt;

&lt;p&gt;Master节点会定期地Ping每个Work节点，一旦发现Work节点不可达，针对其当前执行的是Map还是Reduce任务，会有不同的策略。&lt;/p&gt;

&lt;p&gt;如果是Map任务，无论任务已完成或是未完成，都会废除当前节点的任务。。之后，Master会将任务重新分配给其他节点，同时由于已经生成的中间文件不可访问，还会通知还未拿到中间文件的Reduce Worker去新的节点拿数据。&lt;/p&gt;

&lt;p&gt;如果是Reduce任务，由于结果文件存在GFS中，文件的可用性和一致性由GFS保证，所以Master仅将未完成的任务重新分配。&lt;/p&gt;

&lt;h3 id=&quot;优化&quot;&gt;优化&lt;/h3&gt;

&lt;p&gt;如果集群中有某个 Worker 花了特别长的时间来完成最后的几个 Map 或 Reduce 任务，整个 MapReduce 计算任务的耗时就会因此被拖长，这样的 Worker 也就成了落后者。MapReduce 在整个计算完成到一定程度时就会将剩余的任务即同时将其分配给其他空闲 Worker 来执行，并在其中一个 Worker 完成后将该任务视作已完成。&lt;/p&gt;

&lt;p&gt;这里论文中还提出了其他一些策略，但是我认为不是十分重要也就不再提及。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;MapReduce是一个相当简单的计算模型，它尝试将所有的计算任务都拆分成基础的Map和Reduce，以此降低实现的复杂度。但是，这恰恰提高了编程逻辑的复杂度。我看过使用MapReduce实现Join功能的代码，十分地巧妙灵活。但是看似巧妙的背后，是模型过于简单而导致复杂度转移到了代码逻辑的层面。&lt;/p&gt;

&lt;p&gt;另一方面，MapReduce的程序类似于批处理程序，需要完整的输入程序才能开始运算，而且每次运算都要至少写入两次磁盘。这就导致每次运算都要等待很长的时间，完全不能实现需要快速响应的业务场景的需求。&lt;/p&gt;

&lt;p&gt;以上两个方面，一个引出了支持类SQL的计算工具，另一个引出了支持流式计算的工具，而这两个特性正是今天流行的计算工具的热点。&lt;/p&gt;

&lt;p&gt;总得来说，虽然MapReduce在今天几乎抛弃了，但是在当初那个年代以及谷歌的业务需求看来，是相当合适的。&lt;/p&gt;</content><author><name>Trafalgar Ricardo Lu</name></author><category term="distributedsystem" /><summary type="html">谷歌在2003到2006年间发表了三篇论文，《MapReduce:Simplified Data Processing on Large Clusters》，《Bigtable:A Distributed Storage System for Structured Data》和《The Google File System》介绍了Google如何对大规模数据进行存储和分析。这三篇论文开启了工业界的大数据时代，被称为Google的三驾马车。本文介绍MapReduce的相关内容。</summary></entry><entry><title type="html">浅谈Google的三驾马车之Bigtable</title><link href="http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BBigtable" rel="alternate" type="text/html" title="浅谈Google的三驾马车之Bigtable" /><published>2020-08-10T18:00:00+08:00</published><updated>2020-08-10T18:00:00+08:00</updated><id>http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BBigtable</id><content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BBigtable">&lt;p&gt;上图为真·大桌子&lt;/p&gt;

&lt;p&gt;谷歌在2003到2006年间发表了三篇论文，《MapReduce: Simplified Data Processing on Large Clusters》，《Bigtable: A Distributed Storage System for Structured Data》和《The Google File System》介绍了Google如何对大规模数据进行存储和分析。这三篇论文开启了工业界的大数据时代，被称为Google的三驾马车。本文介绍Bigtable的相关内容。&lt;/p&gt;

&lt;h2 id=&quot;背景介绍&quot;&gt;背景介绍&lt;/h2&gt;

&lt;p&gt;在21世纪初，互联网上的内容，大多数企业需要存储的数据量并不大。但是Google不同，Google的搜索引擎的数据基于爬虫，而由于网页的大量增加，爬虫得到的数据也随之急速膨胀，单机或简单的分布式方案已经不能满足业务的需求，所以Google必须设计新的数据存储系统，其产物就是Google File System（GFS）。不过，在Google的设计中，为了尽可能的解耦，GFS仅负责数据存储而不提供类似数据库的服务。也就是说，GFS只存数据，而对数据的具体内容一无所知，自然也就不能提供基于内容的检索功能。所以，更进一步，Google开发了Bigtable作为数据库，向上层服务提供基于内容的各种功能。此外，Google 的搜索结果依赖于PageRank算法的排序，而该算法又需要一些额外的数据，比如某网页的被引用次数，所以他们还开发了对于的数据处理工具MapReduce，在读取了Bigtable数据的技术上，根据业务需求，对数据内容进行运算。其总体架构如下，GFS能充分利用多个Linux服务器的磁盘，并向上掩盖分布式系统的细节。Bigtable在GFS的基础上对数据内容进行识别和存储，向上提供类似数据库的各种操作。MapReduce则使用Bigtable中的数据进行运算，再提供给具体的业务使用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Google troika.png&quot; alt=&quot;Troika&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;bigtable&quot;&gt;Bigtable&lt;/h2&gt;

&lt;p&gt;Bigtable实现在Google File System的基础上，它关心数据的内容，根据的数据的内容建立数据模型，对外提供读写数据的接口。&lt;/p&gt;

&lt;h3 id=&quot;数据模型&quot;&gt;数据模型&lt;/h3&gt;

&lt;p&gt;Bigtable基本的数据结构和关系型数据库类似，都是以行列构成的表，但是，它还另外增加了新的维度——时间。也就是说，在行列确定的情况下，一个单元格（Cell）中有多个以事件为版本的数据。Bigtable用(row:string, column:string, time:int64) → string表示映射关系。下图为论文中给出的一个例子。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Bigtable example.png&quot; alt=&quot;Bigtable example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果想要在表中查询指定版本的内容，我们需要指出行，列，及版本。比如（“com.cnn.www”，“anchor：cnnsi.com”，t9）→ “CNN”。我个人猜想，增加时间这个维度是因为“三驾马车”被设计出来的时候主要是为了支持搜索引擎，搜索引擎可能需要保留多个时间段的网页数据，而GFS也使用追加（Append）作为数据的主要修改方式，所以增加时间戳作为版本既充分利用了GFS的特性，也能满足业务的需求。&lt;/p&gt;

&lt;p&gt;另外，Bigtable还把多个Column Keys并入到被称为Column Family的集合中，并将Column Family作为访问控制的基础单元。我认为，这种方案其实是一种事务（Transaction）的实现方案。传统的事务以行为基本操作单位，在读写时对行上锁以实现隔离，而Bigtable则是以Column Family为单位，这里的访问控制其实就是锁的思想。&lt;/p&gt;

&lt;h3 id=&quot;相关组件&quot;&gt;相关组件&lt;/h3&gt;

&lt;p&gt;在介绍系统的整体架构之前，我们要对Bigtable用到的两个重要组件有一些了解。由于Bigtable是分布式的数据库，在节点之间的协调上需要额外的处理，这里，Bigtable使用了Google内部的Chubby。我们可以把Chubby看做是Zookeeper，因为Zookeeper本质也就是Chubby的开源版本。另一方面，为了加快数据的查找和存储效率，Bigtable在存储数据之前都进行了排序，而此处用到的存储文件文论称之为SSTable（Sorted String Table）。&lt;/p&gt;

&lt;p&gt;在Bigtable中，由于单个表（Table）存储的数据可能相当地多，那么读写的效率就会十分低下，于是Bigtable将Table分割为固定大小的Tablet，将其作为数据存储和查找的基本单位。每当Table增加了这里要说明的是，tablet是数据存储的基本单元，是用户感知不到的。而Column Family则是访问的基本单元，是编程时指定的，两者一前一后，不是一个概念。&lt;/p&gt;

&lt;h3 id=&quot;tablet-定位&quot;&gt;Tablet 定位&lt;/h3&gt;

&lt;p&gt;因为是在分布式系统中，那么每个Tablet所在的机器不同，需要记录相关信息（METADATA）对其进行管理。而存储这些METADATA又需要分布式的系统，所以Bigtable又将这些METADATA的METADATA记录在一个文件中，并将这个文件的位置保存在Chubby中。总结一下，Bigtable有以下三层结构：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;在Chubby中保存着Root Tablet的位置&lt;/li&gt;
    &lt;li&gt;Root Tablet中保存着METADATA Table中所有 Tablet 的位置&lt;/li&gt;
    &lt;li&gt;METADATA Table中保存着所有存储数据的Tablet的位置&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Tablet Location.png&quot; alt=&quot;Tablet Location&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这其中有几点值得注意。由于Root Tablet的特殊性，哪怕它的数据量再大，它也不允许被分割。METADATA tables被读取到内存中以加快速度，其中存储的是以开始和结尾的Row Key作为键，tablet位置作为值的映射。&lt;/p&gt;

&lt;p&gt;如果客户端希望读取特定的数据，那么它会以此读取Chubby中的文件，Root Tablet，METADATA Tablet，最后读取存储改数据的Tablet。同时，为了加快读取的速度，它会将这些信息缓存到本地，直到信息失效。&lt;/p&gt;

&lt;h3 id=&quot;tablet分配&quot;&gt;Tablet分配&lt;/h3&gt;

&lt;p&gt;在谈Table分配之前，论文先讨论了怎么处理成员变更的问题。类似于GFS，Bigtable使用Master节点来管理这些相关的事情。&lt;/p&gt;

&lt;p&gt;首先，Bigtable使用Chubby来检测Tablet Server的变化。这里的操作和Zookeeper的用法类似，当有新节点加入时，它需要在Chubby中新建一个对应的文件，并获取该文件的锁。由于所有的节点在Chubby中都有对应的文件，那么Master可以通过监听Chubby来获取所有Tablet Server的信息。这里有两种节点失效的情况，一种是仅仅回收了锁但是文件还在，这种情况很可能是节点崩溃了。由于节点不能自己退出，所以在Master节点得到该文件的锁后，它会将文件删除，以此表示节点退出。另一种情况是，文件已经被删除，这种情况说明节点是主动退出系统，那么可以直接重新分配Tablet给其他节点即可。&lt;/p&gt;

&lt;p&gt;在正常的情况下，系统中会有大量数据写入，Master需要负责将这些数据分配到合适的Tablet Server。Bigtable并没有明确指出分配所使用的的算法，但是它提出了一个要求。为了保证数据的一致性，同一时间，一个 Tablet只能被分配给一个Tablet Server。Master通过向 Tablet Server 发送载入请求来分配 Tablet。如果该载入请求被Tablet Server接收到前Master仍是有效的，那么就可以认为此次 Tablet 分配操作已成功。&lt;/p&gt;

&lt;p&gt;在这里，我们还要考虑Master崩溃的情况，论文中描述了Master恢复的步骤如下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;在 Chubby 上获取 Master 独有的锁，确保不会有另一个 Master 同时启动&lt;/li&gt;
    &lt;li&gt;从 Chubby 了解在工作的 Tablet Server&lt;/li&gt;
    &lt;li&gt;从各个 Tablet Server 处获取其所负责的 Tablet 列表，并向其表明自己作为新 Master 的身份，确保 Tablet Server 的后续通信能发往这个新 Master&lt;/li&gt;
    &lt;li&gt;Master 确保 Root Tablet 及 &lt;code class=&quot;highlighter-rouge&quot;&gt;METADATA&lt;/code&gt; 表的 Tablet 已完成分配&lt;/li&gt;
    &lt;li&gt;Master 扫描 &lt;code class=&quot;highlighter-rouge&quot;&gt;METADATA&lt;/code&gt; 表获取集群中的所有 Tablet，并对未分配的 Tablet 重新进行分配&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;其中，第四步是为了第五步的正确执行。&lt;/p&gt;

&lt;h3 id=&quot;读写tablet&quot;&gt;读写Tablet&lt;/h3&gt;

&lt;p&gt;上面我们谈了Bigtable的数据模型，如何寻找和分配Tablet，那么数据是怎么以（row，column，time）的格式被组织成Tablet的呢？论文中给出的流程图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Tablet Operation.png&quot; alt=&quot;Tablet Operation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;每个Tablet由若干个位于 GFS 上的 SSTable、一个位于内存内的MemTable以及一份Tablet Log组成。&lt;/p&gt;

&lt;p&gt;我们来解释一下这张图。为了保证系统可恢复，Google首先使用Table Log（即WAL）将客户端发出的写操作请求记录在磁盘中，那么，一旦系统崩溃，仍然可以从磁盘读取数据，继续执行命令。然后，相关的数据被放入位于内存中的Memtable中，因为内存的速度相当快，那么执行排序等操作就要快得多。当Memtable的大小达到设定的值后，它就会以SSTable的形式被存储到GFS中，这被称为Minor Compaction。&lt;/p&gt;

&lt;p&gt;客户端的读操作请求则要综合考虑Memtable和SSTable中的数据，如果Memtable中已经有需要读的数据，就无需读取SSTable。由于Memtable和SSTable都是有序的，所以读取的速度都相当快。&lt;/p&gt;

&lt;p&gt;在这里，虽然论文没有明确指出，我认为Memtable和SSTable的大小很可能是64MB。因为GFS将单个Chunk设置为64MB，那么为了最大化地利用磁盘空间，Memtable和SSTable的大小设置为这个值是相当合理的。&lt;/p&gt;

&lt;p&gt;由于SSTable中的数据有可能被标记为删除，那么我们需要定期对其进行处理，Bigtable将其称为Major Compaction。在这个过程中，Bigtable会将过期或者被删除的数据删除，并合并多个SSTable。这里似乎和GFS的Garbage Collection有点类似，但是我认为这可能是两个层面的活动。Bigtable清理的是单个Chunk中的数据，而GFS清理的是磁盘中的单个Chunk。&lt;/p&gt;

&lt;h3 id=&quot;优化&quot;&gt;优化&lt;/h3&gt;

&lt;p&gt;论文中提到，仅靠上述这些方法还不能达到要求的速度，因此，Bigtable还做了一些优化。&lt;/p&gt;

&lt;p&gt;第一，为了提高读取的速度，Bigtable使用布隆过滤器判断数据是否在某个SSTable中。&lt;/p&gt;

&lt;p&gt;第二，Tablet Server使用Scan Cache缓存SSTable返回的数据，在重复读时提高效率。使用Block Cache缓存从GFS读取的SSTable，这样在读取附近的数据时就无需从磁盘读取。&lt;/p&gt;

&lt;p&gt;第三，Bigtable把所有的写入操作都写入到同一个Bigtable Log文件中，而不是每个Server分配一个。同时，因为这个文件相当大，恢复起来很费事。Bigtable会对其进行排序并进行切分，每个Tablet Sever只需读取自己的那部分就可以了。&lt;/p&gt;

&lt;p&gt;第四，Bigtable允许针对特定的Column Family生成SSTable，同时进行压缩，以提高读取的效率。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;Bigtable重要的贡献是证明了在分布式的系统中，针对超大规模的数据量，使用排序大表的来设计数据库是可行的。这直接带动了LSM Tree的流行，在后来的HBase，LevelDB中都使用了这种方式处理数据。另外，Bigtable系统中Chubby的使用，还告诉工业界分布式协调组件的重要性，这也引导了Zookeeper的设计实现，而其仍然是今天的分布式系统中重要的组件。&lt;/p&gt;</content><author><name>Trafalgar Ricardo Lu</name></author><category term="distributedsystem" /><summary type="html">谷歌在2003到2006年间发表了三篇论文，《MapReduce:Simplified Data Processing on Large Clusters》，《Bigtable:A Distributed Storage System for Structured Data》和《The Google File System》介绍了Google如何对大规模数据进行存储和分析。这三篇论文开启了工业界的大数据时代，被称为Google的三驾马车。本文介绍Bigtable的相关内容。</summary></entry><entry><title type="html">浅谈Google的三驾马车之GFS</title><link href="http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BGFS" rel="alternate" type="text/html" title="浅谈Google的三驾马车之GFS" /><published>2020-08-08T18:00:00+08:00</published><updated>2020-08-08T18:00:00+08:00</updated><id>http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BGFS</id><content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Google%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E4%B9%8BGFS">&lt;p&gt;上图为古天乐——真·高富帅（GFS）&lt;/p&gt;

&lt;p&gt;谷歌在2003到2006年间发表了三篇论文，《MapReduce: Simplified Data Processing on Large Clusters》，《Bigtable: A Distributed Storage System for Structured Data》和《The Google File System》介绍了Google如何对大规模数据进行存储和分析。这三篇论文开启了工业界的大数据时代，被称为Google的三驾马车。本文介绍Google File System的相关内容。&lt;/p&gt;

&lt;h2 id=&quot;背景介绍&quot;&gt;背景介绍&lt;/h2&gt;

&lt;p&gt;在21世纪初，互联网上的内容，大多数企业需要存储的数据量并不大。但是Google不同，Google的搜索引擎的数据基于爬虫，而由于网页的大量增加，爬虫得到的数据也随之急速膨胀，单机或简单的分布式方案已经不能满足业务的需求，所以Google必须设计新的数据存储系统，其产物就是Google File System（GFS）。不过，在Google的设计中，为了尽可能的解耦，GFS仅负责数据存储而不提供类似数据库的服务。也就是说，GFS只存数据，而对数据的具体内容一无所知，自然也就不能提供基于内容的检索功能。所以，更进一步，Google开发了Bigtable作为数据库，向上层服务提供基于内容的各种功能。此外，Google 的搜索结果依赖于PageRank算法的排序，而该算法又需要一些额外的数据，比如某网页的被引用次数，所以他们还开发了对于的数据处理工具MapReduce，在读取了Bigtable数据的技术上，根据业务需求，对数据内容进行运算。其总体架构如下，GFS能充分利用多个Linux服务器的磁盘，并向上掩盖分布式系统的细节。Bigtable在GFS的基础上对数据内容进行识别和存储，向上提供类似数据库的各种操作。MapReduce则使用Bigtable中的数据进行运算，再提供给具体的业务使用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Google troika.png&quot; alt=&quot;Troika&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;google-file-system&quot;&gt;Google File System&lt;/h2&gt;

&lt;p&gt;GFS是三驾马车中最底层的组件，当然也是最复杂的，因为他直接和分布式的系统接触。在具体探讨实现细节之前，Google给出了一些设计前提和设计目标。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;由于使用的机器是廉价的商业化机器，那么机器崩溃被认为是一种常态。&lt;/li&gt;
    &lt;li&gt;系统存储以大文件为主，但也支持小文件。文件大小通常在100MB左右并且需要高效的操作几个GB的文件。&lt;/li&gt;
    &lt;li&gt;系统需要支持大规模的连续读取和小规模的随机读取，以及大规模的追加写。&lt;/li&gt;
    &lt;li&gt;高性能稳定的网络带宽比延迟更重要。&lt;/li&gt;
    &lt;li&gt;以及最重要的，能在分布式的系统上运行。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;在下面，我们根据具体的措施讨论如何实现以上目标。&lt;/p&gt;

&lt;h3 id=&quot;总体架构&quot;&gt;总体架构&lt;/h3&gt;

&lt;p&gt;首先我们来看看GFS的总体架构。在这个架构中，GFS采用了单Master（Single Master）的设计来简化系统的复杂度。Master负责两点，一是存储和维护Chunk Server和数据块的相关信息，二是处理客户端的请求。也就是说，Master并不存储任何具体的数据，这些数据被存在被称为Chunk Server的数据节点上。其中Chunk就是指数据块，在GFS中被固定为64MB大小，我想着可能跟第二个目标相关。每当数据需要被写入时，就更新GFS中的信息，并把数据封装成Chunk写入到数据库中，具体流程在下面会仔细介绍。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/GFS Architecture.png&quot; alt=&quot;GFS Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图演示了应用如何调用GSF进行读操作的具体流程&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;应用调用GFS Client的函数，要求其读取具体的文件/foo/bar，假设大小为50MB。&lt;/li&gt;
    &lt;li&gt;GFS Client根据Chunk的固定大小计算出/foo/bar的Chunk的Index，即64/50向上取整，Chunk Index=1。以及其在Chunk内的偏移量Byte Range[0，50]，并将File Name和Chunk Index作为参数发送给GFS Master&lt;/li&gt;
    &lt;li&gt;Master返回了对应的Chunk Handle（也就是Chunk的ID，上图中的2ef0）和Chunk Locations（Chunk Server和其副本的IP）&lt;/li&gt;
    &lt;li&gt;GFS Client根据返回的Chunk Locations找到最近的Chunk Server，然后根据Chunk Handle找到对应的Chunk，最后按照这个文件在Chunk中偏移量Byte Range读取文件。&lt;/li&gt;
    &lt;li&gt;Chunk Server按照其要求返回文件数据。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;整个的流程相当清晰，客户端负责将文件在File Namespace的位置交给Master，Master根据其位置返回对应的Chunk和Chunk Server，然后客户端再根据这些信息去拿数据。但是，这里有很巧妙的设计，就是GFS将所有数据传输的压力都放到Chunk Server上，而不是Master。假设由Master根据Chunk Handle去找数据并返回给客户端，那么这里就有会造成系统带宽压力增大。如果按照假设设计，Chunk Server将数据传回给Master后，Master还要将数据传回给客户端，也就是64MB的流量陡然翻倍成128MB。而实际的GFS不仅降低了Master带宽的压力，还把读取数据的压力均摊到每个Chunk Server上，降低了整个系统的压力。这些设计明显有助于实现目标三。&lt;/p&gt;

&lt;p&gt;通过对读取过程的分析，可以发现，GFS已经完全实现目标二中的大规模连续读和小规模随机读的要求。&lt;/p&gt;

&lt;h3 id=&quot;single-master&quot;&gt;Single Master&lt;/h3&gt;

&lt;p&gt;Master中保存三种信息（MATEDATA）&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;文件和chunk的namespace，即文件树形式的命名方式。&lt;/li&gt;
    &lt;li&gt;文件到chunk的映射，即每个文件需要哪几个Chunk来记录。&lt;/li&gt;
    &lt;li&gt;每一个chunk的具体位置。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;这些信息都平时都存在内存中，以此提高响应速度。这似乎只能存储很少的信息，但是，Master只需要64byte的空间就能记录64MB的Chunk的相关信息，也就是说，128MB的内存能存储1PB的数据的相关信息。不过，为了保证Master能在崩溃后恢复，在执行改变前两种信息的操作前需要使用WAL的形式定期地保存到磁盘上。这样，在Master恢复的时候，就能通过磁盘上的WAL来重建前两种信息。而Chunk的位置则可以通过Heartbeats的形式查询并更新，这样不仅加速了Master的恢复，也能使GFS的配置更加灵活。&lt;/p&gt;

&lt;h3 id=&quot;写数据write&quot;&gt;写数据（Write）&lt;/h3&gt;

&lt;p&gt;由于每个Chunk Server都有备份的副本，所以写操作要比读操作稍微复杂一点。具体的流程如下图所示&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/GFS Write.png&quot; alt=&quot;GFS Write&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;客户端向 Master 询问目前哪个Chunk Server持有该Chunk的Lease&lt;/li&gt;
    &lt;li&gt;Master 向客户端返回Primary Replica和其他Replica的位置&lt;/li&gt;
    &lt;li&gt;客户端将数据推送到所有的Replica上。Chunk Server会把这些数据保存在缓冲区中，等待所有 Replica 都接收到数据。&lt;/li&gt;
    &lt;li&gt;客户端发送写请求给 Primary，Primary 为来自各个客户端的修改操作选定执行序列号，并按顺序地应用于其本地存储的数据。&lt;/li&gt;
    &lt;li&gt;Primary 将写请求转发给其他 Secondary Replica，Replica 们按照相同的顺序应用这些修改&lt;/li&gt;
    &lt;li&gt;Secondary Replica 响应 Primary，示意自己已经完成操作。&lt;/li&gt;
    &lt;li&gt;Primary 响应客户端，并返回该过程中发生的错误&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里要说明的是，每个Chunk被复制到多个Chunk Server中，以此避免单个节点崩溃可能造成的数据损失。而这些Chunk Server中负责相应写操作的Chunk Server被称为Primary Replica，其余的被称为Secondary Replica，接受来自Primary Replica的请求。而为了保证写入数据的一致性，只能有一个Primary Replica，这里的唯一性就由Lease来实现。当需要写入数据时，Master会将特定的Lease分配给某个Replica，拿到Lease的这个Replica就称为了Primary Replica。&lt;/p&gt;

&lt;p&gt;我们再仔细分析整个写的的流程。我们可以注意到为了保证数据的一致性，一次写入只能有一个Primary Replica，同样地，在正式写入数据之前要求所有Secondary Replica缓存数据也是为了一致性。这里的思想类似于WAL，将要执行的操作先保存下来，这样万一崩溃了也能从磁盘读入数据，继续执行未完成的操作。等待所有Secondary Replica完成操作后再相应客户端也是为了保证数据的一致性。&lt;/p&gt;

&lt;p&gt;和读操作类似，在写操作中，为了降低Master的压力，所有的数据由客户端发向Chunk Server。&lt;/p&gt;

&lt;h3 id=&quot;追加append&quot;&gt;追加（Append）&lt;/h3&gt;

&lt;p&gt;为了提升性能，GFS提供并推荐使用追加操作修改文件。它与写操作不同之处仅仅在于它向文件尾端添加数据而不是覆盖，它的流程也he 写操作大同小异&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;客户端将数据推送到每个 Replica，然后将请求发往 Primary&lt;/li&gt;
    &lt;li&gt;Primary 首先判断将数据追加到该块后是否会令块的大小超过上限：如果是，那么 Primary 会为该块写入填充至其大小达到上限，并通知其他 Replica 执行相同的操作，再响应客户端，通知其应在下一个块上重试该操作&lt;/li&gt;
    &lt;li&gt;如果数据能够被放入到当前块中，那么 Primary 会把数据追加到自己的 Replica 中，拿到追加成功返回的偏移值，然后通知其他 Replica 将数据写入到该偏移位置中&lt;/li&gt;
    &lt;li&gt;最后 Primary 再响应客户端&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;特别值得一提的是，GFS保证追加操作至少被执行一次（at least once），这意味着追加操作可能被执行多次。当追加操作失败时，为了保证偏移量，GFS会在对应的位置填充重复的数据，然后重试追加。也就是说，GFS不保证在每个副本中的数据完全一致，而仅仅保证数据被写入了。&lt;/p&gt;

&lt;h3 id=&quot;数据一致性&quot;&gt;数据一致性&lt;/h3&gt;

&lt;p&gt;在GFS中，由于分布式系统的原因，不同节点间处理请求和存储数据速度不一致导致了客户算读取数据时可能出现各种不同的情况。&lt;/p&gt;

&lt;p&gt;文件的数据修改则相对复杂。在讲述接下来的内容前，首先我们先明确，在文件的某一部分被修改后，它可能进入以下三种状态的其中之一：&lt;/p&gt;

&lt;p&gt;在文件的某一部分被修改后，它可能进入以下三种状态的其中之一：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;客户端读取不同的 Replica 时可能会读取到不同的内容，那这部分文件是不一致的（Inconsistent）。&lt;/li&gt;
    &lt;li&gt;所有客户端无论读取哪个 Replica 都会读取到相同的内容，那这部分文件就是一致的（Consistent）。&lt;/li&gt;
    &lt;li&gt;所有客户端都能看到上一次修改的所有完整内容，且这部分文件是一致的，那么我们说这部分文件是确定的（Defined）。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;也就是说，在一致性和强度上，Defined&amp;gt;Consistent&amp;gt;Inconsistent。&lt;/p&gt;

&lt;p&gt;在修改后，一个文件的当前状态将取决于此次修改的类型以及修改是否成功。具体来说：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;如果一次写入操作成功且没有与其他并发的写入操作发生重叠，那这部分的文件是确定的（同时也是一致的）。&lt;/li&gt;
    &lt;li&gt;如果有若干个写入操作并发地执行成功，那么这部分文件会是一致的但会是不确定的：在这种情况下，客户端所能看到的数据通常不能直接体现出其中的任何一次修改。也就是说，操作成功执行了，但是有的操作的数据改变被覆盖了，客户端看不到被覆盖的数据改变。&lt;/li&gt;
    &lt;li&gt;失败的写入操作会让文件进入不一致的状态。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;论文中给出了总结的表格：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/consistency.png&quot; alt=&quot;consistency&quot; /&gt;&lt;/p&gt;

&lt;p&gt;至于为什么追加（Append）是Defined但是有可能是不一致是因为：在追加写操作失败时，为了保证数据的偏移，可能为填充重复的数据，此时导致了不一致。但是失败的操作会被再次执行，此时又保证了数据的一致性。而由于使用的是追加，所以任何数据的改动都可以观察到，所以是Defined。&lt;/p&gt;

&lt;h3 id=&quot;快照snapshot&quot;&gt;快照（Snapshot）&lt;/h3&gt;

&lt;p&gt;这里的快照的目的不同于Raft中的压缩，它仅仅驶出为了生成一个新的Replica，可以看做是一个简单的复制操作。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;在 Master 接收到快照请求后，它首先会撤回相关Chunk  Server的 Lease，保证在创建快照的过程中，客户端对不会对相关的Chunk Server进行写操作或者追加操作。&lt;/li&gt;
    &lt;li&gt;当Lease收回后，Master会先将相关的改动写入日志，然后对自己管理的命名空间进行复制操作，复制产生的新记录指向原本的 Chunk。&lt;/li&gt;
    &lt;li&gt;当有客户端尝试对新的Chunk Server进行写入时，Master 会注意到这个 Chunk 的引用计数大于1（可能是一个标记）。此时，Master 会为要读取的Chunk生成一个Handle，然后通知所有持有这些 Chunk 的 Chunk Server 在本地复制并使用出新的 Chunk，然后再返回给客户端&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;我想GFS提供快照的原因可能是为了在一个副本损坏时，从Primary Replica或者其他副本复制数据，然后用新的节点代替损坏的节点。&lt;/p&gt;

&lt;h3 id=&quot;垃圾回收&quot;&gt;垃圾回收&lt;/h3&gt;

&lt;p&gt;当GFS收到删除文件的请求时，它并不直接删除文件，而是给文件打上删除的时间戳并将其命名为掩藏文件（文件开头加”.”）。在周期性扫描过程中，当发现文件的删除时间超过设定期限后，才真正地将文件删除。Google认为其有以下优点&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;对于大规模的分布式系统来说，这样的机制更为&lt;strong&gt;可靠&lt;/strong&gt;：在 Chunk 创建时，创建操作可能在某些 Chunk Server 上成功了，在其他 Chunk Server 上失败了，这导致某些 Chunk Server 上可能存在 Master 不知道的 Replica。除此以外，删除 Replica 的请求可能会发送失败，Master 会需要记得尝试重发。相比之下，由 Chunk Server 主动地删除 Replica 能够以一种更为统一的方式解决以上的问题&lt;/li&gt;
    &lt;li&gt;这样的删除机制将存储回收过程与 Master 日常的周期扫描过程合并在了一起，这就使得这些操作可以以批的形式进行处理，以减少资源损耗；除外，这样也得以让 Master 选择在相对空闲的时候进行这些操作&lt;/li&gt;
    &lt;li&gt;用户发送删除请求和数据被实际删除之间的延迟也有效避免了用户误操作的问题&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;高可用&quot;&gt;高可用&lt;/h3&gt;

&lt;p&gt;论文中主要探讨了三个方面的高可用，分别是Master，Chunk Server和数据完整性。&lt;/p&gt;

&lt;p&gt;当Master崩溃时，有两种情况，一是进程崩溃但是服务器没有，这种情况下，重开一个进程即可。另一种情况是整个机器崩溃了，在GFS还有被称为Shadow Master的机器，复制Master节点的信息。当Master机器崩溃后，Shadow Master会接替Master进行服务，但是仅提供读取操作的服务，不能更改信息。&lt;/p&gt;

&lt;p&gt;当Chunk Server崩溃时，Master会安排新的Replica代替它，从其他Replica复制原始数据。而当Chunk Server恢复时，由于数据不同步，不应该提供服务，Master就需要区别新旧Chunk Server。GFS使用版本号来标记这个信息，每分配以此Lease，版本号就会增加并同步给其他Replica，而由于Chunk Server崩溃后不能更新，我们就能从版本号上辨别新旧。&lt;/p&gt;

&lt;p&gt;由于写操作和追加操作可能不成功，所以数据可能会损坏。GFS使用检验和检查是否损坏，每次客户端读取数据时，Chunk Server都会检查检验和，一旦发现损坏，就会向Master报告。Master则会将请求发送给其他Replica，并从其他Replica复制数据到该机器。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;2003年GFS的横空出世具有划时代的意义，它标志着学术上的分布式理论和一些实验性质的尝试在工业界有了大规模商用的案例，尤其还是在Google这样的公司。它的系统设计在后续的系统中被屡次参考复用，而其设计确实有独到之处，比如Master负责控制流而完全将数据流从其剥离，这样的设计不能不说是优雅。这篇论文催生了HDFS，至今仍被许多公司使用，足以可见其影响力之大。哪怕这篇论文距离今天已经有快20年的时间，GFS在Google内部迭代多次，但是其设计仍然值得每一个分布式程序员学习。&lt;/p&gt;</content><author><name>Trafalgar Ricardo Lu</name></author><category term="distributedsystem" /><summary type="html">谷歌在2003到2006年间发表了三篇论文，《MapReduce:Simplified Data Processing on Large Clusters》，《Bigtable:A Distributed Storage System for Structured Data》和《The Google File System》介绍了Google如何对大规模数据进行存储和分析。这三篇论文开启了工业界的大数据时代，被称为Google的三驾马车。本文介绍Google File System的相关内容。</summary></entry><entry><title type="html">浅谈Amazon Aurora</title><link href="http://localhost:4000/%E6%B5%85%E8%B0%88Amazon-Aurora" rel="alternate" type="text/html" title="浅谈Amazon Aurora" /><published>2020-08-02T18:00:00+08:00</published><updated>2020-08-02T18:00:00+08:00</updated><id>http://localhost:4000/%E6%B5%85%E8%B0%88Amazon%20Aurora</id><content type="html" xml:base="http://localhost:4000/%E6%B5%85%E8%B0%88Amazon-Aurora">&lt;p&gt;上图为极地极光&lt;/p&gt;

&lt;p&gt;Amazon在2017年的SIGMOD上发表了《Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases》并在对Amazon Aurora进行了介绍，简要描述了他们由于对传统MySQL性能的不满，而设计了Aurora来代替，其性能有相当大的提升。从时间和公司我们就可以看出，这是比较新的工业界的解决方案，有很高的学习参考价值。&lt;/p&gt;

&lt;h2 id=&quot;aurora的总体架构&quot;&gt;Aurora的总体架构&lt;/h2&gt;

&lt;p&gt;虽然论文中在结尾时才对其作出总结，但是在开头就点名其架构，再步步深入会更加合理。下面是Aurora的总体架构图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Aurora Architecture.png&quot; alt=&quot;Aurora Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;需要指出的是，由于Aurora是为了代替MySQL，而MySQL用于关系型数据库，所以Aurora仅负责处理关系型数据库的服务，即RDS（Relational Database Service）。我们其实可以从图中看出相当多的信息，Aurora仅有Primary RW（Read/Write） DB一个主节点用于处理写请求，而其余的则为从节点Secondary RO（Read-Only） DB用于处理读请求，论文中指出Secondary RO DB可以多达15个。另外，每个Aurora配备六个存储节点，其中有两个节点使用Amazon Simple Storage Service（S3）存储技术进行备份，而剩余4个节点则直接存储在本地的SSD上。&lt;/p&gt;

&lt;p&gt;用户的应用通过Customer VPC接入，然后可以读写位于不同AZ（Availability Zone）的数据库。而不同的AZ分布于全球的不同的Region中。当用户的请求发送到Primary RW DB时，RDS HM（Host Manager）会检测到请求，并调用Aurora进行相应的操作。如果是写操作，则将相关信息发送给Secondary RO DB进行备份，同时将命令写入存储节点。如果是读操作，则直接从存储节点读取数据返回。&lt;/p&gt;

&lt;h2 id=&quot;使用传统mysql遇到的问题&quot;&gt;使用传统MySQL遇到的问题&lt;/h2&gt;

&lt;p&gt;Amazon在日常开发和维护中发现，计算能力和存储性能已经不再是其工作的瓶颈了，取而代之的是网络的流量。其实对于Amazon来说，只要有钱，CPU能用最好的就能解决计算能力的问题，机械硬盘不够用固态硬盘，固态硬盘不够就上内存，存储性能也解决了，但是网络的延迟靠大带宽是很难解决的，而拉近机房位置也是有上限的，必须要从业务逻辑和服务组件上找问题。所以他们发现了MySQL在分布式系统中消耗了大量的流量，还提高了延迟。具体如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/MySQL network.png&quot; alt=&quot;MySQL network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从图中我们可以看出，传统的MySQL如果想要执行一次写入操作必须经历以下几步：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;主节点将数据写入EBS1&lt;/li&gt;
    &lt;li&gt;EBS1将数据写入备份镜像EBS2&lt;/li&gt;
    &lt;li&gt;主节点将相关数据发送给从节点&lt;/li&gt;
    &lt;li&gt;从节点将数据写入EBS3&lt;/li&gt;
    &lt;li&gt;EBS3将数据写入EBS4&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;其中，第1,3,5步是串行的，也就是说，只有第1步完成了，才能执行第3步，第3步完成了才能执行第5步。这无疑增加了服务器返回数据的延迟。另外传统的MySQL在写入和传输数据时还需要很多的额外信息，这又增加了网络带宽的消耗。也就是说，MySQL的使用在分布式系统产生了两个问题&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;应答延迟太高&lt;/li&gt;
    &lt;li&gt;消耗网络带宽太多&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;所以当Amazon发现使用传统MySQL的弊端之后，决定设计新的组件来代替MySQL以解决上述两个问题。&lt;/p&gt;

&lt;h2 id=&quot;the-log-is-the-database&quot;&gt;The Log Is The Database&lt;/h2&gt;

&lt;p&gt;上面我们提到，MySQL在同步数据的过程中发送的信息太多，这该怎么办呢？Amazon也算是家大业大，直接自己重新设计标准，以往的数据库是真的数据库，现在他们用WAL也就是Log来整合所有有用的信息并删去无用的信息，既减少了数据传输量又保证了需要保留的信息。同时，他们使用了链式复制结构代替主从结构，简化了保证数据一致性的复杂度。具体架构如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Aurora Network.png&quot; alt=&quot;Aurora Network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以三个副本为例，当位于AZ1的主节点收到写请求后，它将请求的相关数据直接写入六个存储节点中，然后，将数据和一些额外的信息通过链式复制结构传递给位于AZ2和AZ3其他节点。和上图进行对比，明显可以看到主从节点之间网络通信中传输的数据减少了，主节点向存储节点写入数据时也从五种数据变为一种。这里要特别指出的是，此处的数据已经从MySQ定义的Log变为Amazon为Aurora量身定制的Log。由于需要传输数据量的减少，同步所消耗的网络带宽也大幅地减少了。&lt;/p&gt;

&lt;p&gt;另外，因为主节点负责将Log写入存储节点，而从节点仅存储Log不需要负责写入存储节点，这样就减少了在MySQL中额外的第四步和第五步操作的时间。而MySQL中的两级EBS存储操作也由一级Quorum的代替，就像上一篇文章提到的，两级存储的时间是两次操作的时间之和，而一级的Quorum操作的时间则是取决于Quorum中最长的应答时间。这样，Aurora也优化了应答延迟的时间。&lt;/p&gt;

&lt;p&gt;在上一篇文章中我们提到，链式复制仅仅适用于节点较少且物理位置较近的情况。很巧的是，Amazon提供的服务中副本不会超过15个，而经典的情况仅有3个，而虽然不同AZ可能会跨节点，但是Amazon实在有钱，能让AZ之间的延迟低于2ms。在这种情况下，使用链式复制实在合适不过，还大大降低了保证共识的复杂度，简直是完美的设计。&lt;/p&gt;

&lt;h2 id=&quot;storage-node&quot;&gt;Storage Node&lt;/h2&gt;

&lt;p&gt;上面，我们提到主节点将Redo Log写入存储节点。但是，此时Redo Log还未执行，需要在存储节点中执行相应的操作后才算真正完成。下面，我们再来看看Redo Log到达存储节点以后需要进行哪些操作。论文中给出的流程图如下。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Aurora Storage Node.png&quot; alt=&quot;Aurora Storage Node&quot; /&gt;&lt;/p&gt;

&lt;p&gt;具体的流程解释如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;存储节点通过Incoming Queue接受主节点的Log。&lt;/li&gt;
    &lt;li&gt;存储节点将Log存到本地硬盘后向主节点发送ACK，用以确认Quorum。&lt;/li&gt;
    &lt;li&gt;由于网络的不可靠和Quorum机制，当前存储节点可能缺失了部分Log。在这一步，它将Log排序并找出缺失的Log。&lt;/li&gt;
    &lt;li&gt;通过和其他存储节点进行交换信息，将缺失的Log复制到本地，将所有Log填充完整。&lt;/li&gt;
    &lt;li&gt;到目前为止，系统中存储的仍是Log而非用户需要数据，这一步执行Log对应的操作，并写入数据库中。&lt;/li&gt;
    &lt;li&gt;定期地将数据存为快照并存入Amazon S3中。&lt;/li&gt;
    &lt;li&gt;定期地进行垃圾收集，删除过期数据。&lt;/li&gt;
    &lt;li&gt;用CRC定期检验数据。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;从流程中我们可以看到，只有第一步和第二步可能影响应答延迟，其余的步骤都由存储节点在后台执行。这样一来，因为无需等待执行完毕，应答延迟就进一步降低了。&lt;/p&gt;

&lt;h2 id=&quot;读写操作&quot;&gt;读写操作&lt;/h2&gt;

&lt;p&gt;Amazon在设计Log时，为了实现一些功能给它添加了一些标记，具体如下&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;LSN：Log Sequence Number，相当于Log的自增主键，类似于Raft中的Index。&lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;VCL：Volume Complete LSN，受到Quorum承认的最大LSN。&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;CPLs：Consistenc yPoint LSNs，单个存储节点中已经收到ACK的最大LSN，所以每个节点各一个&lt;/li&gt;
    &lt;li&gt;VDL：Volume Durable LSN，已经持久化最大的LSN，也就是CPLs中最大的LSN&lt;/li&gt;
    &lt;li&gt;SCL：Segment Complete LSN，由每个段维护，代表段中已经持久化的最大LSN&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;在读写，复制和提交等操作中，Aurora会使用这些标记实现对应功能。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;写操作&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们在之前的文章中提到，如果未执行的Log积压过多会产生很不好的后果。所以在写操作时，Aurora会设置VDL+N作为未分配LSN的上限，通过设置N的值来限制未写入磁盘的Log的条数。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;读操作&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;为了提高效率，Aurora会在缓存中先查找是否有需要读取的数据，如果没有，再置换页面。在这里，Aurora要求置换页面中的LSN&amp;gt;VDL以确保数据为最新版本。这保证了所有页面的更新都已经持久化到日志并且在缓存区没有该数据页的情况下，可以根据 VDL 获取最新版本数据。&lt;/p&gt;

&lt;h2 id=&quot;fault-tolerance&quot;&gt;Fault-Tolerance&lt;/h2&gt;

&lt;p&gt;Aurora 将数据库文件切分成 10GB 大小的段（Segment）。在崩溃恢复的时候，Aurora要通过Quorum读得到VDL，并将大于此的Log阶段。由于在写操作时设置了LSN的上限，所以可以将需要Redo Log的LSN上限设置为VDL+N。然后重做已经标记的Log，就能恢复到初始状态，Aurora实验显示这个过程相当地快。&lt;/p&gt;

&lt;h2 id=&quot;不一样的quorum&quot;&gt;不一样的Quorum&lt;/h2&gt;

&lt;p&gt;我们上面提过，Aurora的六个存储节点部署在3个AZ中，每个AZ运行两个存储节点。Amazon考虑到可能整个AZ挂掉，导致两个存储节点崩溃，而AZ又有可能在同一个Region中，所以Aurora考虑的最坏情况是一个AZ崩溃加上一个存储节点崩溃，即AZ+1。&lt;/p&gt;

&lt;p&gt;Aurora提出了以下两个要求：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;在AZ+1崩溃的情况下不丢失数据，也就是保证读数据能力。&lt;/li&gt;
    &lt;li&gt;在AZ崩溃的情况下保证写数据能力。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;于是Aurora提出了读写两种情况的Quorum。在写情况下，需要六个节点中的四张票，即4/6。在读情况下，仅需要六个节点中的三张票，即3/6.&lt;/p&gt;

&lt;p&gt;很明显，写操作的Quorum和我们之前在Paxos和Raft中讨论的Quorum是一致的，也是2f+1需要f+1张票。而由于写操作每次至少写入四个节点，那么根据抽屉原理，每两次写操作至少有一个节点重复，那么读操作无论读哪一半都能在三个节点中读取到最新的全部数据。以此类推，哪怕一半的节点崩溃，Aurora也能读取到最新最全的数据。&lt;/p&gt;

&lt;p&gt;但是，要特别指出的是，读Quorum的要求仅仅在恢复时才使用，正常读是不需要的。&lt;/p&gt;

&lt;p&gt;从这里来看，其实这个Quorum也就是Raft中2f+1个节点容许f个崩溃的另一种说法。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;Amazon Aurora中描述的技术看起似乎很通用，使用WAL代替MySQL的信息，在存储节点执行命令而不是在本机执行，使用Chain Replication等等。但是能将这些技术恰到好处地使用在实际的系统中，并进行优化才是大厂的技术底蕴。比如Log的设计这一块，论文中就介绍地相当模糊，读写操作的细节也没有纰漏。这篇论文恐怕只算是对Aurora的惊鸿一瞥，真的想了解实现细节还得去Amazon内部看看。毕竟，Aurora在每个事务的IO花费的1/8，而事务处理量是MySQL的35倍，这可不是简单的系统设计就能完成的。&lt;/p&gt;</content><author><name>Trafalgar Ricardo Lu</name></author><category term="distributedsystem" /><summary type="html">Amazon在2017年的SIGMOD上发表了论文对Amazon Aurora进行了介绍，简要描述了他们由于对传统MySQL性能的不满，而设计了Aurora来代替，其性能有相当大的提升。从时间和公司我们就可以看出，这是比较新的工业界的解决方案，有很高的学习参考价值。</summary></entry></feed>